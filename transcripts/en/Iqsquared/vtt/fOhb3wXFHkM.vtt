WEBVTT

1
00:00:00.060 --> 00:00:04.380
<v 0>Hello, I'm Tom Whipple, author and science editor. At the times,</v>

2
00:00:04.620 --> 00:00:08.040
I'm here with Stewart Richie psychologist, professor,

3
00:00:08.041 --> 00:00:10.290
and a frequent debater on intelligence squared.

4
00:00:10.560 --> 00:00:15.120
He is the author of the new book science fictions, exposing fraud, bias,

5
00:00:15.180 --> 00:00:19.650
negligence, and hype in science. Welcome Stewart. Hi Tom.

6
00:00:20.430 --> 00:00:21.390
Hi. Um,

7
00:00:21.720 --> 00:00:26.520
so I think it'd be great to start this discussion where your book starts

8
00:00:26.550 --> 00:00:30.330
because you yourself have some experience of,

9
00:00:31.230 --> 00:00:34.620
for exposing fraud bias and hype in science, um,

10
00:00:35.130 --> 00:00:38.160
and quite an instructive anecdote about how that happened.

11
00:00:38.161 --> 00:00:40.530
You came across a paper, which if it was true,

12
00:00:40.531 --> 00:00:44.070
I think it's fair to say it would have completely changed our understanding of

13
00:00:44.160 --> 00:00:47.340
physics of humanity, of what it is to be a human.

14
00:00:47.341 --> 00:00:50.610
It would be the most important paper in the world.

15
00:00:50.640 --> 00:00:53.790
Pretty much ever people like me, journalists. Uh,

16
00:00:53.820 --> 00:00:56.850
we covered it as that's a bit funny, isn't it quirky,

17
00:00:56.851 --> 00:00:59.970
but you had a different response when it came out to tell us about it.

18
00:01:00.380 --> 00:01:05.050
<v 1>This was when I was a PhD student. We, uh, we, uh, we're um, I,</v>

19
00:01:05.060 --> 00:01:08.450
I always take an interest in the kind of latest controversies in, in,

20
00:01:08.630 --> 00:01:11.780
in psychology and, and w this is in 2011,

21
00:01:12.230 --> 00:01:15.890
the most controversial paper appear, which was one that, uh,

22
00:01:15.920 --> 00:01:20.000
showed that psychic powers exist. And it was published in, uh, a very, um,

23
00:01:20.420 --> 00:01:22.910
mainstream, uh, very well-respected journal,

24
00:01:22.911 --> 00:01:24.470
the sort of journal where if you get paper in it,

25
00:01:24.740 --> 00:01:28.130
that kind of makes your career in some, in some ways, uh,

26
00:01:28.340 --> 00:01:29.840
people talk about getting, you know,

27
00:01:29.870 --> 00:01:33.410
if you've got three journal articles in this journal, then, you know,

28
00:01:33.411 --> 00:01:35.660
you'll definitely get a job at a good university. So like a really,

29
00:01:35.720 --> 00:01:39.470
really mainstream prestigious place. And it was by a Darryl bam,

30
00:01:39.471 --> 00:01:43.520
who's a social psychologist at Cornell university in the U S

31
00:01:44.390 --> 00:01:49.040
and he, uh, had this amazing study where he had nine experiments, um,

32
00:01:49.070 --> 00:01:53.290
which kind of reversed the usual time sequence of, uh, of,

33
00:01:53.291 --> 00:01:57.950
of psychology studies. So, um, normally in a psychology study, you would, um,

34
00:01:58.100 --> 00:02:01.160
you would show people, uh, maybe so in a memory study,

35
00:02:01.161 --> 00:02:03.230
you would give them a list of words to remember,

36
00:02:03.470 --> 00:02:05.510
then you would remind them of half of the words,

37
00:02:05.750 --> 00:02:08.870
and then it turns out obviously that they would remember the words that you

38
00:02:08.871 --> 00:02:10.340
reminded them all. That makes perfect sense.

39
00:02:10.550 --> 00:02:14.810
But what he did in this study was we made the remainder happen after the test.

40
00:02:14.960 --> 00:02:16.370
So people saw a list of words,

41
00:02:16.890 --> 00:02:19.100
then they were given a test to try and remember those words,

42
00:02:19.550 --> 00:02:21.710
then they were reminded of them afterwards.

43
00:02:22.040 --> 00:02:24.890
And then that was the end of the experiment. And the idea was,

44
00:02:24.891 --> 00:02:28.190
and what he claimed to find in this paper was that the words that they were

45
00:02:28.191 --> 00:02:33.020
about to see in future were remembered better on the test.

46
00:02:33.170 --> 00:02:35.270
So, um, this is like, um,

47
00:02:35.300 --> 00:02:40.010
you study for an exam and then you sit the exam and then you study again after

48
00:02:40.011 --> 00:02:44.150
it. And that's some who looks backwards in time to help you on the exam that you

49
00:02:44.151 --> 00:02:47.240
just did, which is obviously like a mind blowing thing. And kind of,

50
00:02:47.510 --> 00:02:49.130
it takes a few seconds, even when you're reading the paper,

51
00:02:49.131 --> 00:02:53.030
you think is really far, and you kind of, you have to clarify, like, um, uh,

52
00:02:53.060 --> 00:02:54.680
get yourself into a complete different frame of mind. So,

53
00:02:54.800 --> 00:02:58.270
but that was published in a mainstream journal. So what we thought we would do,

54
00:02:58.430 --> 00:03:00.310
we would, we would try and replicate it.

55
00:03:00.550 --> 00:03:02.650
We would try and run the same experiment again, uh,

56
00:03:02.680 --> 00:03:06.700
using exactly the same software, exactly the same memory test, exactly the same,

57
00:03:06.701 --> 00:03:10.000
everything, um, uh, on our undergraduates, um,

58
00:03:10.060 --> 00:03:14.100
back when I worked at the university of Edinburgh. And so we, we published, uh,

59
00:03:14.380 --> 00:03:17.230
we, we did that replication and no,

60
00:03:17.470 --> 00:03:21.790
no surprise there that we didn't find a positive result. We found that people,

61
00:03:22.390 --> 00:03:25.060
um, did not remember the words that they were about to see they weren't psychic.

62
00:03:26.260 --> 00:03:26.290
Um,

63
00:03:26.290 --> 00:03:29.440
and we tried to publish that paper in the same journal that had been published

64
00:03:29.441 --> 00:03:33.950
in the original, had been published in, uh, the one, uh, um, the, the,

65
00:03:33.951 --> 00:03:36.910
the journal of personality and social psychology it's called.

66
00:03:37.330 --> 00:03:40.390
And that what they said to us instantly was, um,

67
00:03:40.420 --> 00:03:42.880
we do not accept replication studies.

68
00:03:42.881 --> 00:03:46.510
We do not consider a publication that publication of replication studies under

69
00:03:46.511 --> 00:03:50.830
any circumstances. Um, and that really shocked me because, um,

70
00:03:50.860 --> 00:03:53.980
they had published the original they'd published the original really exciting

71
00:03:54.040 --> 00:03:57.160
finding that it was kind of mind-blowing and got on the news and that, you know,

72
00:03:57.161 --> 00:04:01.180
the author got on the Colbert report in the U S it was like a big deal,

73
00:04:01.630 --> 00:04:05.200
but they wouldn't even consider publishing something that said actually,

74
00:04:05.201 --> 00:04:09.190
maybe people aren't psychic. Um, and so, um, that kind of stands in,

75
00:04:09.191 --> 00:04:13.360
I think for quite a lot of science where the focus is on exciting new

76
00:04:13.480 --> 00:04:18.070
groundbreaking findings and not on findings that are probably more likely to be

77
00:04:18.071 --> 00:04:20.830
true, but are maybe a bit more boring. Yeah.

78
00:04:21.450 --> 00:04:23.340
<v 0>Yeah. And this,</v>

79
00:04:23.341 --> 00:04:26.760
so the interesting thing about your book is I think the people who've been

80
00:04:26.761 --> 00:04:31.110
following this will know about this thing, that's called moving capital letters,

81
00:04:31.140 --> 00:04:33.840
the replication crisis, uh,

82
00:04:33.930 --> 00:04:38.820
and a lot of psychology has fallen down.

83
00:04:39.150 --> 00:04:43.170
Um, a lot of things that we, we sort of still a common knowledge,

84
00:04:43.350 --> 00:04:46.020
the sort of things that you would repeat at dinner parties, you know,

85
00:04:46.350 --> 00:04:48.360
the Stanford prison experiments,

86
00:04:48.420 --> 00:04:52.950
the power posing where suddenly the whole conservative party cabinet were

87
00:04:53.100 --> 00:04:57.720
standing like sort of slightly in constant Cowboys on the conference stage.

88
00:04:58.620 --> 00:05:01.530
Um, and all of these things that, you know,

89
00:05:01.560 --> 00:05:04.980
I think a lot of people heard about these falling. They thought, yeah,

90
00:05:04.981 --> 00:05:09.840
that figures that sort of slightly too easy, Ted talk kind of thing.

91
00:05:10.440 --> 00:05:13.860
Um, but what you're saying is that far from being a problem with psychology,

92
00:05:13.861 --> 00:05:18.180
this is, there are generalized problems here that spread into other disciplines.

93
00:05:18.660 --> 00:05:19.360
<v 1>Yeah, absolutely.</v>

94
00:05:19.360 --> 00:05:23.670
I think psychology has been the field where we've looked at this most, uh,

95
00:05:23.880 --> 00:05:27.780
most kind of systematically, like there's been these large attempts, you know,

96
00:05:27.781 --> 00:05:31.650
because of problems like the psychic experiment and various other things that

97
00:05:31.651 --> 00:05:34.590
happened around about 2011 and 12 since then,

98
00:05:34.710 --> 00:05:37.110
psychology has been much more aware of this problem.

99
00:05:37.230 --> 00:05:39.090
And so we've systematically start to look at it.

100
00:05:39.091 --> 00:05:41.820
So we've got a good idea of the problems in psychology,

101
00:05:41.850 --> 00:05:43.410
although there's still debate over, you know,

102
00:05:43.411 --> 00:05:46.020
exactly what is reliable and what isn't, as you would expect,

103
00:05:46.080 --> 00:05:49.590
scientists are constantly debating and arguing. That makes sense. But, um,

104
00:05:49.620 --> 00:05:53.310
we have indications that there are the same kinds of problems in other areas of

105
00:05:53.311 --> 00:05:56.210
science as well. So, um, uh, even if you, you know,

106
00:05:56.211 --> 00:05:59.000
if you start off in fields that are kind of closely allied to psychology,

107
00:05:59.001 --> 00:06:00.590
like economics and so on,

108
00:06:00.890 --> 00:06:04.940
and they've done some replications surveys and find similar problems, um,

109
00:06:05.000 --> 00:06:05.471
then you, you,

110
00:06:05.471 --> 00:06:08.810
you kind of move out further into more kind of what you might call hard sciences

111
00:06:08.811 --> 00:06:12.110
a little again, that's not a term that a lot of people like these days, but, um,

112
00:06:12.440 --> 00:06:15.350
harder sciences like physics and chemistry and so on. Um,

113
00:06:15.800 --> 00:06:20.330
I think probably they have less problems in, in those, in those, uh, um,

114
00:06:20.450 --> 00:06:24.710
disciplines they have, you know, um, slightly more, uh, control over the

115
00:06:26.360 --> 00:06:31.220
Germans, uh, less, um, kind of fickle human beings in their, in their, in their,

116
00:06:31.280 --> 00:06:33.530
you know, as, as their, as their test subjects, they can,

117
00:06:33.560 --> 00:06:37.280
they can really control things in a, in a, in a more, uh, in a stronger way,

118
00:06:37.310 --> 00:06:42.200
but there are glimmers of the same kinds of issues, um, fraudulent, uh,

119
00:06:42.230 --> 00:06:47.060
scientists publishing papers, uh, and getting away with it for years. Um, uh,

120
00:06:47.120 --> 00:06:51.020
unreplicable experiments being retracted surveys of scientists that say that

121
00:06:51.140 --> 00:06:54.100
they've tried to find the same result as a previous paper phoned,

122
00:06:54.101 --> 00:06:57.770
and they've been unable to do that. Um, and, and even ones, you know,

123
00:06:58.430 --> 00:07:00.440
you see this quite a lot, for instance, in, uh,

124
00:07:00.460 --> 00:07:03.440
in computer science algorithms and so on, um,

125
00:07:03.560 --> 00:07:07.280
people being unable to even make the same experiment work again, uh,

126
00:07:07.310 --> 00:07:10.610
even if you use exactly the same code and exactly the same data. Um,

127
00:07:10.640 --> 00:07:12.470
so you know, all these algorithms that are published in journals,

128
00:07:12.471 --> 00:07:13.610
people take the code and go, okay,

129
00:07:13.611 --> 00:07:15.380
I'll run that and maybe try and extend it in some way,

130
00:07:15.530 --> 00:07:17.060
and then they can't even get it to work in the first place.

131
00:07:17.300 --> 00:07:18.680
So we've got all these problems.

132
00:07:18.681 --> 00:07:21.230
And the scariest of all of course is when it happens in medicine.

133
00:07:21.380 --> 00:07:22.880
And we don't have these treatments that are used in,

134
00:07:23.000 --> 00:07:26.090
are written up in medical guidelines and recommended doctors.

135
00:07:26.540 --> 00:07:29.630
And then another study comes along and completely contradicts. Um,

136
00:07:29.750 --> 00:07:30.860
what was the case before?

137
00:07:30.950 --> 00:07:34.490
Probably because the quality of the original studies on which the original

138
00:07:34.491 --> 00:07:37.340
recommendation was based was really poor.

139
00:07:38.590 --> 00:07:43.510
<v 0>So for listeners who aren't familiar about how science works,</v>

140
00:07:43.511 --> 00:07:46.720
how should it talk me through the ideal, if you were,

141
00:07:46.990 --> 00:07:51.640
if you were describing this sort of pure mechanism that humans have derived for

142
00:07:51.880 --> 00:07:55.990
finding knowledge and furthering our race, how should it work? What, what, from,

143
00:07:56.020 --> 00:07:59.500
from the experiment to the publication, to what comes next,

144
00:07:59.501 --> 00:08:02.050
what should be happening and what are the checks and balances?

145
00:08:02.140 --> 00:08:02.973
It shouldn't be that well.

146
00:08:03.130 --> 00:08:05.890
<v 1>Normally you would expect. I think if you ask people, you know,</v>

147
00:08:05.920 --> 00:08:07.900
what they expect scientists to do,

148
00:08:07.930 --> 00:08:10.750
the idea would be that they would come up with a strong, uh,

149
00:08:10.810 --> 00:08:14.420
experiment before they've even really looked to the day. They come up with,

150
00:08:14.740 --> 00:08:15.640
come up with an experiment.

151
00:08:15.641 --> 00:08:19.420
They come up with an idea of how to analyze that experiment and then write it

152
00:08:19.421 --> 00:08:24.370
all down and plan it all out. And then they go and collect some data. Then the,

153
00:08:24.730 --> 00:08:29.170
uh, once the data are in the run, their fiscal analysis, maybe even,

154
00:08:29.770 --> 00:08:32.620
uh, they get someone else to run the circle analysis. Who's not, you know,

155
00:08:32.710 --> 00:08:35.620
part of the same, who's not, doesn't have the same biases and so on.

156
00:08:36.100 --> 00:08:40.810
And then they would submit it to the ultimate system of checks and balances,

157
00:08:40.811 --> 00:08:43.210
which is of course the peer review system, uh,

158
00:08:43.240 --> 00:08:47.890
where you send your article off to a journal, you pick a journal that's, uh,

159
00:08:48.010 --> 00:08:51.430
you know, particularly close to your field. You know, you would, uh,

160
00:08:52.300 --> 00:08:54.970
for the experiment we talked about before is the journal of personality.

161
00:08:55.050 --> 00:08:57.360
Social psychology is very clear what the public is.

162
00:08:57.660 --> 00:09:00.300
So kind of specific to psychology journal. Um,

163
00:09:00.480 --> 00:09:01.560
there are also the general journals,

164
00:09:01.561 --> 00:09:05.520
like nature and science and things which publish, uh, you know,

165
00:09:05.550 --> 00:09:09.210
results from across the sciences. And then the editor of that journal.

166
00:09:09.211 --> 00:09:13.500
If they're interested in the paper, we'll send out to, um, your colleagues,

167
00:09:13.501 --> 00:09:15.780
your peers for peer review, um, uh, and, uh,

168
00:09:16.050 --> 00:09:20.400
they will give a completely disinterested look at the paper and analyze it

169
00:09:20.401 --> 00:09:23.940
statistics and stand back and say, does this make sense? Does this work,

170
00:09:24.500 --> 00:09:27.780
that we'll dig into the details and, uh, and, and, and look at the, um,

171
00:09:28.020 --> 00:09:32.760
whether your numbers make sense and so on. And then, uh, you will, um, uh,

172
00:09:32.790 --> 00:09:35.160
get your publication and you'll get your light on your CV.

173
00:09:35.161 --> 00:09:37.530
And you'll be very pleased. And, uh, and so on,

174
00:09:37.890 --> 00:09:41.520
you've made a contribution to the scientific, uh,

175
00:09:41.580 --> 00:09:43.080
the scientific literature.

176
00:09:43.500 --> 00:09:48.150
The problem is that at every single stage of that process, that's the ideal, um,

177
00:09:48.290 --> 00:09:51.890
at every single stage, all sorts of biases and failures and,

178
00:09:51.891 --> 00:09:53.520
and issues come in and not for them.

179
00:09:53.960 --> 00:09:56.330
<v 2>Okay. So let's, um, let's start. Where does,</v>

180
00:09:56.360 --> 00:09:58.530
where does the first bit of [inaudible].

181
00:09:59.120 --> 00:10:03.740
<v 1>If you ask again, if you ask just someone who, you know,</v>

182
00:10:03.741 --> 00:10:06.290
who wasn't an expert in science, you've just asked them, you know,

183
00:10:06.320 --> 00:10:09.410
do you think scientists plan out their analysis beforehand,

184
00:10:09.980 --> 00:10:14.060
or do you think they collect the data and then kind of just do the analysis kind

185
00:10:14.061 --> 00:10:17.870
of ad hoc as they go along and kind of let themselves do anything as they,

186
00:10:17.871 --> 00:10:20.510
as they, as they go through? Of course they would say that it was the former,

187
00:10:20.660 --> 00:10:22.880
but actually the vast majority of cases,

188
00:10:23.000 --> 00:10:25.490
scientists are just kind of running an analysis, ad hoc as the,

189
00:10:25.491 --> 00:10:28.670
as they go along. There's no plan.

190
00:10:28.880 --> 00:10:32.180
Normally there's sometimes a very vague idea of what they're going to do once

191
00:10:32.181 --> 00:10:35.450
they get the data. But the problem is that when you run your analysis ad hoc,

192
00:10:35.810 --> 00:10:39.920
you allow yourself to be, um, to be dragged along in different directions.

193
00:10:39.950 --> 00:10:44.240
And you can introduce lots and lots of biases into your, um,

194
00:10:44.480 --> 00:10:49.100
into your, into your experiment. Now, um, uh, one of the,

195
00:10:49.130 --> 00:10:52.280
one of the major biases of course, is that people want to find positive results.

196
00:10:52.281 --> 00:10:56.570
They want to find statistically significant results. Uh, and, um, uh,

197
00:10:57.290 --> 00:10:59.370
if you look across the literature, you find that in,

198
00:10:59.371 --> 00:11:03.590
in subjects like psychology, um, about 90 something percent of,

199
00:11:03.600 --> 00:11:05.750
of experiments have positive results,

200
00:11:06.290 --> 00:11:08.960
which you wouldn't necessarily expect if scientists were just trying out new

201
00:11:08.961 --> 00:11:11.750
ideas and some of them failed, and some of them were true and, you know,

202
00:11:11.751 --> 00:11:15.700
they just, you know, they, they were, they were, um, they were kind of, uh,

203
00:11:16.100 --> 00:11:18.020
really properly experimenting with the world.

204
00:11:18.170 --> 00:11:21.560
But actually what you find is that 90% of them or more are, uh, are,

205
00:11:21.561 --> 00:11:25.610
are positive, which implies that we're not seeing the ones that didn't work.

206
00:11:25.640 --> 00:11:29.090
We're not seeing the blind alleys, we're not seeing the, the, the, uh,

207
00:11:29.150 --> 00:11:32.960
experiments. Um, and so these kind of these two things I've mentioned,

208
00:11:33.020 --> 00:11:37.010
the statistical biases and the bias towards finding positive results work

209
00:11:37.011 --> 00:11:40.760
together. And you're really insidious way, which is that they, uh,

210
00:11:40.790 --> 00:11:45.770
which is that scientists sometimes unconsciously run their analysis in such a

211
00:11:45.771 --> 00:11:49.670
way that they find positive results. And the reason that they do that of course,

212
00:11:49.820 --> 00:11:51.620
is, is that the journals won't post the results.

213
00:11:51.621 --> 00:11:53.030
They want the exciting flashy results.

214
00:11:53.230 --> 00:11:55.510
Like I've mentioned with that psychic study. Um,

215
00:11:55.630 --> 00:11:58.870
they themselves want to find positive results because it's cool to find positive

216
00:11:58.871 --> 00:11:59.230
results.

217
00:11:59.230 --> 00:12:02.200
You want to be able to show that you've advanced the literature that you've

218
00:12:02.201 --> 00:12:05.770
advanced the treatment of a disease, or, uh, discovered some new,

219
00:12:05.771 --> 00:12:07.630
exciting knowledge about the world. Um,

220
00:12:07.660 --> 00:12:10.300
and what you don't want to find is that your treatment idea didn't work.

221
00:12:10.330 --> 00:12:13.360
Your trial was a failure, your, um, you know, your,

222
00:12:13.361 --> 00:12:17.920
your new mechanism for how some, uh, um,

223
00:12:18.250 --> 00:12:22.150
you know, some psychological trait works with actually not true, you know, so,

224
00:12:22.151 --> 00:12:26.020
so, so, um, you, you wanna, you want to, uh, find positive results.

225
00:12:26.290 --> 00:12:28.760
So that's the first place that the, the, the,

226
00:12:28.790 --> 00:12:33.100
the bias comes in is that scientists make the decision as to whether to publish

227
00:12:33.101 --> 00:12:34.810
stuff on the basis of what the results were.

228
00:12:35.460 --> 00:12:37.860
<v 0>So the ju just to explain that,</v>

229
00:12:38.160 --> 00:12:42.330
to read as an example in your book, um, I think,

230
00:12:42.331 --> 00:12:47.310
I think with this, this is something that's called P hacking. Um, uh,

231
00:12:47.550 --> 00:12:51.300
and I think the canonical example of that is probably someone that most people

232
00:12:51.540 --> 00:12:53.670
will have heard of they'll have heard about how,

233
00:12:53.790 --> 00:12:57.690
if you keep on filling a soup bowl from the bottom and you'll eat indefinitely,

234
00:12:57.691 --> 00:13:01.980
or if you take a bigger plate to a buffet, you'll fill it up faster.

235
00:13:02.370 --> 00:13:05.430
How will these are glorious results? I'm along the journalist,

236
00:13:05.460 --> 00:13:08.220
I'm waiting for the bit where you get to how we screw up the process,

237
00:13:08.670 --> 00:13:11.970
but one of the ways is we absolutely love stories and

238
00:13:13.470 --> 00:13:14.760
ruin the story for me.

239
00:13:16.330 --> 00:13:20.370
<v 1>Well, unfortunately, yeah, a lot of those findings have come into question. So,</v>

240
00:13:20.580 --> 00:13:23.100
and this is this, this is the, the, um, this is Brian Wansink,

241
00:13:23.160 --> 00:13:25.140
who was a professor again at Cornell.

242
00:13:25.141 --> 00:13:27.690
And I don't have an animus against Cornell university. It's just,

243
00:13:27.810 --> 00:13:31.620
that happens to be that, that that's where these results were from. Um,

244
00:13:31.650 --> 00:13:35.430
Brian Wansink, uh, was the head of the Cornell food and brand lab.

245
00:13:35.431 --> 00:13:38.670
And it was his idea that they had this bottom was bowl, as you say.

246
00:13:38.671 --> 00:13:41.100
And it was also his idea that bigger plates, if you take a bigger plate,

247
00:13:41.101 --> 00:13:43.170
you'll put more food on it. And these were fine. Like, as you say,

248
00:13:43.171 --> 00:13:46.410
everyone knows this, and there were published in, you know, psychology, food,

249
00:13:46.411 --> 00:13:51.300
psychology journals. Um, he wrote a really unfortunate blog post in, I think,

250
00:13:51.301 --> 00:13:56.070
2006, um, towards the end of 2006, he wrote, sorry, I turned 16. I should say,

251
00:13:56.280 --> 00:13:58.860
wrote this really unfortunate blog post, where he said, um,

252
00:13:59.700 --> 00:14:01.980
I really like it when my students just get,

253
00:14:02.160 --> 00:14:04.260
they gather a whole bunch of data with a whole bunch of stuff,

254
00:14:04.470 --> 00:14:06.960
and then they just keep analyzing it until they find something.

255
00:14:07.140 --> 00:14:09.830
And then we publish that. Um, and, you know, as I was just talking about it,

256
00:14:09.831 --> 00:14:12.870
I don't think that's how most people would understand that scientists do stuff.

257
00:14:12.900 --> 00:14:15.540
They think that scientists would come up with an idea and then test whether it's

258
00:14:15.541 --> 00:14:18.990
true and then published whether or not it was true. Um,

259
00:14:19.020 --> 00:14:22.050
but actually what he was admitting to doing inadvertently,

260
00:14:22.051 --> 00:14:23.760
I don't think he realized that he was saying this,

261
00:14:23.761 --> 00:14:26.760
but what he was admitting to doing was just collecting a bunch of data dredging

262
00:14:26.790 --> 00:14:30.360
through it until he finds anything. Um, and, and then publishing it.

263
00:14:30.480 --> 00:14:32.910
And the reason you can do that is that, uh,

264
00:14:33.090 --> 00:14:37.900
the more you run the statistical tests that most scientists use to check whether

265
00:14:37.901 --> 00:14:41.190
the result is, is, uh, is, is true or not, or is, uh, is,

266
00:14:41.191 --> 00:14:45.510
is just due to fiscal chance or not, which is a statistical significance test.

267
00:14:46.080 --> 00:14:49.260
Um, and you get, you get that. You get a P value from that,

268
00:14:49.261 --> 00:14:52.730
which is where the term you used P hacking comes in. Um,

269
00:14:52.850 --> 00:14:56.510
the more change you run that test, the more likely you are to be fooled,

270
00:14:56.511 --> 00:14:57.710
essentially to be shown,

271
00:14:57.930 --> 00:15:00.920
to attempt to find a test result that looks like it's a positive result,

272
00:15:00.921 --> 00:15:05.120
but actually it's just due to chance. Um, and so, um,

273
00:15:05.540 --> 00:15:07.490
this, um, this,

274
00:15:07.491 --> 00:15:10.910
this problem becomes really acute when you're just dragging through and just

275
00:15:10.911 --> 00:15:14.240
running more and more and more, more tests check with her check with her men,

276
00:15:14.660 --> 00:15:18.320
uh, pay more in tips when there's a woman, uh, with them at the, at the table,

277
00:15:18.321 --> 00:15:21.220
or whether it's a man check, whether, um, they,

278
00:15:21.221 --> 00:15:24.920
they pay more in tips when it's a pizza versus it's, it's past that check.

279
00:15:24.921 --> 00:15:27.650
Whether, you know, you can do all these, you know, if you collect enough data,

280
00:15:27.651 --> 00:15:31.450
you can just keep running tests until you find a result and then,

281
00:15:31.451 --> 00:15:33.740
and then publish it. And that is what he was admitting to doing. So,

282
00:15:33.741 --> 00:15:34.760
unfortunately at that point,

283
00:15:34.761 --> 00:15:39.440
people started looking into his research and finding all sorts of really sloppy

284
00:15:39.441 --> 00:15:44.000
errors, really sloppy mistakes. And even he was, he was accused of,

285
00:15:44.660 --> 00:15:48.410
uh, by his university, who've actually did an investigation of misconduct,

286
00:15:48.411 --> 00:15:51.320
not necessarily fraud, and that he wasn't making the data up,

287
00:15:51.530 --> 00:15:53.750
but he was analyzing them in such a way that,

288
00:15:53.940 --> 00:15:57.380
that it was essentially meaningless the results. Um, and so he's, he's actually,

289
00:15:57.500 --> 00:16:02.240
he's retired from his position now, he's, he's, he's gone. Um, uh, because,

290
00:16:02.241 --> 00:16:04.130
you know, he just wasn't really doing science.

291
00:16:04.131 --> 00:16:08.720
He was doing kind of dredging through numbers and statistical noise. Uh,

292
00:16:09.170 --> 00:16:12.650
um, and if you know, there's enough statistical noise around that,

293
00:16:12.651 --> 00:16:15.080
you can find basically wherever you want, if you have a big enough data set.

294
00:16:15.830 --> 00:16:15.880
Yeah.

295
00:16:15.880 --> 00:16:20.670
<v 0>Yeah. Okay. And then, so, so I I've, I haven't P hacked I've,</v>

296
00:16:20.850 --> 00:16:25.690
I've got my paper through with, uh, legitimate statistical analyses. Um,

297
00:16:25.720 --> 00:16:26.553
what's the next.

298
00:16:26.640 --> 00:16:31.570
<v 1>This goes wrong. The problem that, that can then come in is that, um, so you,</v>

299
00:16:31.780 --> 00:16:31.971
uh, you,

300
00:16:31.971 --> 00:16:34.390
you have to also have made sure that you haven't made any mistakes in your

301
00:16:34.391 --> 00:16:37.390
analysis. So, um, you know, P hacking can be completely unconscious,

302
00:16:37.391 --> 00:16:41.110
but another unconscious problem is that a lot of people just sloppily make

303
00:16:41.111 --> 00:16:44.110
mistakes and, you know, copy and paste errors and these lab.

304
00:16:44.140 --> 00:16:46.030
But assuming that you've not done any of that,

305
00:16:46.510 --> 00:16:50.020
the next problem comes when you're actually communicating it to the world. So,

306
00:16:50.230 --> 00:16:52.150
um, when writing the papers, uh,

307
00:16:52.180 --> 00:16:55.240
scientists spend an awful lot of time hyping up their results.

308
00:16:55.420 --> 00:16:58.000
So I'm writing papers as if, uh,

309
00:16:58.030 --> 00:17:00.790
the results were the most important thing ever to be discovered on this,

310
00:17:00.820 --> 00:17:04.180
on this topic. Um, the classic line about, you know,

311
00:17:04.330 --> 00:17:06.790
this has implications for, and then, you know,

312
00:17:06.791 --> 00:17:08.410
it might be the treatment of mental health disorders,

313
00:17:08.411 --> 00:17:12.940
or this has serious implications for how we understand X, Y,

314
00:17:12.941 --> 00:17:15.520
Z thing about the world, whatever it happens to be, um,

315
00:17:15.550 --> 00:17:18.070
scientists love seeing things like that. And again,

316
00:17:18.071 --> 00:17:20.080
it's from it's because of pressure from the journals,

317
00:17:20.081 --> 00:17:23.050
the journals want to publish papers that, uh,

318
00:17:23.080 --> 00:17:25.090
that discover exciting new things about the world.

319
00:17:25.180 --> 00:17:30.010
So scientists write up the papers in ways that kind of decouple themselves from

320
00:17:30.011 --> 00:17:33.430
the data, the actual, you know, the texts, which is what most people read,

321
00:17:33.460 --> 00:17:36.460
especially if you don't understand this, this takes, and most people,

322
00:17:36.461 --> 00:17:39.730
even who are scientists don't understand the statistical methods that are used

323
00:17:39.731 --> 00:17:42.250
in other fields. So, you know, we're looking at the, the, you know,

324
00:17:42.251 --> 00:17:46.420
the discussion section and the paper that tells you the, the implications. Um,

325
00:17:46.870 --> 00:17:48.590
they write it in such a way that makes the,

326
00:17:48.880 --> 00:17:53.550
is it goes way beyond what the data have actually shown. Um, and then, uh,

327
00:17:53.610 --> 00:17:57.900
they often then move towards, um, uh, you know,

328
00:17:58.530 --> 00:18:02.040
actually taking the, the beyond the scientific journals and into the real world,

329
00:18:02.250 --> 00:18:03.660
which is where press releases come in.

330
00:18:03.870 --> 00:18:07.530
I think a lot of people think that press releases are just written by, um,

331
00:18:07.920 --> 00:18:10.420
are just written by, uh, uh, you know,

332
00:18:11.670 --> 00:18:14.390
PR people at universities and so on, but actually they're,

333
00:18:14.610 --> 00:18:17.550
they're in many cases written by the scientists themselves. Um,

334
00:18:17.730 --> 00:18:19.530
and that's a place where there's no peer review,

335
00:18:19.531 --> 00:18:21.510
so you can see whatever you want is actually,

336
00:18:21.840 --> 00:18:25.710
and people make up all sorts of stuff about the research and in press releases.

337
00:18:26.110 --> 00:18:26.111
They,

338
00:18:26.111 --> 00:18:31.050
they jumped from correlational research to causal conclusions that jump from

339
00:18:31.051 --> 00:18:35.000
research that's in mice or in rats to this is definitely true in, in, in,

340
00:18:35.001 --> 00:18:39.270
in humans. Um, they jumped from, um, a study of, uh,

341
00:18:39.300 --> 00:18:42.870
just a few people to, this is definitely a how everyone should, you know,

342
00:18:42.900 --> 00:18:47.820
eat or exercise or treat their kids or whatever it happens to be. Um,

343
00:18:47.880 --> 00:18:50.340
so, uh, the scientists themselves,

344
00:18:50.341 --> 00:18:53.760
I think a lot of people blame journalists and blame PR people and so on it,

345
00:18:53.761 --> 00:18:58.200
but it's the scientists themselves that are often the source of this hype, um,

346
00:18:58.230 --> 00:19:00.350
when, when, when results go out into the real world.

347
00:19:00.530 --> 00:19:03.980
<v 0>Yeah. Yeah. There's a, um, I endorsed,</v>

348
00:19:04.100 --> 00:19:07.730
you're blaming of people who aren't meat. Uh, uh,

349
00:19:08.030 --> 00:19:09.560
there's a,

350
00:19:09.620 --> 00:19:14.510
quite a nice sort of examples slash parable in your book where

351
00:19:14.511 --> 00:19:17.780
you talk about, uh, because at the moment we've chatted about funding,

352
00:19:17.810 --> 00:19:21.200
things like soup bowls and paranormal, but, uh,

353
00:19:21.230 --> 00:19:23.480
this is obviously this a very serious side, as you've made clear.

354
00:19:23.720 --> 00:19:27.650
And you talk about studies into antidepressant, um,

355
00:19:27.710 --> 00:19:31.340
and there was a kind of study of studies where the scientists looked at a

356
00:19:31.341 --> 00:19:34.550
hundred, um, I guess pre-registered, um,

357
00:19:34.760 --> 00:19:39.540
studies and looked what actually happened. And I don't know if you can remember,

358
00:19:39.640 --> 00:19:42.530
but can you talk through what happened to these hundred papers?

359
00:19:43.430 --> 00:19:46.000
<v 1>Yeah, it's an incredible study actually. And it's got this really, it's got,</v>

360
00:19:46.100 --> 00:19:46.851
it's really vivid figure,

361
00:19:46.851 --> 00:19:50.930
which I reproduced in the book where you can see at the start, there were, um,

362
00:19:51.080 --> 00:19:54.350
actually of the studies that were registered because clinical trials have to be

363
00:19:54.440 --> 00:19:58.370
registered before you, uh, before you, uh, run the study. That's,

364
00:19:58.371 --> 00:20:02.510
that's the only kind of study where like legally you have to register them. Um,

365
00:20:03.200 --> 00:20:07.190
so we know all the clinical trials that are taking place unless they're done the

366
00:20:07.460 --> 00:20:09.200
legally, which I think is not very, it's not very common,

367
00:20:09.201 --> 00:20:12.230
but we know that they're all taken. We know which ones are taking place. Um,

368
00:20:12.320 --> 00:20:16.040
and then it turned out that, you know, about 50, 50, um, uh,

369
00:20:16.070 --> 00:20:19.070
in half the cases the drug works and half the cases, it didn't,

370
00:20:19.071 --> 00:20:23.780
so you've got about 50 50, but then you see the process of, um,

371
00:20:24.170 --> 00:20:27.920
uh, the negative results basically disappearing. So we know that they exist.

372
00:20:27.950 --> 00:20:29.180
We know that they happened,

373
00:20:29.210 --> 00:20:32.450
that the trial was run that had those negative results,

374
00:20:32.690 --> 00:20:35.120
but as you go through the process of, uh,

375
00:20:35.160 --> 00:20:38.210
of which ones get chosen to be published in journals, um,

376
00:20:38.610 --> 00:20:42.080
and it's basically all the positive ones get published and all the negative

377
00:20:42.081 --> 00:20:45.950
ones, uh, just kind of dissipate and you just have a few negative ones there.

378
00:20:46.370 --> 00:20:47.203
Then you go through the,

379
00:20:47.350 --> 00:20:49.900
of how they're actually written up in the papers themselves,

380
00:20:50.170 --> 00:20:55.000
and you find no results, um, uh, often becomes subject to spin. So,

381
00:20:55.180 --> 00:20:57.400
uh, they are, they're written verbally as if they were, you know,

382
00:20:57.430 --> 00:21:01.630
kind of almost there. And this, this probably works kind of the old, sure.

383
00:21:01.631 --> 00:21:04.180
The statistics didn't show that it works, but, but probably it works, you know,

384
00:21:04.410 --> 00:21:08.740
and you can write it up in a, in a kind of spun way like that. Um,

385
00:21:09.400 --> 00:21:10.780
and then actually by that, by the end,

386
00:21:10.960 --> 00:21:14.410
they show that the studies that got positive results were cited more by other

387
00:21:14.411 --> 00:21:17.500
scientists than the ones that go out negative results as well.

388
00:21:17.560 --> 00:21:19.240
So by the time you've been through those process,

389
00:21:19.480 --> 00:21:23.680
you change the real scientific literature from, which was about 50,

390
00:21:23.681 --> 00:21:26.530
50 into something where I don't know, I can't remember the exact numbers,

391
00:21:26.531 --> 00:21:29.380
but it's like 90% positive or something, or even worse than that.

392
00:21:30.120 --> 00:21:32.010
<v 0>Yeah. If you, you dropped, um,</v>

393
00:21:32.040 --> 00:21:35.010
50 odd papers that were negative to basically five.

394
00:21:35.970 --> 00:21:37.980
<v 1>Yeah. It's incredible. Really when you, when you see it like that,</v>

395
00:21:38.250 --> 00:21:40.440
and it's not just antidepressants, you know, this, this happens,

396
00:21:40.560 --> 00:21:43.590
they showed in that same paper that it happens for studies on, um,

397
00:21:43.620 --> 00:21:46.770
for trials of, of psychotherapy as well for depression. Um,

398
00:21:46.800 --> 00:21:50.880
but I'm pretty sure given all the other evidence that there is publication bias

399
00:21:50.881 --> 00:21:54.480
and all these issues that this happens for lots of other, um,

400
00:21:54.750 --> 00:21:56.550
for lots of other topics as well.

401
00:21:57.090 --> 00:21:59.970
<v 0>Yeah. Yeah. Um, so I mean,</v>

402
00:22:00.420 --> 00:22:04.650
I'm listening to this and regularly when you look at surveys, um,

403
00:22:05.130 --> 00:22:09.420
scientists are always the most trusted profession. Um,

404
00:22:09.750 --> 00:22:13.800
journalists very unfairly are always the least trusted profession other than the

405
00:22:13.801 --> 00:22:18.060
state agents. Um, are we right? Well, first of all,

406
00:22:18.061 --> 00:22:20.580
are we right to trust you all? And secondly,

407
00:22:21.210 --> 00:22:25.680
is this the result of scientists overly selecting

408
00:22:25.681 --> 00:22:29.400
for malevolent and deceitful individuals? Um,

409
00:22:29.430 --> 00:22:33.840
or is there something else going on about the way that science works,

410
00:22:34.230 --> 00:22:36.810
that chronic decent people and

411
00:22:39.210 --> 00:22:41.640
being in this, this system where this is how it works.

412
00:22:41.950 --> 00:22:44.820
<v 1>We were right to trust the idea of science we're right.</v>

413
00:22:44.821 --> 00:22:48.600
To trust the process of science, the, the methods of science. I mean,

414
00:22:48.840 --> 00:22:49.673
there's no,

415
00:22:49.890 --> 00:22:52.650
one's such thing as the scientific method and philosophers have argued for a

416
00:22:52.651 --> 00:22:54.320
long, long time about that, but, you know, were,

417
00:22:54.600 --> 00:22:59.250
were the basic principles of science are Bulletproof, as far as I'm concerned.

418
00:22:59.760 --> 00:23:03.240
Um, it's the, it's the way that those are instantiated in the world, uh,

419
00:23:03.241 --> 00:23:06.090
and the way that those are essentially acting in academia, that becomes,

420
00:23:06.360 --> 00:23:10.980
that becomes problematic. So, um, uh, we're totally, we're totally,

421
00:23:11.750 --> 00:23:14.630
uh, the, the idea of doing peer review is a great one where you're,

422
00:23:14.790 --> 00:23:17.910
you're asking for outside scrutiny of, of results. Um,

423
00:23:18.060 --> 00:23:21.120
but the way it's instantiated is, well, you're, you're, you, you know,

424
00:23:21.121 --> 00:23:25.470
you give a paper to other scientists who are themselves busy scientists with

425
00:23:25.471 --> 00:23:28.890
other stuff on their minds. You give them a time limit to do it.

426
00:23:28.920 --> 00:23:31.440
You don't give them any additional reward or P to do it.

427
00:23:31.441 --> 00:23:33.390
You just get them to do the goodness of their heart.

428
00:23:33.900 --> 00:23:37.310
And they're probably going to do, you know, a not great job on it. Or,

429
00:23:37.311 --> 00:23:40.200
or in many cases, they're not going to do a great job on it. Um, of course,

430
00:23:40.410 --> 00:23:44.130
lots of peer reviewers are really heroic and that they, they, they still,

431
00:23:44.160 --> 00:23:47.780
despite all that stuff still do a great job, but, um, but, uh, you know,

432
00:23:47.781 --> 00:23:49.340
in many cases you can see why they would,

433
00:23:49.370 --> 00:23:52.370
they would be rushed and they would not notice errors in papers.

434
00:23:52.670 --> 00:23:55.190
Dodgies discal analysis, even sometimes fraud, you know,

435
00:23:55.191 --> 00:23:56.150
slips right through the neck.

436
00:23:57.110 --> 00:24:01.130
So you can see how that's a system where in theory it's great,

437
00:24:01.131 --> 00:24:03.200
but in practice it's, it's, it really,

438
00:24:03.260 --> 00:24:06.620
it really doesn't work and it's full of holes and interrupt.

439
00:24:08.140 --> 00:24:10.300
<v 0>Because you've, um, we've been talking about all the serious stuff,</v>

440
00:24:10.301 --> 00:24:13.390
and I want to give a sense that your book's really good, fun as well.

441
00:24:13.630 --> 00:24:16.810
And the section on peer review is, uh, you, uh,

442
00:24:17.530 --> 00:24:21.190
you talk about how I hadn't realized that peer review was quite a new thing. Um,

443
00:24:21.520 --> 00:24:25.090
and that when Albert Einstein got peer reviewed here,

444
00:24:25.360 --> 00:24:30.210
he announced he was drawing his paper from consideration because that

445
00:24:30.211 --> 00:24:34.310
send it through another physicist. We comment. And then you look at, uh, you,

446
00:24:34.311 --> 00:24:38.410
you show some real peer review comments and you can sort of sympathize with

447
00:24:38.620 --> 00:24:43.030
Einstein. Um, what, my examples, you quote,

448
00:24:43.060 --> 00:24:46.210
the manuscript makes three claims. The first we've known for years,

449
00:24:46.390 --> 00:24:50.200
the second for decades, the third for centuries, or my,

450
00:24:50.880 --> 00:24:54.670
my favorite I'm afraid this manuscript may contribute not so much towards the

451
00:24:55.000 --> 00:24:57.310
field's advancement as much towards it's eventual.

452
00:24:59.620 --> 00:25:00.453
These are these glorious.

453
00:25:01.090 --> 00:25:05.380
<v 1>Great quotes, genuine quotes from peer reviews and, um, uh, that they, um,</v>

454
00:25:05.470 --> 00:25:05.860
you can,

455
00:25:05.860 --> 00:25:08.230
you can imagine getting that it's still demoralizing when you get that sort of

456
00:25:08.231 --> 00:25:10.660
comment in your, in your, in your review. Um, but you know,

457
00:25:10.661 --> 00:25:12.280
I don't know if we want it to be any other way,

458
00:25:12.281 --> 00:25:15.310
because we want science to be rigorously, you know, analyzed.

459
00:25:15.311 --> 00:25:18.640
And we want people to be, to be honest and peer reviews. And that's one of the,

460
00:25:19.300 --> 00:25:22.330
it's both the upside and the downside of peer reviews is that you can be honest

461
00:25:22.331 --> 00:25:26.050
if you're, uh, you know, sometimes, uh, in know, journey junior academics,

462
00:25:26.051 --> 00:25:29.530
even sometimes PhD students are asked to review papers and they can say,

463
00:25:29.550 --> 00:25:31.630
because they're anonymous, they can say,

464
00:25:31.870 --> 00:25:35.010
actually this paper by this really big professor,

465
00:25:35.020 --> 00:25:38.980
this really important professor at presented university is completely bad for

466
00:25:38.981 --> 00:25:41.980
all these reasons is, is, is terrible for all these reasons. And,

467
00:25:41.981 --> 00:25:44.050
but of course the downside of peer review is exactly the same thing,

468
00:25:44.051 --> 00:25:47.860
which is that people can trash papers that they disagree with because the theory

469
00:25:47.861 --> 00:25:51.490
doesn't fit with their preconceptions or whatever. So you get these, all these,

470
00:25:51.850 --> 00:25:53.650
um, these hilarious, uh, comments,

471
00:25:53.651 --> 00:25:55.420
and I've had plenty of them myself over the years,

472
00:25:55.421 --> 00:25:57.880
and I've given some over the years, uh,

473
00:25:57.940 --> 00:26:01.090
just you and one tries to control himself, but, um,

474
00:26:01.330 --> 00:26:04.990
sometimes you see something that's just so daft that you can't, uh, you can't,

475
00:26:05.290 --> 00:26:07.750
um, uh, control yourself. So, so yeah, so that's one of the,

476
00:26:07.780 --> 00:26:10.430
one of the problems is with peer review. Um, uh,

477
00:26:10.510 --> 00:26:14.370
but to go back to your previous question, the, the,

478
00:26:14.420 --> 00:26:18.010
the overall problems that I identified in the blue car, or with th the, the,

479
00:26:18.190 --> 00:26:20.740
the, the, the general setup of academia,

480
00:26:20.770 --> 00:26:23.290
which is that the incentives are all wrong.

481
00:26:23.320 --> 00:26:26.710
The incentives are towards publishing more papers. You know, I mentioned you,

482
00:26:26.711 --> 00:26:29.440
you get that line on your CV. That's what people want.

483
00:26:29.441 --> 00:26:32.350
That's what people are looking for in sciences is getting a longer CD,

484
00:26:32.530 --> 00:26:34.690
having a paper published, especially if it's published in a,

485
00:26:34.870 --> 00:26:38.830
in a really glamorous a journal, like a really, really great outlet,

486
00:26:38.920 --> 00:26:42.610
a great publication, like nature or science or places like that. And, um,

487
00:26:42.760 --> 00:26:46.770
and they're not necessarily incentivized towards getting true results,

488
00:26:46.771 --> 00:26:50.190
getting results that are real gain results that are going to stand the test of

489
00:26:50.191 --> 00:26:53.730
time, and that are robust to, uh, other, other people's, um,

490
00:26:53.960 --> 00:26:58.650
you're trying to replicate or extend them. So the whole system of universities,

491
00:26:58.651 --> 00:27:02.580
that's how universities reward researchers. They, they, they promote, uh,

492
00:27:02.581 --> 00:27:06.810
and give tenure to researchers who have lots of papers on their CV. Um, they,

493
00:27:07.560 --> 00:27:11.580
uh, funders are more likely to give you, you know, uh, funding,

494
00:27:11.581 --> 00:27:15.120
if it looks like your ideas are flashy and exciting and so on.

495
00:27:15.360 --> 00:27:18.900
And they're less likely to be interested in, you know, SLU,

496
00:27:18.960 --> 00:27:22.380
replication studies building up a foundation and so on.

497
00:27:22.470 --> 00:27:25.560
So the same kind of incentives that apply to, um, uh,

498
00:27:25.590 --> 00:27:28.500
journalists who are looking for exciting stories, as you, as you say,

499
00:27:28.800 --> 00:27:33.600
apply exactly the same to, to universities, funders, journals, the whole,

500
00:27:33.660 --> 00:27:37.950
the whole system that's around science. And that really knocks it off track of,

501
00:27:38.070 --> 00:27:40.680
you know, which is it's meant to be finding things that are true.

502
00:27:40.950 --> 00:27:42.870
It knocks it off towards, you know,

503
00:27:42.900 --> 00:27:46.200
just publishing stuff and publishing it in glamorous places.

504
00:27:47.480 --> 00:27:50.690
<v 0>So how do we, how do we fix this.</v>

505
00:27:52.330 --> 00:27:54.680
<v 1>Different things? Uh, so, um,</v>

506
00:27:54.710 --> 00:27:57.740
one of the things we we've previously mentioned is the clinical trials,

507
00:27:57.741 --> 00:28:00.950
for instance, are, uh, you have to register clinical trials before you,

508
00:28:00.951 --> 00:28:01.784
before you start.

509
00:28:01.790 --> 00:28:05.960
So one thing would be for scientists to start writing down their analysis before

510
00:28:05.961 --> 00:28:09.680
they, before they really do them, um, uh, writing,

511
00:28:10.040 --> 00:28:12.680
writing down and a proper detailed analysis plan.

512
00:28:12.740 --> 00:28:16.790
Like I think most people would assume scientists would be doing, but they don't,

513
00:28:17.600 --> 00:28:19.370
um, uh, and putting it somewhere public and saying,

514
00:28:19.371 --> 00:28:21.950
this is what we're going to do. And if we do anything different than, you know,

515
00:28:22.730 --> 00:28:25.280
you can see, you can see that we've changed the plan there.

516
00:28:25.370 --> 00:28:27.290
You can see that we've, we've kind of, we've, uh,

517
00:28:27.350 --> 00:28:29.960
we've been unable to resist running another analysis, like in the,

518
00:28:30.110 --> 00:28:33.920
like in the pizza restaurant, one where the person is just doing the same, uh,

519
00:28:33.950 --> 00:28:37.490
doing, doing, analyzing the same dataset over and over and over again. Um,

520
00:28:37.580 --> 00:28:40.760
so pre-registering analyses like that is something that I think most sciences

521
00:28:40.761 --> 00:28:43.970
can learn from clinical trials. We can learn that, that that's a really good,

522
00:28:44.000 --> 00:28:48.470
that's really good idea. Um, journals can change the way they, uh, uh,

523
00:28:48.590 --> 00:28:51.830
reward or their publication strategies.

524
00:28:51.840 --> 00:28:56.750
There are no publication strategies where scientists can send in, uh, um,

525
00:28:57.140 --> 00:28:58.590
an introduction, uh,

526
00:28:58.970 --> 00:29:02.930
kind of introductory section and their planned method and get it reviewed before

527
00:29:03.080 --> 00:29:07.580
any data are collected. And the journal agrees to publish the paper,

528
00:29:07.670 --> 00:29:11.510
whether or not the results are positive or not. So that takes, um,

529
00:29:11.750 --> 00:29:14.000
that takes away all the problems of publication bias,

530
00:29:14.001 --> 00:29:17.750
because you've agreed beforehand that you're going to publish this paper no

531
00:29:17.751 --> 00:29:21.500
matter how it turns out. And then you go off and do the actual data collection.

532
00:29:21.501 --> 00:29:23.780
And so journalists can change the way they do that.

533
00:29:24.080 --> 00:29:26.270
Scientists can change how open and transparent they are.

534
00:29:26.270 --> 00:29:28.550
They can start sharing their data with each other at the moment,

535
00:29:28.670 --> 00:29:31.100
if you email scientists and ask for the data, say,

536
00:29:31.280 --> 00:29:35.060
they're normally extremely reluctant to give you that, um, in some cases,

537
00:29:35.061 --> 00:29:38.420
because they worry that they've made a mistake in some cases,

538
00:29:38.421 --> 00:29:40.820
because it's just a mess that they have an annotated it properly,

539
00:29:40.821 --> 00:29:44.800
and that would be able to understand, um, except themselves, um,

540
00:29:44.860 --> 00:29:47.410
which I think is not really an adequate excuse for not sharing your data,

541
00:29:47.411 --> 00:29:51.520
but it's a very common, uh, thing. Um, so that's another, that's another thing.

542
00:29:51.940 --> 00:29:53.830
Um, universities can change the way they hire people.

543
00:29:53.831 --> 00:29:57.340
They can hire scientists who are open and who, uh,

544
00:29:57.370 --> 00:30:01.210
contribute data and contribute tools for people to use rather than just hiring

545
00:30:01.211 --> 00:30:03.250
people who have long CVS. Um,

546
00:30:03.251 --> 00:30:05.590
so there are lots of different changes that can be made at different levels in

547
00:30:05.591 --> 00:30:07.450
your bottom up and top down changes that can meet.

548
00:30:07.451 --> 00:30:11.860
And some of them are being made. Um, I am somewhat optimistic. Um, but, um,

549
00:30:11.920 --> 00:30:16.630
I have to say that, uh, certainly in recent times with the COVID-19 pandemic,

550
00:30:17.410 --> 00:30:21.790
um, I am, um, seeing a lot of the problems kind of, uh, uh, come,

551
00:30:21.791 --> 00:30:23.620
come up again and again and again, um,

552
00:30:23.650 --> 00:30:25.750
despite all these changes that people have tried to make.

553
00:30:26.400 --> 00:30:28.440
<v 0>So we've, I mean, we've, COVID, um,</v>

554
00:30:28.470 --> 00:30:33.390
we've had the greatest explosion of scientific research, I think,

555
00:30:33.391 --> 00:30:36.060
in, in human history. Um,

556
00:30:36.120 --> 00:30:39.090
it's been a massive global collaborative effort. What,

557
00:30:39.091 --> 00:30:42.180
what are your worries about the way in which this has been done?

558
00:30:43.590 --> 00:30:46.620
<v 1>Um, I think the main worry is that it's being done really quickly. Um,</v>

559
00:30:46.680 --> 00:30:50.340
and one of the biggest problems with that publish or perish culture that I was

560
00:30:50.341 --> 00:30:54.210
just describing is that, is that people do research too fast.

561
00:30:54.360 --> 00:30:55.620
They peer review research too fast.

562
00:30:55.960 --> 00:31:00.540
They publish research too fast and allows all sorts of mistakes and,

563
00:31:00.870 --> 00:31:03.840
and corner cutting to, to, to come in. Um,

564
00:31:03.930 --> 00:31:07.560
we just recently started a case where two papers were attracted from two of the

565
00:31:07.561 --> 00:31:09.240
top medical journals in the world,

566
00:31:09.300 --> 00:31:12.720
the land sets and the new England journal of medicine, because the,

567
00:31:12.740 --> 00:31:15.930
the Harvard researchers who published them, this was on the drug, uh,

568
00:31:15.960 --> 00:31:16.920
had dropped a chloroquine,

569
00:31:17.670 --> 00:31:21.210
the Harvard researchers who published them had not looked at the data that they

570
00:31:21.211 --> 00:31:25.350
had been provided by, um, a company who had apparently come up with these,

571
00:31:25.440 --> 00:31:29.040
with these data on patients who've been given hydroxychloroquine and it just

572
00:31:29.041 --> 00:31:32.100
rushed to publish the papers, uh, and, and, and get them out there in these,

573
00:31:32.130 --> 00:31:36.120
in these top journals, it's absolutely mind boggling.

574
00:31:36.120 --> 00:31:38.730
It turned out that the data had loads of had loads of problems,

575
00:31:38.731 --> 00:31:40.920
and that they'd been questioned in various ways.

576
00:31:41.190 --> 00:31:44.970
And it looks like they're going to have to be, um, uh, well,

577
00:31:45.390 --> 00:31:46.111
they've been retracted,

578
00:31:46.111 --> 00:31:49.380
but investigations are continuing and to exactly what has gone on with those,

579
00:31:49.381 --> 00:31:50.760
with those data. But, um,

580
00:31:50.790 --> 00:31:54.270
imagine that imagine publishing data in the top journals in the world without

581
00:31:54.300 --> 00:31:57.660
having really checked it yourself. And when you check it, uh,

582
00:31:57.690 --> 00:32:00.270
the person that gave you, it says, Oh, sorry, I can't,

583
00:32:00.390 --> 00:32:02.840
I can't provide you with that idea, but that's astonishing. And that's a,

584
00:32:02.841 --> 00:32:06.780
that's an example of how the haste that the pandemic has given us has really,

585
00:32:07.470 --> 00:32:11.250
um, pushed her, you know, science, uh, off a cliff in, in, in, in many ways.

586
00:32:11.251 --> 00:32:14.940
We've also seen really Georgie research on masks. Uh,

587
00:32:14.970 --> 00:32:16.440
we've seen dodgy research on other,

588
00:32:16.470 --> 00:32:17.940
other drugs that are related to the varsity scenes,

589
00:32:18.060 --> 00:32:21.660
all the research on the genetics of the virus, um, uh, on,

590
00:32:21.661 --> 00:32:24.600
on the psychology of the, of the, of the virus. Um,

591
00:32:24.930 --> 00:32:28.170
and a lot of it comes from just people thinking, Oh, I can be helpful here.

592
00:32:28.171 --> 00:32:32.820
My field is, you know, related in some way to the, to the pandemic. So I'll,

593
00:32:32.821 --> 00:32:33.960
I'll publish another paper. I mean,

594
00:32:34.860 --> 00:32:37.920
a lot of the research is not necessarily damaging in any way. It's just,

595
00:32:38.010 --> 00:32:42.800
it's just useless. Or like, like, did we really need the paper that was, um, uh,

596
00:32:42.801 --> 00:32:47.100
about how COVID relates to the finger length, but the, the,

597
00:32:47.101 --> 00:32:50.090
the relation between your second and fourth finger, the ratio of, of lent,

598
00:32:50.091 --> 00:32:52.340
like there's a paper published in the literature on that too.

599
00:32:53.020 --> 00:32:55.690
<v 0>Do we really need, but how does it make a great story?</v>

600
00:32:58.150 --> 00:32:59.020
<v 1>No, I can't even remember.</v>

601
00:32:59.021 --> 00:33:02.350
I can't even recall a result of it cause I was just like, Oh, w when I saw it,

602
00:33:02.351 --> 00:33:04.680
you know, it's a, it's a common method that is used in, in, in,

603
00:33:04.900 --> 00:33:08.870
in testosterone and so on. And it's been kind of debated, but like,

604
00:33:09.040 --> 00:33:11.770
it's not really something new. And, um, uh, you know,

605
00:33:11.771 --> 00:33:14.860
there's loads of psychology studies that have come out hundreds of studies,

606
00:33:14.861 --> 00:33:19.210
which are things like we've developed a new questionnaire to ask people how

607
00:33:19.211 --> 00:33:22.570
scared they are of COVID and here we tested it in our sample, in our sample.

608
00:33:22.571 --> 00:33:26.290
Understandable. I mean, fair enough, but there's, but there's this huge, um, uh,

609
00:33:26.760 --> 00:33:30.250
uh, upsurge of research, as you say. Um, a lot of it is, is waste.

610
00:33:30.580 --> 00:33:35.310
A lot of it is, uh, is, is, is, uh, is, is done too quickly. Um, and,

611
00:33:35.311 --> 00:33:37.780
and a lot of it is just playing wrong, unfortunately. So it's kind of,

612
00:33:37.840 --> 00:33:39.550
it's not really showing science in very good late.

613
00:33:40.390 --> 00:33:43.910
<v 0>And to what extent, I mean, you may, you make quite a powerful case and,</v>

614
00:33:44.020 --> 00:33:46.760
you know, in some of the fields, you'll find, we're finding when,

615
00:33:46.761 --> 00:33:49.630
when we do these analyses, um, you know,

616
00:33:49.920 --> 00:33:53.260
it's picking up research, that's bad, fraudulent,

617
00:33:53.261 --> 00:33:57.340
doesn't replicate at very, very significant sensors. I mean,

618
00:33:57.610 --> 00:34:01.420
percentages that, you know, 50% plus in some cases, um,

619
00:34:01.480 --> 00:34:04.810
why are humans making progress at all? If this is the case,

620
00:34:04.830 --> 00:34:08.350
it feels like there's a bit missing from this picture that despite these

621
00:34:08.351 --> 00:34:12.610
imperfections, we are bumbling forwards and doing our thing. Yeah.

622
00:34:12.890 --> 00:34:14.940
<v 1>Shouldn't we shouldn't, uh, you know, push the, the,</v>

623
00:34:14.941 --> 00:34:18.250
the argument too far and say that, you know, all the, all the, everything is,

624
00:34:18.310 --> 00:34:20.500
uh, is untrustworthy and so on. It's just,

625
00:34:20.590 --> 00:34:25.480
it's just that we would be making such better progress if we could get over

626
00:34:25.510 --> 00:34:27.220
these problems that we have in literature,

627
00:34:27.460 --> 00:34:30.760
we would be making such quicker progress on diseases like COVID,

628
00:34:30.761 --> 00:34:33.460
but also all the other diseases that we study we'd be making much better

629
00:34:33.461 --> 00:34:36.610
progress, but understanding the world and understanding space and understanding,

630
00:34:37.420 --> 00:34:40.030
uh, the human mind and all the other stuff that we're interested in,

631
00:34:40.270 --> 00:34:43.690
if we didn't have all these things are slowing the progress of science, then,

632
00:34:44.440 --> 00:34:48.640
um, uh, all these academic issues that we could, we could solve, you know, by,

633
00:34:48.700 --> 00:34:52.390
by changing the incentive structure by changing the rules. Um, although, I mean,

634
00:34:52.720 --> 00:34:53.770
issues like fraud,

635
00:34:53.771 --> 00:34:57.010
people making results up and so on will always be with us because humans are

636
00:34:57.011 --> 00:35:01.750
humans and they occasionally do these kinds of things, but yeah, um,

637
00:35:01.810 --> 00:35:05.380
we clearly are still making progress and I wouldn't want the book to be, um,

638
00:35:05.410 --> 00:35:06.160
you know,

639
00:35:06.160 --> 00:35:10.090
use if I rate the writers in the book as an argument against science in general,

640
00:35:10.660 --> 00:35:14.800
uh, I don't want it to be used for people who are, you know, anti-vaccine or,

641
00:35:15.010 --> 00:35:16.990
or, uh, against, uh,

642
00:35:17.140 --> 00:35:20.020
the idea that there's manmade climate change or whatever,

643
00:35:20.170 --> 00:35:23.740
whatever weird unscientific idea people want to have.

644
00:35:24.100 --> 00:35:26.920
The idea is that we should raise our standards across the board, um,

645
00:35:26.921 --> 00:35:30.720
and that we should raise our standards for all science, including the, you know,

646
00:35:30.721 --> 00:35:34.990
the stuff that's published in the very top journals. Um, and so, uh, uh,

647
00:35:35.290 --> 00:35:37.410
a lot of these ideas, like, you know, climate,

648
00:35:37.560 --> 00:35:42.090
climate denial and anti-vaccination and creationism or whatever it happens to

649
00:35:42.091 --> 00:35:45.990
be, wouldn't stand up to this new, uh, raised really standard anyway.

650
00:35:45.991 --> 00:35:49.500
So it's not like people who are critics of science from a kind of unscientific

651
00:35:49.501 --> 00:35:52.050
perspective will get much out of this. I hope.

652
00:35:52.160 --> 00:35:56.900
<v 0>Yeah. Yeah. One of the causes for hope, um,</v>

653
00:35:57.290 --> 00:35:58.550
that I found on reading,

654
00:35:58.551 --> 00:36:03.500
it was the ingenuity of some of these tests that are able

655
00:36:03.501 --> 00:36:06.500
to look at papers on mass and,

656
00:36:06.940 --> 00:36:09.560
and there are loads of them they're quite complicated to go and do some of them,

657
00:36:09.561 --> 00:36:13.070
but look at papers on mass and find out just even from what's published in the

658
00:36:13.071 --> 00:36:15.620
paper automatically, or there's a problem here.

659
00:36:15.980 --> 00:36:20.420
And one of them that I felt was it was very clever and just about

660
00:36:20.421 --> 00:36:24.380
understandable, um, and also had a brilliant acronym, uh,

661
00:36:24.381 --> 00:36:28.070
was the grim test. Are you able to describe this?

662
00:36:28.071 --> 00:36:30.740
So as I was actually looking at papers and just trying to find the ones that are

663
00:36:30.741 --> 00:36:31.574
a bit iffy.

664
00:36:31.880 --> 00:36:32.601
<v 1>I can do my best. Yeah.</v>

665
00:36:32.601 --> 00:36:36.320
So this is the granularity related inconsistency of means test.

666
00:36:36.330 --> 00:36:40.190
So grim for sure. It's, it's a great, it's a great name. And this is by, uh, by,

667
00:36:40.400 --> 00:36:42.950
uh, uh, Nick Brown and James Heathers who are, um,

668
00:36:42.980 --> 00:36:47.870
kind of data sleuths who look into, uh, dodgy, dodgy papers.

669
00:36:48.230 --> 00:36:48.530
Um, well,

670
00:36:48.530 --> 00:36:51.470
they look into all papers and they discover a few dodgy ones occasionally. Um,

671
00:36:51.620 --> 00:36:55.580
but what this does, is it discovers, um, uh, so often in,

672
00:36:55.760 --> 00:36:59.480
particularly in areas like psychology, you've given people a scale,

673
00:36:59.630 --> 00:37:04.220
maybe from zero to five of like, I don't know how happy they are with their,

674
00:37:04.760 --> 00:37:07.340
how happy they are with their job or happy they are with their family life

675
00:37:07.370 --> 00:37:09.080
around the city and they'll rate that.

676
00:37:09.380 --> 00:37:12.380
But it turns out that there are only certain ways say that you have a sample of

677
00:37:12.381 --> 00:37:15.140
20 people that you've given that, that test to,

678
00:37:15.290 --> 00:37:18.470
there are only certain numbers that can come out of that. If you average,

679
00:37:18.530 --> 00:37:20.120
if you take the average score.

680
00:37:20.390 --> 00:37:24.710
So like maybe it's possible that you could get a score of 3.49,

681
00:37:25.070 --> 00:37:29.360
but it wouldn't be possible to get a score of 3.47 or a score of 3.4. I'm not,

682
00:37:29.430 --> 00:37:31.850
I would need to do the math for the second.

683
00:37:32.060 --> 00:37:32.990
<v 0>Because it's, while you're trying to stand it,</v>

684
00:37:32.991 --> 00:37:34.970
I think it's just with two people with two people,

685
00:37:34.971 --> 00:37:36.370
you can either get a whole or.

686
00:37:36.680 --> 00:37:38.510
<v 1>Right, right. That's true. That's true. Yeah. So if you,</v>

687
00:37:38.511 --> 00:37:41.090
if you have two people in your average score, you can either get, um,

688
00:37:41.420 --> 00:37:45.170
point 0.5 or you can get 0.0 at the end of the number. There's no other,

689
00:37:45.350 --> 00:37:49.730
if you get, if you get a 0.37, uh, after, after the,

690
00:37:49.760 --> 00:37:53.490
after the number you've done something wrong, there's something wrong with this,

691
00:37:53.491 --> 00:37:57.650
with the analysis that you've done or the calculation that you've done. So, um,

692
00:37:57.710 --> 00:38:02.000
what they do is they look through the averages that are published in, in, uh,

693
00:38:02.360 --> 00:38:04.790
in, in papers and the tables and papers.

694
00:38:04.791 --> 00:38:06.740
So they don't even need access to the raw data. They just need,

695
00:38:06.741 --> 00:38:10.940
the average is published in paper. Um, and they say, well, wait a minute,

696
00:38:10.941 --> 00:38:13.940
that number is impossible. If it, either,

697
00:38:13.941 --> 00:38:15.710
either you've described the wrong sample size,

698
00:38:15.711 --> 00:38:18.830
so that's a mistake or you've described the wrong average. So that's a mistake,

699
00:38:19.040 --> 00:38:21.800
or you've just described your questionnaire completely incorrectly.

700
00:38:21.801 --> 00:38:25.280
So that's some mistakes, so something must be wrong. And in some of these cases,

701
00:38:25.670 --> 00:38:29.570
uh, it's laid on to, um, discoveries of actual fraud. Like the people,

702
00:38:29.600 --> 00:38:32.540
the reason that the numbers are, uh, are, are, are not,

703
00:38:32.590 --> 00:38:36.310
are not possible is because they're made up. Um, and the,

704
00:38:36.400 --> 00:38:38.950
and the fraudster didn't make them up in such a way that made them look

705
00:38:38.951 --> 00:38:42.430
realistic, which is, you know, something was to love fraudsters for gay is that,

706
00:38:42.670 --> 00:38:44.110
is that you need to actually meet your data,

707
00:38:44.111 --> 00:38:46.570
look as if they came about naturally. And not that they were, you know,

708
00:38:46.571 --> 00:38:51.070
made by a human being. Um, uh, uh, and, um,

709
00:38:51.100 --> 00:38:54.160
in some cases it's just a mistake. It's an error. It's just, uh, you know,

710
00:38:54.161 --> 00:38:56.860
someone's typed the wrong data into the spreadsheet,

711
00:38:56.861 --> 00:39:00.100
or they've run the wrong calculation, or they've made a copy and paste their,

712
00:39:00.101 --> 00:39:03.070
when they were like taking our data from their statistical program into their

713
00:39:03.071 --> 00:39:05.200
word document, which they were writing their scientific paper in,

714
00:39:05.500 --> 00:39:07.600
which seems like such a kind of low grade mistake,

715
00:39:07.601 --> 00:39:11.230
but it happens all the time in science. Um, and so, yeah,

716
00:39:11.231 --> 00:39:14.080
that's probably the grim test can really be, can really be useful.

717
00:39:14.081 --> 00:39:16.690
And there are very similar tests that are related to that. Um,

718
00:39:16.720 --> 00:39:19.600
some of which also have good acronyms, which had, um,

719
00:39:20.410 --> 00:39:23.500
James Henderson and Nick Cronin have come up with that, um, that can,

720
00:39:23.530 --> 00:39:26.440
that can do similar things for other types of data. You know,

721
00:39:26.441 --> 00:39:29.650
this one works for questionnaires where you have zero to five or whatever,

722
00:39:29.800 --> 00:39:33.070
but there are other types of data that we would want to do something. And the,

723
00:39:33.240 --> 00:39:35.860
and there are other algorithms that kind of check, um,

724
00:39:35.890 --> 00:39:38.350
the P values that we were discussing before. You know, have you written,

725
00:39:38.440 --> 00:39:40.720
have you written down the right P value in your study? If not, again,

726
00:39:40.721 --> 00:39:44.300
it might be, you've made a mistake somewhere. Um, and, and,

727
00:39:44.301 --> 00:39:47.860
and various other ways of, um, of looking and just saying,

728
00:39:48.010 --> 00:39:51.490
does this data look as if it came about through natural normal means,

729
00:39:51.670 --> 00:39:54.370
or does it look as if it's been tampered with, or if it's,

730
00:39:54.760 --> 00:39:55.750
or if it's just mistaken.

731
00:39:56.430 --> 00:40:00.330
<v 0>Yeah. Yeah. And do you think that, um,</v>

732
00:40:01.020 --> 00:40:05.670
your opinion is mainstream, does it anger, scientists,

733
00:40:05.940 --> 00:40:07.890
do you think it's being taken notice of,

734
00:40:08.580 --> 00:40:11.640
is everyone getting their house in order to the extent that psychology seems to

735
00:40:11.641 --> 00:40:13.200
be? I mean, there was.

736
00:40:13.390 --> 00:40:17.010
<v 1>Huge debate in psychology. Uh, when, you know, when the,</v>

737
00:40:17.210 --> 00:40:20.970
the replication crisis started coming about and people started doing replication

738
00:40:21.000 --> 00:40:23.910
attempts and so on, and there were psychologists who came along and said,

739
00:40:23.911 --> 00:40:27.000
this is useless. You're not doing the right thing. You're not running my,

740
00:40:27.030 --> 00:40:28.890
you know, you're not running my experiment correctly,

741
00:40:28.891 --> 00:40:30.960
so no wonder you're gonna not get a replication of it.

742
00:40:30.961 --> 00:40:32.610
You changed one tiny aspect of it.

743
00:40:32.611 --> 00:40:36.180
And so how do you expect to find the same result and their response to that as

744
00:40:36.181 --> 00:40:36.660
well?

745
00:40:36.660 --> 00:40:40.290
If the result doesn't survive one tiny aspect being changed in the experiment,

746
00:40:40.291 --> 00:40:43.620
then probably it wasn't that important to resolve in the first place. Um, but,

747
00:40:43.800 --> 00:40:48.570
um, but the, um, uh, across other fields, I, again, there's cause for optimism,

748
00:40:48.720 --> 00:40:50.400
because I think a lot of people are catching onto this.

749
00:40:50.700 --> 00:40:52.920
A lot of journals are starting to change. Um, you know,

750
00:40:53.040 --> 00:40:56.280
the idea where you submit your first part of your paper before you,

751
00:40:56.370 --> 00:40:58.740
before you collect the data that's being instituted at lots and lots of

752
00:40:58.741 --> 00:41:02.910
journals, not just in psychology, it's called the registered reports. Um,

753
00:41:02.940 --> 00:41:05.640
and it's, it's, it's, it's being instituted, um,

754
00:41:05.700 --> 00:41:08.700
lots and lots and lots of journals learn in those different fields. Um,

755
00:41:08.730 --> 00:41:13.170
open science data sharing is becoming a thing I think in many respects because

756
00:41:13.171 --> 00:41:16.800
the technology is, is, is, is better now. I mean, just a few years ago,

757
00:41:17.010 --> 00:41:21.990
you had to actually physically fit all your information on pages of paper,

758
00:41:22.440 --> 00:41:25.950
uh, that were in the journal. No, of course you can share data much more easily.

759
00:41:25.951 --> 00:41:28.490
There are repositories online that you can upload your data set to,

760
00:41:28.540 --> 00:41:31.500
you can download other people's data, run the analysis again,

761
00:41:31.501 --> 00:41:34.880
you can put all your code up there, you know, run everything. So, you know,

762
00:41:35.090 --> 00:41:39.110
the technology is, is, is no barrier. So more fields can actually, you know,

763
00:41:39.140 --> 00:41:41.600
make the change. It's one thing that I discuss in the book is that,

764
00:41:41.750 --> 00:41:45.620
is that making, getting people on board with these changes is one thing,

765
00:41:45.770 --> 00:41:50.180
but making it easier for them to do it is, is, is really the, is the best way.

766
00:41:50.720 --> 00:41:54.620
And, um, uh, yeah, so you do see these,

767
00:41:54.680 --> 00:41:57.110
these changes and you see other fields, um, you know,

768
00:41:57.111 --> 00:41:59.930
so ecology and evolution is one that's really going to come on board.

769
00:41:59.931 --> 00:42:01.550
You see a lot of papers saying, Hmm,

770
00:42:01.580 --> 00:42:04.550
all these psychologists might have a point that we're doing all these incorrect

771
00:42:04.551 --> 00:42:06.050
analysis. Maybe we should look at our own fields.

772
00:42:06.200 --> 00:42:08.930
I saw a paper in criminology that did the same thing that said, Oh,

773
00:42:08.960 --> 00:42:12.950
how much of this criminology research is actually is actually not useful at all.

774
00:42:13.470 --> 00:42:17.240
And so, um, uh, you know, you're starting to see, you're starting to see these,

775
00:42:17.670 --> 00:42:18.503
uh, uh,

776
00:42:19.370 --> 00:42:22.460
issues being discussed in lots of different fields and actually someone that

777
00:42:22.461 --> 00:42:26.090
some of them, sometimes they had to just have different words to, to use it.

778
00:42:26.091 --> 00:42:29.060
So in medicine, instead of P hacking,

779
00:42:29.270 --> 00:42:31.790
they talk about outcome switching in clinical trials,

780
00:42:31.791 --> 00:42:35.150
which is where your trial was actually, um, was planned to be about.

781
00:42:35.810 --> 00:42:39.230
I had pill that reduces headache, but actually it turned out that it reduced,

782
00:42:40.610 --> 00:42:44.660
uh, I dunno, anxiety. And so you say, uh, Oh, uh,

783
00:42:44.800 --> 00:42:46.580
we were always doing a study on anxiety all along,

784
00:42:46.581 --> 00:42:49.040
and that's basically what P hacking is, but it's just called a different name.

785
00:42:49.041 --> 00:42:49.821
So, you know, there's,

786
00:42:49.821 --> 00:42:52.880
there's kind of convergent discussions across different fields of all these

787
00:42:52.881 --> 00:42:54.050
problems. Yeah.

788
00:42:54.310 --> 00:42:56.260
<v 0>Yeah. Well, um,</v>

789
00:42:56.320 --> 00:43:01.270
I think we've reached the end of the time of the podcast. Um, but, uh,

790
00:43:01.300 --> 00:43:03.400
that's at least something to hopeful for the future.

791
00:43:03.760 --> 00:43:06.940
And if people want to find out more and genuinely it's, uh,

792
00:43:06.960 --> 00:43:11.830
it's entertaining and it's not all stats and there's a lot of fun

793
00:43:11.831 --> 00:43:12.664
stuff in it.

794
00:43:13.240 --> 00:43:17.230
<v 1>Science fiction is currently out now. Thanks so much. Thanks for having me on.</v>

