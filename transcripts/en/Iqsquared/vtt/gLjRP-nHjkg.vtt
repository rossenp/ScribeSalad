WEBVTT

1
00:00:00.060 --> 00:00:03.120
<v 0>Hi there everyone I'm Carmella and academic and research.</v>

2
00:00:03.121 --> 00:00:06.270
And I'm here today with Nina shack, a political advisor,

3
00:00:06.300 --> 00:00:10.050
broadcaster and author of the new book, deep fakes,

4
00:00:10.110 --> 00:00:12.930
and in Fokker lips, I think that's how you describe it.

5
00:00:13.420 --> 00:00:14.730
I'm an expiration of fake news.

6
00:00:14.731 --> 00:00:19.080
AI generated deep fakes and the dangerous political consequences of our

7
00:00:19.081 --> 00:00:24.000
discipline is information, age welcoming. So, um,

8
00:00:24.480 --> 00:00:26.010
let's, let's begin, um,

9
00:00:26.040 --> 00:00:30.300
with kind of staring down the kind of technological black hole that you begin

10
00:00:30.301 --> 00:00:35.220
the book with this, this staring into the world of deep fakes. Um,

11
00:00:35.340 --> 00:00:36.173
what does it look like?

12
00:00:37.250 --> 00:00:41.690
<v 1>So I came across the deep, fake only about three years ago when they first,</v>

13
00:00:41.750 --> 00:00:44.450
less than three years ago when they first erupted on the internet.

14
00:00:44.900 --> 00:00:49.400
And for those who don't know, and many listeners will know what a deep fake is.

15
00:00:49.401 --> 00:00:54.320
Essentially an AI generated either partially or wholly fake piece of

16
00:00:54.321 --> 00:00:58.520
media content. It could be a video, it can be an image, it can also be texts.

17
00:00:59.090 --> 00:01:03.800
And I came to it as somebody who was advising the

18
00:01:03.801 --> 00:01:05.210
former NATO secretary general,

19
00:01:05.211 --> 00:01:08.120
and we were working on election interference and disinformation,

20
00:01:08.121 --> 00:01:12.320
and what Russia was doing to really disrupt the politics of the Western world.

21
00:01:12.740 --> 00:01:17.720
So when I first found a deep fake on this very obscure area of

22
00:01:17.721 --> 00:01:22.220
the internet on Reddit and its first use was in non-consensual

23
00:01:22.221 --> 00:01:26.480
pornography, I was like, Oh my goodness, this is huge.

24
00:01:26.510 --> 00:01:29.330
Forget about fake news, forget about the disinformation,

25
00:01:29.600 --> 00:01:33.440
all the kinds of manipulated media we've seen before.

26
00:01:33.680 --> 00:01:38.300
This is the heralding of what's to come when you can actually fake

27
00:01:38.330 --> 00:01:41.900
video and make it look like it's the real deal.

28
00:01:41.901 --> 00:01:45.650
So immediately when it kind of came across it in pornography, I was like,

29
00:01:45.740 --> 00:01:49.880
this is going to have massive implications on everything from politics

30
00:01:50.390 --> 00:01:54.200
to also individuals. Um, and I kind of lay that out in the book,

31
00:01:54.201 --> 00:01:58.760
but there is the first kind of introduction as to like what a deep fake actually

32
00:01:58.850 --> 00:01:59.683
is.

33
00:02:00.140 --> 00:02:03.530
<v 0>And so most people thinking about this information probably in their minds,</v>

34
00:02:03.531 --> 00:02:08.030
I will recall maybe some dodgy tweets or Facebook posts, but,

35
00:02:08.180 --> 00:02:10.280
but you're, you're, you're kind of sounding the alarm,

36
00:02:10.460 --> 00:02:12.200
not about textual information,

37
00:02:12.201 --> 00:02:16.970
but about the world of video and pictures and the things that we actually see

38
00:02:17.000 --> 00:02:18.500
that seems so visceral to us.

39
00:02:19.190 --> 00:02:23.180
<v 1>Absolutely. So I think when I first came to deep fakes,</v>

40
00:02:23.210 --> 00:02:27.830
I recognized how potent they were because in our

41
00:02:27.831 --> 00:02:32.060
current information ecosystem, right, which is like changing so quickly,

42
00:02:32.300 --> 00:02:35.390
video has become the medium of communication.

43
00:02:35.780 --> 00:02:39.080
And if you look at how it's changed over the last 30 years,

44
00:02:39.081 --> 00:02:41.960
we first had the internet, then we had smartphones.

45
00:02:42.320 --> 00:02:46.460
Then we have social media and I come from a part of the world, Nepal, you know,

46
00:02:46.461 --> 00:02:48.440
part of the developing world where, um,

47
00:02:48.460 --> 00:02:52.160
until recently people were just kind of completely disconnected from this

48
00:02:52.161 --> 00:02:53.330
information ecosystem.

49
00:02:53.600 --> 00:02:58.160
But now everybody has a smartphone and everybody consumes

50
00:02:58.190 --> 00:03:02.650
information. Mostly through video. If you look at the stats for next year,

51
00:03:02.651 --> 00:03:07.630
I think it's over 82% of global internet traffic is going to be to the

52
00:03:07.631 --> 00:03:11.530
consumption of video content. So just at this time,

53
00:03:11.560 --> 00:03:16.270
then video is becoming the most important medium of human

54
00:03:16.271 --> 00:03:17.140
communication,

55
00:03:17.500 --> 00:03:22.390
which is going to reach parts of the globe in a way that no other form

56
00:03:22.450 --> 00:03:25.360
of human communication has ever done before.

57
00:03:25.900 --> 00:03:30.130
That is just the time when it can be subverted by AI. I should point out that,

58
00:03:30.550 --> 00:03:32.350
um, when I talk about deep fakes,

59
00:03:32.440 --> 00:03:36.400
it's essentially what I describe as a malicious use of synthetic media,

60
00:03:36.401 --> 00:03:39.490
synthetic media, being AI generated content.

61
00:03:39.700 --> 00:03:42.370
And there'll be loads of positive use cases. I mean,

62
00:03:42.371 --> 00:03:47.110
it's completely going to open up like the creative industries and advertising

63
00:03:47.111 --> 00:03:52.030
industries, but there will be bad use cases. If I, if I told you, Hey,

64
00:03:52.031 --> 00:03:55.780
if you're a criminal or you want to do some kind of political disinformation and

65
00:03:55.781 --> 00:03:59.650
you have the power to make any video or audio,

66
00:03:59.740 --> 00:04:03.490
any media of anyone saying or doing anything, you can see how that can,

67
00:04:03.520 --> 00:04:05.710
that technology can quickly be used by the bad guys.

68
00:04:05.711 --> 00:04:08.380
And I think what I wanted to do with this book,

69
00:04:08.381 --> 00:04:11.860
so the hook was the deep fake because I thought, wow,

70
00:04:11.890 --> 00:04:14.260
that's so fascinating when I first came across them.

71
00:04:14.740 --> 00:04:18.040
But increasingly as I started writing the book, I was like, well,

72
00:04:18.550 --> 00:04:20.140
there's something else going on here.

73
00:04:20.230 --> 00:04:25.090
And that is that our information ecosystem in the age of information has just

74
00:04:25.091 --> 00:04:27.490
become so dangerous and untrustworthy.

75
00:04:27.491 --> 00:04:32.350
And I think a part of that is because it has changed so exponentially with

76
00:04:32.351 --> 00:04:36.040
these advances that I've just described from internet to social media,

77
00:04:36.041 --> 00:04:38.680
to smartphones, to the coming age of AI,

78
00:04:38.710 --> 00:04:42.310
that society simply hasn't had time to adjust.

79
00:04:42.760 --> 00:04:47.650
And if you look at kind of the big changes in the ways that humans have

80
00:04:47.651 --> 00:04:50.320
communicated from the printing press to photography, I mean,

81
00:04:50.321 --> 00:04:53.020
there were 400 years between that, right?

82
00:04:53.021 --> 00:04:57.070
From the invention of Gutenberg printing press to the advent of modern

83
00:04:57.071 --> 00:05:01.060
photography. And I just don't think we're ready for what AI can do.

84
00:05:01.061 --> 00:05:05.650
I'm not saying technology is bad. I'm just saying it will have bad use cases.

85
00:05:05.880 --> 00:05:10.030
And my first thing that I wanted to do with writing this book is to talk about

86
00:05:10.031 --> 00:05:13.090
that. So people recognize it's not only coming,

87
00:05:13.091 --> 00:05:15.610
it's already here and we need to be able to recognize it.

88
00:05:16.740 --> 00:05:21.520
<v 0>And, and boiling down deep fakes, just so that people know. So, so in, in,</v>

89
00:05:21.521 --> 00:05:26.160
in very, very concrete times, what do they let people do cry? Any video,

90
00:05:26.190 --> 00:05:29.610
looking like anyone that people want, um,

91
00:05:29.640 --> 00:05:31.560
which is entirely synthetic.

92
00:05:32.280 --> 00:05:36.840
<v 1>So a deep fake is essentially a maliciously used piece of</v>

93
00:05:36.841 --> 00:05:37.800
synthetic media.

94
00:05:37.801 --> 00:05:42.270
So synthetic media is either wholly generated by AI or partially

95
00:05:42.271 --> 00:05:45.450
generated by AI. And we're still at the very,

96
00:05:45.451 --> 00:05:48.270
very beginning of the synthetic media revolution.

97
00:05:48.271 --> 00:05:51.780
So what we've seen so far with deep fakes and a lot of people who talk about

98
00:05:52.160 --> 00:05:56.460
defects are like, Oh my God, this is the end of democracy. It can make you say,

99
00:05:56.461 --> 00:05:59.900
or do make you take anyone or say, or do anything.

100
00:06:00.230 --> 00:06:04.850
But the reality is the barriers to entry right now are

101
00:06:04.851 --> 00:06:09.170
still quite high. However, they're coming down very, very quickly.

102
00:06:09.410 --> 00:06:13.760
So essentially what that means is that wholly would level special effects.

103
00:06:13.761 --> 00:06:16.850
You know, the type of things that you see in these blockbusters,

104
00:06:16.851 --> 00:06:19.910
which would previously have been inaccessible to anyone,

105
00:06:19.911 --> 00:06:23.180
except those with a lot of money, you know, a whole,

106
00:06:23.181 --> 00:06:27.680
a special effects studio that is basically going to become something that

107
00:06:27.681 --> 00:06:29.570
anybody can create or do.

108
00:06:30.020 --> 00:06:34.490
And the reason why they are dangerous or can be used

109
00:06:34.491 --> 00:06:38.540
dangerously is because of the AI element of it. Um,

110
00:06:38.541 --> 00:06:43.340
and this is where I kind of like go into the anecdotes in my story where,

111
00:06:43.970 --> 00:06:46.400
um, you can take content of anyone,

112
00:06:46.401 --> 00:06:50.480
so I can take your personal information and then create a deep fake of you

113
00:06:50.481 --> 00:06:54.350
either in a video form or I can steal your voice and I can make you say

114
00:06:54.351 --> 00:06:58.100
something with audio. And like I mentioned,

115
00:06:58.101 --> 00:07:00.980
this only started really coming about three years ago.

116
00:07:00.981 --> 00:07:04.250
And it's already getting to the point where it's quite advanced and in the next

117
00:07:04.251 --> 00:07:06.580
kind of 18 to 12 months, it's good.

118
00:07:06.620 --> 00:07:09.230
The barriers to entry are going to come down even further.

119
00:07:09.470 --> 00:07:12.290
So it's next level disinformation,

120
00:07:12.710 --> 00:07:17.360
but this is not only the domain of high geopolitics or, um,

121
00:07:17.900 --> 00:07:21.590
you know, the information war between China and Russia and the United States.

122
00:07:21.830 --> 00:07:25.370
This is something that's going to impact people in their very personal lives.

123
00:07:25.371 --> 00:07:26.720
It's something, uh, it's,

124
00:07:26.890 --> 00:07:31.490
it's a technology that already is and will continue to be misused by criminals.

125
00:07:31.850 --> 00:07:36.740
It's a technology that can be used by somebody who has a grudge

126
00:07:36.741 --> 00:07:38.930
against you, for example, by making, um,

127
00:07:38.960 --> 00:07:41.540
as we've already been seeing revenge porn. Um,

128
00:07:41.570 --> 00:07:46.040
so this is something that really has the possibility to impact

129
00:07:46.250 --> 00:07:49.130
everyone, not only on the level of the political,

130
00:07:49.310 --> 00:07:51.050
but even in our most personal lives.

131
00:07:51.760 --> 00:07:55.420
<v 0>And is this kind of all of that together? Is that the [inaudible]?</v>

132
00:07:55.570 --> 00:07:56.740
Did I pronounce that correctly?

133
00:07:57.520 --> 00:08:00.300
<v 1>Yeah, the apocalypse, it's just that a nice,</v>

134
00:08:00.310 --> 00:08:03.700
a word that I decided to kind of throw into the title. It was actually,

135
00:08:03.701 --> 00:08:08.530
it was actually a word that was coined by a U S technologist in

136
00:08:08.531 --> 00:08:13.330
2016 when he was kind of a warning of the era of fake

137
00:08:13.331 --> 00:08:18.250
news. Um, and essentially how I described the apocalypse is,

138
00:08:18.790 --> 00:08:23.410
um, our increasingly dangerous information ecosystem that's untrustworthy.

139
00:08:23.830 --> 00:08:28.750
And it's something that I think is going to continue to become more dangerous

140
00:08:28.780 --> 00:08:32.740
and more untrustworthy. So the InFocus lips I think, is going to advance.

141
00:08:33.040 --> 00:08:35.080
And within that, I think, um,

142
00:08:35.140 --> 00:08:38.650
AI and deep stakes are going to definitely play a role.

143
00:08:38.830 --> 00:08:41.470
So I think the main point,

144
00:08:41.471 --> 00:08:46.420
and this is what researchers like you also are thinking about Carl is how

145
00:08:46.450 --> 00:08:47.980
do we, um,

146
00:08:48.310 --> 00:08:52.060
change the infrastructure of our information ecosystem as to make it more

147
00:08:52.061 --> 00:08:56.280
trustworthy and safe, because it is so important, evermore.

148
00:08:56.280 --> 00:08:58.710
<v 0>Important? Well, there's lots of,</v>

149
00:08:58.890 --> 00:09:02.370
kind of stops that we might take on our tour through the [inaudible].

150
00:09:02.390 --> 00:09:05.820
So in the next 40 minutes or so, um, but let's begin with,

151
00:09:05.850 --> 00:09:07.470
I think perhaps the most obvious,

152
00:09:07.471 --> 00:09:11.550
the one foregrounded very recently in a big report in parliament that we've been

153
00:09:11.551 --> 00:09:15.120
talking about this for years, Russia, um,

154
00:09:16.350 --> 00:09:20.490
zoom us into one of these operations that the Russia is mounted.

155
00:09:20.491 --> 00:09:24.840
What does that actually look like when you begin to kind of tear it apart and

156
00:09:24.841 --> 00:09:26.070
kind of look at the innards?

157
00:09:27.380 --> 00:09:29.180
<v 1>So, I mean, I,</v>

158
00:09:29.210 --> 00:09:34.010
my background is somebody who was started working in

159
00:09:34.011 --> 00:09:38.750
like national security and European politics in, at the start of,

160
00:09:38.840 --> 00:09:39.800
for about 10 years.

161
00:09:40.370 --> 00:09:44.960
And I was really at the front lines of watching what was happening

162
00:09:44.961 --> 00:09:48.560
when Russia annexed, Crimea,

163
00:09:48.740 --> 00:09:52.760
and also then invaded Eastern Ukraine Ukraine in 20 2013.

164
00:09:53.210 --> 00:09:56.390
And the really interesting thing about Russia is that I have a whole chapter in

165
00:09:56.391 --> 00:10:00.800
my book on Russia because I call Russia the monster. I say,

166
00:10:00.801 --> 00:10:03.230
they are the masters of information warfare,

167
00:10:03.260 --> 00:10:06.020
and it's not just something that they started doing and then vocalists, right?

168
00:10:06.021 --> 00:10:08.090
They were doing this in the cold war.

169
00:10:08.600 --> 00:10:13.520
And I recount a story in my book about how successful they were at spreading

170
00:10:13.521 --> 00:10:16.820
disinformation and consistently throughout history,

171
00:10:16.821 --> 00:10:20.990
they have used disinformation and information operations to have an outsized

172
00:10:21.740 --> 00:10:25.130
geopolitical influence. So if you look back at the cold war,

173
00:10:25.430 --> 00:10:28.610
one of the most daring disinformation campaigns,

174
00:10:28.611 --> 00:10:31.910
the Soviets ever launched was called operation infection.

175
00:10:32.270 --> 00:10:36.560
Now this was the myth that

176
00:10:36.620 --> 00:10:41.270
HIV AIDS was a virus created by the CIA in a

177
00:10:41.271 --> 00:10:43.970
laboratory to kill African-Americans.

178
00:10:45.350 --> 00:10:47.720
And in the eighties, this took this,

179
00:10:47.920 --> 00:10:52.700
th this seed of disinformation that the Soviets came up with was kind of planted

180
00:10:52.701 --> 00:10:56.240
in a newspaper in India. And then over the years,

181
00:10:56.241 --> 00:11:00.950
it was like reiterated in the Soviet press and it kind of grew and grew and grew

182
00:11:00.951 --> 00:11:04.430
and grew until about almost a decade later, it went completely viral.

183
00:11:04.970 --> 00:11:08.030
Now in the info clips with modern technology,

184
00:11:08.060 --> 00:11:11.510
Russia still has more or less the same strategy.

185
00:11:11.511 --> 00:11:15.350
It's just that the tools by which to spread this kind of disinformation have

186
00:11:15.351 --> 00:11:18.620
become far more potent. So if we fast forward,

187
00:11:18.860 --> 00:11:22.520
I kind of do a little tour of what the Russians did in 2016,

188
00:11:22.521 --> 00:11:25.970
because I was working on these political events. Then, um,

189
00:11:26.000 --> 00:11:28.250
if you look at what happened in the election,

190
00:11:28.490 --> 00:11:32.000
one of the things they did apart from hacking,

191
00:11:32.060 --> 00:11:36.590
and also trying to compromise the critical voting infrastructure,

192
00:11:36.830 --> 00:11:38.120
one of the things that they did,

193
00:11:38.121 --> 00:11:41.450
which was super interesting was social media operations.

194
00:11:41.900 --> 00:11:46.070
And this is something that they have become really good at, um,

195
00:11:46.520 --> 00:11:50.810
in the Ukrainian war. So they really cut their teeth, uh, in about 2013,

196
00:11:50.811 --> 00:11:54.820
2014. And then they started looking at the S not only in 2016,

197
00:11:54.821 --> 00:11:56.920
but for a period of three years,

198
00:11:56.921 --> 00:11:59.320
they started kind of looking at the U S in 2013.

199
00:11:59.740 --> 00:12:04.630
And what they did was really find a way where they could

200
00:12:04.870 --> 00:12:09.760
attack Western democracy from within and the tactic to do this was to

201
00:12:09.761 --> 00:12:11.980
cause division within society.

202
00:12:12.700 --> 00:12:16.840
And what they did was create these communities on social media,

203
00:12:16.841 --> 00:12:20.500
by using Instagram, Facebook, um, Twitter,

204
00:12:20.680 --> 00:12:25.420
where they built up people's tribal identities on

205
00:12:25.421 --> 00:12:27.040
both sides of the political spectrum.

206
00:12:27.041 --> 00:12:30.970
So it's wrong to think that Russian disinformation just targets one side,

207
00:12:30.971 --> 00:12:33.220
either the left or the right of the political spectrum.

208
00:12:33.700 --> 00:12:37.690
And they injected these groups with a lot of pride. So for instance,

209
00:12:37.720 --> 00:12:41.590
it could be Confederates, you know, Texas secessionists, uh,

210
00:12:41.620 --> 00:12:44.860
gun owners on the right, on the conservative side of the spectrum.

211
00:12:45.100 --> 00:12:49.870
And on the other side of the spectrum, it was, um, African-American groups,

212
00:12:49.871 --> 00:12:54.670
you know, feminists. And then after they had kind of built up these groups,

213
00:12:54.671 --> 00:12:58.600
loads of feelings of pride, uh, a distinct identity,

214
00:12:58.630 --> 00:13:03.610
they started injecting them with messages of legitimate political grievances.

215
00:13:04.300 --> 00:13:06.160
And it was so Doster. Thirdly,

216
00:13:06.161 --> 00:13:10.720
one of the things that I document in the book is how time and time again,

217
00:13:10.900 --> 00:13:14.890
Russian disinformation starting with operation infection has, um,

218
00:13:15.370 --> 00:13:20.260
focused on the African-American community. And they did that in 2016 as well,

219
00:13:20.261 --> 00:13:25.030
where they use Facebook to build up all these black lives matter groups around

220
00:13:25.510 --> 00:13:28.870
the, some of the controversial shootings and police brutality.

221
00:13:29.020 --> 00:13:33.700
But at the same time, they were also creating these blue lives matter groups,

222
00:13:33.701 --> 00:13:35.560
where they were talking about, you know, uh,

223
00:13:35.590 --> 00:13:38.410
we need to protect our police and we can't defund the police.

224
00:13:38.710 --> 00:13:42.310
And it was so ironic because as I was writing the book, I mean,

225
00:13:42.311 --> 00:13:46.870
I wrote it in a few weeks, uh, during lockdown that's when, just at the end,

226
00:13:46.871 --> 00:13:47.770
when we're going to press,

227
00:13:47.771 --> 00:13:51.580
I had warned how this would continue to be an issue that the Russians would

228
00:13:51.581 --> 00:13:55.840
agitate on. Um, that's when the George George Floyd killing happened.

229
00:13:55.870 --> 00:13:59.980
And that's not to say that everything that's happened since in America is

230
00:13:59.981 --> 00:14:01.300
because of the Russians,

231
00:14:01.301 --> 00:14:04.720
because there is a long history of complicated race relations,

232
00:14:05.140 --> 00:14:07.780
but it is prime picking ground.

233
00:14:07.840 --> 00:14:11.980
And it is such an opportunity for them to cause these divisions in society.

234
00:14:12.400 --> 00:14:14.920
And the interesting thing that they're doing in 2020.

235
00:14:15.070 --> 00:14:19.930
So the methods keep evolving and they keep on becoming more sophisticated.

236
00:14:20.290 --> 00:14:21.550
And in 2020,

237
00:14:21.700 --> 00:14:25.600
what we have discovered the Russians are doing vis-a-vis the election in the

238
00:14:25.601 --> 00:14:30.490
United States is now they've kind of outsourced their

239
00:14:30.520 --> 00:14:32.160
troll farms, which they hadn't St.

240
00:14:32.170 --> 00:14:36.640
Petersburg in 2016 to Africa where a

241
00:14:36.670 --> 00:14:40.360
Ghana, they had, uh, an NGO, which set out,

242
00:14:40.361 --> 00:14:44.320
which was set up to extensively, be a legitimate human rights,

243
00:14:44.321 --> 00:14:48.550
NGO on the people that were working there were actually running the social media

244
00:14:48.551 --> 00:14:51.860
influence operations to divide the United States ahead of the election.

245
00:14:52.100 --> 00:14:56.420
So the point is information warfare is an old game,

246
00:14:56.450 --> 00:15:00.470
but it's becoming more and more potent. And with these new tools,

247
00:15:00.500 --> 00:15:03.860
it's becoming more and more dangerous. We see it in our polarized politics.

248
00:15:04.100 --> 00:15:07.280
It's not only because of the Russians, it's not only because of the Chinese,

249
00:15:07.281 --> 00:15:11.600
but it's something where we as democracy, democracies are very vulnerable.

250
00:15:11.960 --> 00:15:16.280
And the other important point is that Russia has been so successful in these

251
00:15:16.281 --> 00:15:16.881
information.

252
00:15:16.881 --> 00:15:21.740
Operations is that other countries are now looking to Russia and

253
00:15:21.741 --> 00:15:23.990
seeking to emulate what they're doing.

254
00:15:23.991 --> 00:15:27.800
So one very interesting thing that I go into into the book and maybe we'll come

255
00:15:27.801 --> 00:15:31.610
into that is how China has increasingly been playing these games,

256
00:15:31.880 --> 00:15:36.500
especially in light of COVID given the geopolitical consequences for

257
00:15:36.501 --> 00:15:38.540
China. Um, vis-a-vis, COVID.

258
00:15:39.730 --> 00:15:43.870
<v 0>I mean, that particular case kind of exposes a tactic.</v>

259
00:15:43.871 --> 00:15:48.280
I've always been fascinated by and always worry that most people really kind of

260
00:15:48.490 --> 00:15:52.870
don't quite grasp, which is the way that a state can squat on kind of one side.

261
00:15:52.871 --> 00:15:55.270
And then the other side of a political spectrum.

262
00:15:55.600 --> 00:15:59.410
I think we often kind of think that disinformation operations are all about,

263
00:15:59.411 --> 00:16:02.560
you know, making Brexit happen or making Trump win, you know,

264
00:16:02.830 --> 00:16:04.780
trying to force a particular outcome,

265
00:16:05.080 --> 00:16:07.270
but not actually that they're on both sides,

266
00:16:07.271 --> 00:16:11.500
arguing for both things at the same time, um, trying to make everyone angry.

267
00:16:14.080 --> 00:16:16.240
<v 1>And that is so key because,</v>

268
00:16:16.330 --> 00:16:20.950
and I think you're so right to point that out because over the past few weeks,

269
00:16:20.951 --> 00:16:24.700
when the Russia issue has come to the fore and the news agenda here in the UK,

270
00:16:24.701 --> 00:16:27.520
again, I've had so many people come to me and say, well,

271
00:16:27.521 --> 00:16:32.290
it can't possibly be because in 2016, they were supposedly in favor of Brexit.

272
00:16:32.291 --> 00:16:35.530
And now, you know, Oh, they were supposedly in favor of Jeremy Corbyn,

273
00:16:35.531 --> 00:16:40.090
but that is exactly the strategy you cause a

274
00:16:40.091 --> 00:16:44.440
division and chaos. And you hit a democracy where it hurts,

275
00:16:44.770 --> 00:16:48.490
you know, because of our freedom of speech and our freedom of expression,

276
00:16:48.640 --> 00:16:52.630
you seek to drive the rifts into democracy on all sides of the spectrum.

277
00:16:52.840 --> 00:16:57.580
They did it here in the UK with regards to both the conservatives and

278
00:16:57.581 --> 00:17:02.020
labor. And you're absolutely right in the United States in 2016,

279
00:17:02.050 --> 00:17:06.010
they did it, um, for both Donald Trump as well as Bernie Sanders.

280
00:17:06.040 --> 00:17:07.780
So it works on both sides.

281
00:17:08.920 --> 00:17:11.440
<v 0>The last question I have for you, Nina is a difficult one.</v>

282
00:17:11.441 --> 00:17:15.430
It's an important one, but difficult one. I get asked it all the time. Um,

283
00:17:15.820 --> 00:17:19.900
and it's, it's that big question which has proven very,

284
00:17:19.901 --> 00:17:23.890
very difficult to kind of get around and to therefore kind of get real serious

285
00:17:23.891 --> 00:17:27.010
resources and political will mobilized against this problem.

286
00:17:27.011 --> 00:17:29.620
And the question is, does any of this actually matter,

287
00:17:30.160 --> 00:17:32.560
how do we measure the impact of this? You know,

288
00:17:33.580 --> 00:17:35.560
socially it isn't this just social media advertising.

289
00:17:35.561 --> 00:17:38.260
And does that actually work at all? You know, um,

290
00:17:38.290 --> 00:17:42.520
does any of this really shift anyone's minds or beliefs in any way?

291
00:17:43.780 --> 00:17:44.500
Um.

292
00:17:44.500 --> 00:17:48.370
<v 1>Yeah, so I think that's a really good question and my answer would be,</v>

293
00:17:48.371 --> 00:17:53.370
it absolutely matters because that if we're looking at it from

294
00:17:53.371 --> 00:17:58.140
the dimension of foreign interference, right? So if you say to me, Oh, well,

295
00:17:58.170 --> 00:18:01.680
it's, it's, it's lazy to say that Trump won because of the Russians.

296
00:18:01.770 --> 00:18:04.200
I would agree with you, I'd say absolutely. You know,

297
00:18:04.201 --> 00:18:08.970
that is a lazy explanation for a very complicated set of reasons as to why

298
00:18:08.971 --> 00:18:12.750
Trump won the election. And it isn't only because the Russians intervened.

299
00:18:13.140 --> 00:18:17.430
However, the fact that the Russians did intervene is a fact.

300
00:18:17.640 --> 00:18:21.960
So that is something we cannot deny. And we shouldn't deny. Um,

301
00:18:22.080 --> 00:18:26.190
moreover it's not just about foreign interference, right?

302
00:18:26.191 --> 00:18:28.950
It's not just about what the Russians are doing, what the Chinese are doing,

303
00:18:28.951 --> 00:18:32.730
what the Saudis are doing, they're doing it. Okay. That's one angle.

304
00:18:32.940 --> 00:18:37.710
The second angle is that this corroded and corrupted information ecosystem is

305
00:18:37.740 --> 00:18:42.150
increasingly coming to define our own domestic politics as well.

306
00:18:42.450 --> 00:18:44.880
So if you look at it from the political angle, I mean,

307
00:18:44.881 --> 00:18:48.120
really what's at stake here is liberal democracy, right?

308
00:18:48.150 --> 00:18:52.590
Do you want to have a liberal democracy in a kind of functioning

309
00:18:52.591 --> 00:18:56.730
society? Because if you do, you need to have some kind of basis,

310
00:18:57.120 --> 00:18:58.800
some kind of shared reality,

311
00:18:58.801 --> 00:19:03.330
some kind of objective platform on which to decide these are the facts,

312
00:19:03.331 --> 00:19:04.920
and this is how we're going to run society.

313
00:19:04.921 --> 00:19:09.360
So it doesn't work if it's being disrupted by foreign powers from the

314
00:19:09.361 --> 00:19:13.920
outside and internally from, uh, by domestic agents as well.

315
00:19:13.921 --> 00:19:17.580
So if you end up in this state where you're just constantly having information

316
00:19:17.970 --> 00:19:19.530
warfare, because that is literally what it is,

317
00:19:19.920 --> 00:19:21.540
then our society is going to suffer.

318
00:19:21.900 --> 00:19:23.940
So let's say you're not interested in any of that.

319
00:19:23.941 --> 00:19:26.610
And you don't care about what the Russians are doing,

320
00:19:26.640 --> 00:19:28.110
what the Chinese are doing,

321
00:19:28.111 --> 00:19:32.430
and you don't care about like domestic politics either. I would say again,

322
00:19:32.490 --> 00:19:36.750
it matters because it impacts all of us in other ways.

323
00:19:36.751 --> 00:19:41.490
First of all, businesses, every business is at risk and the InFocus lips,

324
00:19:41.820 --> 00:19:45.570
every business faces reputational risk, every business, um,

325
00:19:46.200 --> 00:19:50.760
can be subjected to disinformation. I mean, we're already seeing it, uh, how,

326
00:19:50.790 --> 00:19:52.290
you know, on social media,

327
00:19:52.291 --> 00:19:57.090
these campaigns have been run saying this and this business supported Trump.

328
00:19:57.100 --> 00:20:01.950
So boy caught them or say, you know, there was another instance where, uh,

329
00:20:01.980 --> 00:20:05.340
there was a lot of fake news about Starbucks, um,

330
00:20:05.820 --> 00:20:10.050
giving free coffees to people of, uh, people of color.

331
00:20:10.560 --> 00:20:15.120
Every business is at risk of disinformation in this corrupt and dangerous

332
00:20:15.121 --> 00:20:18.120
information ecosystem aside from business.

333
00:20:18.150 --> 00:20:22.680
Then there's also you the personal, when it comes to the very individual level.

334
00:20:22.740 --> 00:20:25.530
And because of the way that, um,

335
00:20:25.560 --> 00:20:27.750
defects in AI will make this information,

336
00:20:27.751 --> 00:20:32.100
basically a tool that's available to anyone you yourself are

337
00:20:32.580 --> 00:20:36.030
at the front lines of potentially being, um,

338
00:20:36.870 --> 00:20:41.790
becoming a target of different disinformation in a really, really personal way,

339
00:20:41.970 --> 00:20:44.760
because what deep fakes will allow you to do,

340
00:20:44.820 --> 00:20:49.480
or will it allow a bad actor to do is to actually steal your

341
00:20:49.481 --> 00:20:53.860
voice or to steal your face and to make you say, or do something you didn't say.

342
00:20:54.100 --> 00:20:57.580
And again, the first instance, it's not, it's not make-believe right.

343
00:20:57.581 --> 00:21:00.490
It's already happening. It's happening in pornography.

344
00:21:00.670 --> 00:21:03.070
It's happening mostly against women right now,

345
00:21:03.340 --> 00:21:07.090
but that is a harbinger of what's to come,

346
00:21:07.240 --> 00:21:10.780
because let's say, I just don't like you, Carl,

347
00:21:10.781 --> 00:21:12.400
and I want to ruin your reputation.

348
00:21:12.550 --> 00:21:16.930
I could make a leaked audio tape of you saying something very disagreeable,

349
00:21:17.110 --> 00:21:21.430
which would be enough to get you canceled in these political climates.

350
00:21:21.431 --> 00:21:24.400
And I could just put that on Google with your name on it.

351
00:21:24.490 --> 00:21:26.610
And that would be the first thing that comes up. And you could try,

352
00:21:29.200 --> 00:21:32.860
you could try to explain that to potential employer and say, Oh, you know,

353
00:21:32.861 --> 00:21:35.440
that wasn't me, that's a deep fake, but they might just say that, you know what,

354
00:21:35.441 --> 00:21:39.820
it's just too complicated. And so it matters for a politics,

355
00:21:40.000 --> 00:21:41.950
both domestically and geopolitically.

356
00:21:42.130 --> 00:21:46.420
It matters for every single business and it matters in your personal life.

357
00:21:46.480 --> 00:21:50.740
And this is why I think it's not just a question of defects. It's,

358
00:21:50.950 --> 00:21:52.540
it's much bigger than that.

359
00:21:52.570 --> 00:21:56.890
It's about how is our information ecosystem becomes so dangerous?

360
00:21:57.130 --> 00:21:59.680
How are we facing this monumental crisis of,

361
00:22:00.040 --> 00:22:04.630
and disinformation where we've all become potential targets and what the hell do

362
00:22:04.631 --> 00:22:06.880
we do about it? So it absolutely matters.

363
00:22:08.130 --> 00:22:08.431
<v 0>All right. Well,</v>

364
00:22:08.431 --> 00:22:13.020
thank you for that extremely powerful response to that difficult question. It's,

365
00:22:13.260 --> 00:22:16.920
let's pull away from the Russia station on our grant or, um, and,

366
00:22:16.930 --> 00:22:20.760
and go to somewhere. You've already mentioned domestic politics. Um,

367
00:22:21.000 --> 00:22:25.200
so in exactly what way do you think have

368
00:22:25.530 --> 00:22:30.150
domestic politicians or campaigns kind of looked at the Russia example? I mean,

369
00:22:30.151 --> 00:22:34.110
kind of drawn lessons from that about the way that they were going to wage

370
00:22:34.140 --> 00:22:35.490
politics and the digital age?

371
00:22:36.510 --> 00:22:36.871
<v 1>Well,</v>

372
00:22:36.871 --> 00:22:41.610
I think the interesting thing about politics in the digital

373
00:22:41.611 --> 00:22:46.290
age is, is the imperative just to win, you know,

374
00:22:46.291 --> 00:22:49.650
is any, is anything, uh,

375
00:22:49.740 --> 00:22:53.220
allowed when it comes to comes when it comes to winning.

376
00:22:53.221 --> 00:22:56.280
So can you say and do anything or is the imperative,

377
00:22:56.310 --> 00:22:59.280
especially in Western democracies to, to win,

378
00:22:59.310 --> 00:23:01.830
but also to have some kind of cohesive society.

379
00:23:02.130 --> 00:23:06.840
And I think that's kind of the crossroads where we stand at right

380
00:23:06.841 --> 00:23:08.760
now and in my book, um,

381
00:23:08.761 --> 00:23:12.180
when I look at the Western democracies and the domestic threat and the domestic

382
00:23:12.181 --> 00:23:13.014
challenge,

383
00:23:13.200 --> 00:23:17.790
I use the United States as a case study because obviously we have the U S

384
00:23:17.791 --> 00:23:22.020
election coming up and I look at particularly one actor,

385
00:23:22.021 --> 00:23:25.380
who's got so much influence. And of course that's Donald Trump.

386
00:23:25.710 --> 00:23:28.530
And I know so much has been written about Donald Trump,

387
00:23:28.620 --> 00:23:30.210
but so much has been said about Donald Trump.

388
00:23:30.750 --> 00:23:35.640
But I think it's worth retreading that ground because he is

389
00:23:35.760 --> 00:23:40.260
the leader of the free world and where America leads, you know,

390
00:23:40.261 --> 00:23:44.910
the rest of the Western world will follow and seeing how fast and

391
00:23:45.500 --> 00:23:48.110
he has been with not only lying,

392
00:23:48.500 --> 00:23:52.910
but also disinformation on the use of manipulated media,

393
00:23:52.970 --> 00:23:57.710
including the use of deep fakes has been really alarming because to a

394
00:23:57.711 --> 00:23:58.550
certain extent,

395
00:23:58.580 --> 00:24:02.870
it doesn't matter whether or not Trump wins the election in 2020,

396
00:24:03.620 --> 00:24:05.420
uh, that precedent has already been set.

397
00:24:05.421 --> 00:24:09.680
So I'm really interested to see if Trump loses, you know,

398
00:24:09.950 --> 00:24:12.410
what happens then what happens to then fall clips?

399
00:24:12.411 --> 00:24:14.810
What happens to the information ecosystem,

400
00:24:15.080 --> 00:24:17.990
because it's not just the Republicans who are using this tactics,

401
00:24:18.020 --> 00:24:22.190
the Democrats are too. And I, and I talk about that in my book. Um,

402
00:24:23.030 --> 00:24:25.970
this is the perennial kind of debate. You know, what,

403
00:24:25.971 --> 00:24:27.860
where the Democrats have been like, Oh, well,

404
00:24:27.861 --> 00:24:31.460
if the Republicans use these kind of disinformation tactics,

405
00:24:31.490 --> 00:24:35.030
then should we do it too? Or does that make us just as bad?

406
00:24:35.360 --> 00:24:38.360
And the reality is they have used these kinds of tactics.

407
00:24:38.361 --> 00:24:43.340
There was a famous instance where there was a seat in

408
00:24:43.341 --> 00:24:46.670
Alabama and the Democrats run,

409
00:24:47.900 --> 00:24:51.230
Rhonda disinformation companies fired by the Russians against the very safe

410
00:24:51.231 --> 00:24:54.740
Republican candidates who were supposed to win that seat. Um,

411
00:24:55.370 --> 00:24:59.810
so I just think the main question is what

412
00:25:00.110 --> 00:25:04.850
happens with our domestic information ecosystem and is it

413
00:25:04.851 --> 00:25:08.420
just going to become one where anything goes in order to win?

414
00:25:08.570 --> 00:25:11.690
And we see those kinds of divides here in the UK as well, right?

415
00:25:12.140 --> 00:25:15.500
Where everything has become so polarized,

416
00:25:15.680 --> 00:25:20.540
where you have to choose a side and any kind of nuance

417
00:25:20.600 --> 00:25:25.370
or any kind of position being a centrist is way

418
00:25:25.610 --> 00:25:27.800
out dated. So I think this is.

419
00:25:28.480 --> 00:25:29.590
<v 0>Right 1996, isn't it?</v>

420
00:25:31.090 --> 00:25:36.040
<v 1>This is one of the biggest challenges facing Western democracies in my view, uh,</v>

421
00:25:36.050 --> 00:25:39.460
it's true that we are facing a lot of, uh,

422
00:25:40.300 --> 00:25:43.660
geopolitically we're facing a lot of hostile forces in the world is changing.

423
00:25:44.020 --> 00:25:44.471
However,

424
00:25:44.471 --> 00:25:49.270
I don't think we've quite as a democracy has figured out

425
00:25:49.390 --> 00:25:53.470
how we're going to run our politics in the information age,

426
00:25:53.710 --> 00:25:57.340
because we do have certain rights, freedom of expression,

427
00:25:58.180 --> 00:26:02.560
freedom of information, which are inalienable and should be protected. However,

428
00:26:02.561 --> 00:26:07.390
we haven't quite figured out how to square that with the dangerous information

429
00:26:07.391 --> 00:26:08.224
ecosystem.

430
00:26:08.710 --> 00:26:12.400
<v 0>Yeah. I mean, it, it, it's, it's the way as you described, you know, the,</v>

431
00:26:12.430 --> 00:26:16.330
these techniques have been imported into domestic politics that makes me so

432
00:26:16.331 --> 00:26:19.990
nervous that whilst it kind of remained in the hands of Russia, you know,

433
00:26:19.991 --> 00:26:23.050
at least the techniques were kind of shadowy and kind of felt legitimate.

434
00:26:23.080 --> 00:26:26.080
You know, the intelligence agencies might be trying to interfere,

435
00:26:26.081 --> 00:26:28.570
but at least we could kind of know that was wrong and try and stop here.

436
00:26:28.600 --> 00:26:30.730
But it kind of now, it just,

437
00:26:30.910 --> 00:26:35.140
it begins to feel like actually they're just like baked in campaigning tactics.

438
00:26:35.410 --> 00:26:39.250
They've become com ethically normalized where people can't even really see a

439
00:26:39.251 --> 00:26:43.470
moral problem in using them. And, and I, I mean,

440
00:26:43.650 --> 00:26:45.240
it really, uh,

441
00:26:45.300 --> 00:26:49.740
makes me recall the last Indian election where I believe every single mainstream

442
00:26:49.741 --> 00:26:54.570
party in India was each court by Facebook and the other tech giants in an

443
00:26:55.050 --> 00:26:55.920
elicit influence campaign.

444
00:26:56.010 --> 00:27:00.780
All of them kind of implied that it was just now a campaigning tactic that it's

445
00:27:00.781 --> 00:27:02.430
kind of embarrassing if you don't do it.

446
00:27:03.500 --> 00:27:06.590
<v 1>Yeah. And that all evidence you're absolutely right.</v>

447
00:27:06.591 --> 00:27:09.890
All evidence seems to suggest that that is the new normal. I mean,

448
00:27:09.891 --> 00:27:14.300
these researchers at the university of Oxford, who've been kind of tracking,

449
00:27:14.301 --> 00:27:18.170
they call it computational propaganda and influence operations on social media

450
00:27:18.500 --> 00:27:22.580
found that, you know, the number of countries doing it had gone up to 70,

451
00:27:22.581 --> 00:27:25.430
I think it was last year from 17, a few years ago.

452
00:27:25.610 --> 00:27:27.650
And that included many Western countries.

453
00:27:27.920 --> 00:27:32.330
So I think the genie is already out of the bottle. Right.

454
00:27:32.331 --> 00:27:34.070
I don't think we're putting it back in.

455
00:27:34.160 --> 00:27:37.460
So I think the question then is how do we,

456
00:27:37.461 --> 00:27:39.860
as citizens feel about that,

457
00:27:39.861 --> 00:27:44.540
especially in Western democracies and how do we think that is going to

458
00:27:44.541 --> 00:27:47.840
impact our politics? Because it definitely will impact our politics,

459
00:27:47.841 --> 00:27:51.740
especially in these countries where the

460
00:27:52.490 --> 00:27:56.960
state does not have a monopoly over the information ecosystem, like for example,

461
00:27:56.961 --> 00:27:58.370
in China. Um,

462
00:27:58.371 --> 00:28:02.930
so it really can be a case of too much information having the effect of

463
00:28:02.931 --> 00:28:06.470
censorship, which is not exactly how you would think of it when you,

464
00:28:06.530 --> 00:28:10.310
when you think an abundance of information would be a good thing rather than a

465
00:28:10.311 --> 00:28:12.920
bad thing. But if it's an abundance of bad information,

466
00:28:13.610 --> 00:28:14.450
what are we going to do about it?

467
00:28:14.451 --> 00:28:17.900
And these are really the big questions that we need to think about it.

468
00:28:17.901 --> 00:28:22.610
And I honestly think these are the biggest and most important

469
00:28:22.611 --> 00:28:26.780
challenges for, for our democracies in the 21st century.

470
00:28:26.810 --> 00:28:28.460
And in the next five to 10 years.

471
00:28:29.300 --> 00:28:31.640
<v 0>We all, we all gonna get solutions. I promise everyone,</v>

472
00:28:31.641 --> 00:28:34.310
I'm not going to leave us in this, in this Morris of despair.

473
00:28:34.311 --> 00:28:37.990
But I think there is, there is one further stop we need to go to in, in,

474
00:28:38.020 --> 00:28:39.970
in terms of diagnosing the problem. And,

475
00:28:40.310 --> 00:28:43.760
and it's not stopped that we tend to kind of talk less about kind of inference

476
00:28:43.761 --> 00:28:46.580
operations happening elsewhere. I mean, here in the West, of course,

477
00:28:46.581 --> 00:28:49.340
we're obsessed by us politics. So we talk about that all the time.

478
00:28:49.550 --> 00:28:52.550
We're obsessed by Brexit and British politics. We talk about that all the time,

479
00:28:52.880 --> 00:28:57.380
and we'll just kind of information warfare look outside of the West.

480
00:28:57.381 --> 00:29:02.030
What is it like in Saudi Arabia or Nepal or, um, or India, in fact,

481
00:29:02.480 --> 00:29:04.820
you know, places that would just, we just have less, uh, you know,

482
00:29:04.821 --> 00:29:06.770
we just talk about less than a daily basis.

483
00:29:07.520 --> 00:29:07.550
<v 1>Uh,</v>

484
00:29:07.550 --> 00:29:12.140
so this was one of the most interesting chapters in the book for me to research

485
00:29:12.141 --> 00:29:12.860
and write,

486
00:29:12.860 --> 00:29:17.330
because obviously I came at it from the angle of like Western democracies and

487
00:29:17.360 --> 00:29:20.030
foreign countries trying to undermine Western democracy.

488
00:29:20.570 --> 00:29:22.700
A democracy is through information warfare,

489
00:29:23.420 --> 00:29:26.690
but what was also really interesting to look at was, you know,

490
00:29:26.720 --> 00:29:28.820
the rest of the world. Um,

491
00:29:28.910 --> 00:29:33.080
and of course the dangerous information ecosystem

492
00:29:33.530 --> 00:29:35.330
impacts the rest of the world as well,

493
00:29:35.450 --> 00:29:39.830
which is one of the reasons why I think what happens in the West is so important

494
00:29:39.831 --> 00:29:43.870
because if we, as kind of bastion of the free world,

495
00:29:43.871 --> 00:29:48.820
can't deal with this bad information ecosystem with all the institutions and

496
00:29:48.850 --> 00:29:51.220
all the protections that we have in place. And we do,

497
00:29:51.550 --> 00:29:54.040
then there is no hope for those countries,

498
00:29:54.160 --> 00:29:57.280
which have less rigid safeguards. Um,

499
00:29:57.310 --> 00:30:00.820
and I kind of go into a little bit about, um,

500
00:30:01.270 --> 00:30:03.790
I grew up in South Asia. Um,

501
00:30:03.850 --> 00:30:08.710
I grew up in Nepal in India and I go into how then lips has been super

502
00:30:08.711 --> 00:30:11.800
dangerous in parts of the world like India,

503
00:30:12.010 --> 00:30:16.510
where misinformation and disinformation has been rife

504
00:30:16.630 --> 00:30:18.190
on these new kind of,

505
00:30:18.370 --> 00:30:23.290
as this people have plugged into this new information ecosystem with, uh,

506
00:30:23.320 --> 00:30:28.150
tools like the internet and smartphones for the first time,

507
00:30:28.360 --> 00:30:32.110
how they've been less able to deal with some of the disinformation that's gone

508
00:30:32.111 --> 00:30:32.531
viral.

509
00:30:32.531 --> 00:30:36.910
And I give some instances about how in India there's been like numerous cases of

510
00:30:37.330 --> 00:30:42.310
people being killed because of virus viral misinformation spreading on

511
00:30:42.311 --> 00:30:44.050
WhatsApp, for example, uh,

512
00:30:44.051 --> 00:30:47.770
there was also the case of Burma where there was literally, um,

513
00:30:48.280 --> 00:30:50.380
an ethnic genocide against the Rohingya,

514
00:30:50.381 --> 00:30:53.560
which was very much fond on by online disinformation,

515
00:30:53.561 --> 00:30:58.150
which was spread in the case of Burma, uh, actually hate speech, um,

516
00:30:58.180 --> 00:31:02.740
very much on Facebook, right? So online. Um, it's,

517
00:31:02.760 --> 00:31:07.510
it's fascinating to me when I go back to Nepal and I see people who

518
00:31:07.930 --> 00:31:11.440
in the village, you know, barely 10 years ago had running well,

519
00:31:11.441 --> 00:31:13.900
they still don't have running water or electricity,

520
00:31:14.230 --> 00:31:17.560
but they will have a mobile phone and be able to get on the internet.

521
00:31:17.830 --> 00:31:22.270
So a lot of the people in the developing world for lack of a better word,

522
00:31:22.510 --> 00:31:27.280
who are first coming into this information ecosystem have less

523
00:31:27.281 --> 00:31:29.950
protection than we do in the West, because at least in the West,

524
00:31:30.250 --> 00:31:34.690
we have some form of inoculation and we understand that this is underway.

525
00:31:34.900 --> 00:31:39.430
And we also have learned because we've seen some of the more crude and

526
00:31:39.431 --> 00:31:41.890
rudimentary forms of disinformation and misinformation,

527
00:31:41.891 --> 00:31:45.880
and we can recognize them, right. We've been inoculated against the worst,

528
00:31:45.970 --> 00:31:48.520
some of the dangerous and part in part,

529
00:31:48.521 --> 00:31:52.540
I'm writing this book because I want to help inoculate against the coming danger

530
00:31:52.541 --> 00:31:56.650
of deep fake, but in parts of the developing world that doesn't exist yet.

531
00:31:56.710 --> 00:32:01.360
So not only can these tools

532
00:32:01.361 --> 00:32:03.280
cause be used as a,

533
00:32:03.281 --> 00:32:08.020
as a form of kind of oppressing human rights and for real violence. And I mean,

534
00:32:08.021 --> 00:32:12.370
we've literally seen it in the case of ethnic genocide. It's it's, uh,

535
00:32:13.240 --> 00:32:17.320
it's, it's, uh, it's a place in many places in many,

536
00:32:17.321 --> 00:32:18.760
in many parts of the world.

537
00:32:18.970 --> 00:32:23.740
I fear that the citizenry simply not ready for the infant

538
00:32:23.741 --> 00:32:28.210
clips where it could do a lot more damage. So again,

539
00:32:28.510 --> 00:32:33.250
I think we have a responsibility to build a more,

540
00:32:34.270 --> 00:32:38.290
a safer information environment for all these people who are going to be let out

541
00:32:38.291 --> 00:32:42.860
into the wild West and potentially be, be harmed by what's out there.

542
00:32:43.850 --> 00:32:44.400
<v 0>I suppose,</v>

543
00:32:44.400 --> 00:32:48.700
that they are two very stark reminders of what the consequences possibly can be.

544
00:32:48.701 --> 00:32:53.560
You know, sometimes literally death and destruction and genocide, um,

545
00:32:53.740 --> 00:32:57.970
as very tangible reminders to anyone with lingering doubts as to the

546
00:32:57.971 --> 00:33:02.230
significance of this problem. Um, yeah. Um.

547
00:33:05.320 --> 00:33:07.810
<v 1>I wanted to quickly also mention is</v>

548
00:33:09.370 --> 00:33:14.170
there there something called the Liar's dividend? So, um, with deep fakes,

549
00:33:14.171 --> 00:33:18.190
and this is one really interesting thing that's happening with defects.

550
00:33:18.191 --> 00:33:23.110
So just the fact that you have this technology out there where,

551
00:33:23.111 --> 00:33:26.650
you know, you can fake video evidence, um,

552
00:33:27.100 --> 00:33:30.400
means that for in certain parts of the world where there's already kind of a

553
00:33:30.401 --> 00:33:33.100
fragile consensus around human rights,

554
00:33:33.101 --> 00:33:38.050
abuses and video is seen as a form of documenting that the mere existence

555
00:33:38.051 --> 00:33:41.140
of deep fakes hurts them in the other way,

556
00:33:41.230 --> 00:33:44.890
by meaning that everybody gets posable deniability.

557
00:33:44.891 --> 00:33:49.480
So let's say you have a video evidence of someone committing an atrocity,

558
00:33:50.260 --> 00:33:53.680
um, because defects exist. Now, you can say, well,

559
00:33:54.010 --> 00:33:56.380
that's just a deep fake, even when it isn't.

560
00:33:56.410 --> 00:34:01.200
And I kind of talk about in the book, how that happened, for example, in, in,

561
00:34:01.201 --> 00:34:05.680
in Gabon, where there was a failed coup to top because, uh,

562
00:34:05.681 --> 00:34:09.130
the prime minister kind of went missing for, for a few months.

563
00:34:09.520 --> 00:34:14.290
And it was because he had had a stroke, but when he appeared on TV,

564
00:34:14.291 --> 00:34:18.100
he looked kind of strange because he had had the stroke and he had like Botox to

565
00:34:18.101 --> 00:34:19.720
make his face look a bit more straight.

566
00:34:20.080 --> 00:34:24.040
And what his opponent said was that that was a deep fake video.

567
00:34:24.041 --> 00:34:27.700
And that wasn't actually the president and the president had died and they use

568
00:34:27.701 --> 00:34:32.230
that as an opportunity to have a to top. So even if there isn't a deep fake,

569
00:34:32.410 --> 00:34:37.060
the fact that you, there is no more consensus around visual media,

570
00:34:37.090 --> 00:34:42.010
serving as legitimate evidence can have an impact in the other way as

571
00:34:42.011 --> 00:34:42.844
well.

572
00:34:43.720 --> 00:34:44.553
<v 0>Okay.</v>

573
00:34:44.650 --> 00:34:49.130
So there's one more stop we need to get to before we get to the solutions.

574
00:34:49.131 --> 00:34:49.570
So I think,

575
00:34:49.570 --> 00:34:53.050
I think we've began to diagnose a problem and get our hand around how this

576
00:34:53.051 --> 00:34:56.200
happens, where it happens, why, um,

577
00:34:56.590 --> 00:35:01.190
I want to talk blame MENA, who is, who is to blame for this? You, you,

578
00:35:01.270 --> 00:35:04.600
you you've, you've mentioned quite a few times, um,

579
00:35:04.660 --> 00:35:08.650
this kind of corrupted or broken information ecosystem, um,

580
00:35:08.800 --> 00:35:11.660
who is responsible for building that and,

581
00:35:11.661 --> 00:35:16.360
and what have they done to make it so vulnerable to the kinds of campaigns we've

582
00:35:16.361 --> 00:35:17.920
been, we've been talking about today.

583
00:35:18.580 --> 00:35:21.580
<v 1>Yeah. You know what I wish I could tell you,</v>

584
00:35:21.670 --> 00:35:24.850
this is the person who is to blame, but I think,

585
00:35:27.160 --> 00:35:31.990
I think a lot of this has come about by, you know,

586
00:35:31.991 --> 00:35:33.670
the tech utopias.

587
00:35:33.940 --> 00:35:38.250
So the tech utopia that the age of information was supposed to bring,

588
00:35:38.670 --> 00:35:43.500
we've already seen. Um, there are many, many downsides as well.

589
00:35:43.530 --> 00:35:46.300
I mean, technology itself is net neutral, right? It's,

590
00:35:46.301 --> 00:35:51.150
it's just a driver of human intention and human intention is neither wholly

591
00:35:51.151 --> 00:35:52.620
good nor wholly bad.

592
00:35:52.830 --> 00:35:57.240
So the kind of tech revolution or the age of information of the last three

593
00:35:57.241 --> 00:36:00.600
decades is now manifesting itself in both its

594
00:36:01.410 --> 00:36:05.520
amazing aspect as well. It's very, very, very dark aspect.

595
00:36:05.790 --> 00:36:08.880
So I don't think that there is one person to blame. However,

596
00:36:09.720 --> 00:36:14.700
what I will say is that we should not be so naive as

597
00:36:14.701 --> 00:36:18.540
to think that the new developments that are coming down the line.

598
00:36:18.540 --> 00:36:22.260
So for example, when it comes to the advances in AI,

599
00:36:22.261 --> 00:36:24.660
which is what I'm talking about here in particular with,

600
00:36:24.780 --> 00:36:29.400
with regards to deep fake, that that is not going to be used by bad guys.

601
00:36:29.401 --> 00:36:33.240
You know, it's going to have incredible applications. Of course it will,

602
00:36:33.480 --> 00:36:37.050
but we should not be so naive as to think that there won't be opportunists who

603
00:36:37.051 --> 00:36:40.530
will use it for bad. So whether that's a rogue state,

604
00:36:40.860 --> 00:36:44.310
whether that's an opportunist domestic politician,

605
00:36:44.550 --> 00:36:47.880
whether that is a criminal, whether that's a fraudster,

606
00:36:47.910 --> 00:36:50.880
whether that's just your mate, who you really off,

607
00:36:50.881 --> 00:36:53.520
who wants to get some form of revenge on you. Um,

608
00:36:54.780 --> 00:36:59.520
I think this more than having one person to blame is the ultimate

609
00:37:01.440 --> 00:37:06.030
and ultimately a reflection on human nature, right?

610
00:37:06.090 --> 00:37:09.840
Because all this technology which is coming together with human nature is

611
00:37:09.841 --> 00:37:13.230
showing both the best sides and the worst sides of humanity.

612
00:37:13.231 --> 00:37:17.550
And I think that the only thing that we have to do right now is we need to get a

613
00:37:17.551 --> 00:37:19.590
little bit of a handle on some of the bad sides.

614
00:37:20.660 --> 00:37:23.270
<v 0>And how we do that. This is the final stop here we are.</v>

615
00:37:23.480 --> 00:37:26.000
How do we help tell me this is fixable.

616
00:37:26.540 --> 00:37:30.080
Tell me that the information age is not going to be one where inflammation is

617
00:37:30.081 --> 00:37:33.230
more corrupt, dead and corruptive than ever before.

618
00:37:33.440 --> 00:37:34.700
Tell me that there's a way through.

619
00:37:35.990 --> 00:37:37.220
<v 1>I think there is a way through,</v>

620
00:37:37.580 --> 00:37:41.690
but unfortunately there is no silver bullet answer

621
00:37:41.960 --> 00:37:46.940
and because we are going to be entering now a period

622
00:37:47.060 --> 00:37:51.020
of tremendous flux, both Jew politically,

623
00:37:52.430 --> 00:37:53.263
um,

624
00:37:53.390 --> 00:37:57.680
not to mention because of the huge kind of exponential advances in technology

625
00:37:57.681 --> 00:37:59.510
and how that's going to impact our society.

626
00:37:59.720 --> 00:38:03.800
I think there's going to be a lot more chaos before there is structure and calm

627
00:38:03.801 --> 00:38:07.460
again. So I think the first instance in the first instance,

628
00:38:07.461 --> 00:38:11.510
what we need to do is to name it and understand what's happening.

629
00:38:11.750 --> 00:38:16.100
And that was my contribution in writing this book because I kind of saw all of

630
00:38:16.101 --> 00:38:19.850
these trends and how they were coming together over my,

631
00:38:20.120 --> 00:38:23.990
the last decade of my work, where I've started with what Russia was doing.

632
00:38:23.991 --> 00:38:25.340
And then I saw how, you know,

633
00:38:25.341 --> 00:38:29.810
kind of some of the Western influence operation campaigns were running and what

634
00:38:29.811 --> 00:38:32.120
was happening with Brexit, what was happening around Trump?

635
00:38:32.150 --> 00:38:36.280
What was an increasingly polarized debate in our own society.

636
00:38:36.281 --> 00:38:40.390
And then I saw how women were being targeted by deep fake non-consensual

637
00:38:40.391 --> 00:38:45.100
pornography. I think the first thing is to consent,

638
00:38:45.130 --> 00:38:46.810
put a conceptual framework around that.

639
00:38:46.811 --> 00:38:48.820
And that's what I've called in focal lips, right? This,

640
00:38:48.821 --> 00:38:52.780
this information ecosystem, which is increasingly dangerous and untrustworthy,

641
00:38:53.500 --> 00:38:56.350
um, we all are impacted by it. I mean, you,

642
00:38:56.351 --> 00:39:00.550
you raised the point earlier in the interview and I think this is something that

643
00:39:00.551 --> 00:39:03.880
many people ask me saying, well, you know, why does it matter? Is it,

644
00:39:04.150 --> 00:39:08.140
it's just a bit nonsense because it's not tangible. So I think we need to,

645
00:39:08.200 --> 00:39:09.033
first of all,

646
00:39:09.280 --> 00:39:13.720
say stress how important it is and how it impacts all of us.

647
00:39:14.230 --> 00:39:17.680
And then the second part. So that's the first thing understand.

648
00:39:17.740 --> 00:39:19.600
The second part is, um,

649
00:39:19.930 --> 00:39:24.160
we begin to defend more proactively and

650
00:39:24.520 --> 00:39:29.410
there is no one organization or one body that can do that.

651
00:39:29.530 --> 00:39:33.460
You know, it's not, the answers are not going to come from regulation alone.

652
00:39:33.640 --> 00:39:35.650
They're not going to come from policymakers alone.

653
00:39:35.740 --> 00:39:40.600
You can't just go to the big tech companies and be like you fix it because it is

654
00:39:40.630 --> 00:39:44.950
ultimately not a tech problem. It's a societal problem.

655
00:39:44.951 --> 00:39:46.270
It's a human nature problem.

656
00:39:46.570 --> 00:39:51.460
And we all these contingent parts of society need to work together to come

657
00:39:51.461 --> 00:39:53.980
up with solutions. So you need to take a network approach,

658
00:39:54.520 --> 00:39:57.400
obviously that is pretty complicated,

659
00:39:57.820 --> 00:40:00.940
but the good work has already started.

660
00:40:00.970 --> 00:40:03.040
Then in the kind of last chapter of my book,

661
00:40:03.100 --> 00:40:07.600
I lay out various people in organizations that are

662
00:40:07.601 --> 00:40:09.280
tackling different parts of the problem.

663
00:40:09.290 --> 00:40:13.990
So you have technologists who are trying to create AI, um,

664
00:40:14.050 --> 00:40:17.020
machine learning systems that can identify deep fakes online.

665
00:40:17.021 --> 00:40:20.110
And then you have people who are taking more of a human rights angle.

666
00:40:20.111 --> 00:40:22.630
And then you have people who are, uh, you know,

667
00:40:22.660 --> 00:40:26.920
looking at the policy and legal arenas. I think it's not, again,

668
00:40:27.010 --> 00:40:29.050
it's not something that's gonna be fixed overnight.

669
00:40:29.051 --> 00:40:31.420
It's not something that's going to be fixed by one body.

670
00:40:31.750 --> 00:40:35.890
It's only something that when society recognizes

671
00:40:36.520 --> 00:40:41.380
how much this impacts everything, that society as a whole has to mobilize and,

672
00:40:41.830 --> 00:40:42.400
um,

673
00:40:42.400 --> 00:40:46.390
well do something about our corrupt information ecosystem and the information

674
00:40:46.391 --> 00:40:47.224
age.

675
00:40:47.550 --> 00:40:51.480
<v 2>And, and, and, and all we mobilizing. I mean, are you optimistic?</v>

676
00:40:51.540 --> 00:40:54.150
Like is what should be happening happening now?

677
00:40:57.840 --> 00:41:02.370
<v 1>I think it will probably get worse before it gets better. Um,</v>

678
00:41:02.430 --> 00:41:06.930
but I think that will be a tipping point

679
00:41:07.050 --> 00:41:11.640
where Mo there are more and more resources

680
00:41:11.670 --> 00:41:14.910
dedicated to this. I mean, if I was in charge of the government, for example,

681
00:41:15.180 --> 00:41:17.130
I wouldn't have, you know,

682
00:41:17.160 --> 00:41:21.690
technol tech as like a subsection of what government looks at. I mean,

683
00:41:21.880 --> 00:41:26.190
it should be above everything, you know, it should I,

684
00:41:26.200 --> 00:41:27.960
and unfortunately my kind of,

685
00:41:27.990 --> 00:41:32.520
I can only speak from my experience of working in the

686
00:41:32.521 --> 00:41:34.670
policy politics world,

687
00:41:34.730 --> 00:41:39.620
where I have found too large of a gap between the understanding of

688
00:41:40.400 --> 00:41:42.530
policymakers and politicians and the scale of the problem.

689
00:41:42.531 --> 00:41:46.970
So I can only speak from that perspective where I have

690
00:41:46.971 --> 00:41:49.670
seen a lot more work needs to be done,

691
00:41:50.090 --> 00:41:53.630
but the good thing is that there are people who really, really get it.

692
00:41:53.750 --> 00:41:56.510
People like you as well, Carl, we need more,

693
00:41:56.570 --> 00:41:59.360
we need more Carl Millers in the world, um,

694
00:41:59.570 --> 00:42:03.390
who are trying to bridge that gap. And I think there,

695
00:42:03.391 --> 00:42:08.180
there will be a moment when we truly understand

696
00:42:08.510 --> 00:42:13.430
that this is probably one of the biggest challenges of

697
00:42:13.431 --> 00:42:17.090
the information age and I'm, you know, of course, of course it is.

698
00:42:17.120 --> 00:42:18.740
We're living in the information age and there's,

699
00:42:19.160 --> 00:42:23.660
there's the Bob and dangerous information ecosystem we need to fix it. And, um,

700
00:42:24.260 --> 00:42:26.750
I'm hopeful that we will be mobilizing towards them.

701
00:42:28.090 --> 00:42:28.841
<v 0>Well, there we go.</v>

702
00:42:28.841 --> 00:42:33.340
Everyone we've reached the end of our tour through the in lips. Um,

703
00:42:33.370 --> 00:42:36.190
thank you so much, Nina. I thank you to intelligence squad.

704
00:42:36.191 --> 00:42:39.100
And of course thank you to all of you for listening or watching this.

705
00:42:39.101 --> 00:42:42.190
If you do want to find out more about the fates and the info clips,

706
00:42:42.191 --> 00:42:47.030
you can buy the link in the podcast description below. But, um, I,

707
00:42:47.031 --> 00:42:50.710
I, at least we only to say to union, I thank you so much. One more time.

708
00:42:50.740 --> 00:42:53.320
And I hope this is a word, even if I can't pronounce it,

709
00:42:53.321 --> 00:42:56.770
always that we'll be talking about a lot more in the future. Thank you.

