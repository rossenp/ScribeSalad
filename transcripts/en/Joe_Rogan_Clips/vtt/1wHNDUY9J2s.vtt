WEBVTT

1
00:00:00.180 --> 00:00:01.710
<v 0>The Joe Rogan experience.</v>

2
00:00:02.820 --> 00:00:06.330
Well there's journalists are right now and it's not their fault.

3
00:00:06.540 --> 00:00:07.680
It's just a print.

4
00:00:07.681 --> 00:00:12.540
Journalism is almost on the way out in terms of like buying things,

5
00:00:12.690 --> 00:00:17.250
buying newspapers and buying magazines, their numbers are radically down. Sure.

6
00:00:17.310 --> 00:00:20.640
So they resort to online things.

7
00:00:20.790 --> 00:00:23.640
While they're in the online world, you have so much competition.

8
00:00:24.180 --> 00:00:28.080
You have competition from a million different things that people can choose to

9
00:00:28.081 --> 00:00:32.610
look at or read and you know, to get them to read a article, like you gotta,

10
00:00:32.700 --> 00:00:34.920
you gotta have something good in there. So you have to distort,

11
00:00:35.250 --> 00:00:40.140
you have to inflame, you have to get people polarized. You gotta get on a set.

12
00:00:40.440 --> 00:00:44.760
You gotta paint a picture that makes you want to click on this. Like,

13
00:00:44.761 --> 00:00:47.970
what is he doing that idiots wasted his time trying to pull a truck,

14
00:00:48.360 --> 00:00:51.990
dodging the knowledge graph. Thornburg has been saying, how dare you.

15
00:00:52.410 --> 00:00:55.140
That's that's what's going on, man. Yeah. You know, but it's,

16
00:00:56.280 --> 00:01:00.300
it's just a fun, weird time for humans. You know, there's,

17
00:01:00.301 --> 00:01:03.930
there's a lot of negative things, but there's also a lot of positive things.

18
00:01:04.320 --> 00:01:09.090
It's a fun, weird time. There's a lot going on. And it's happening very,

19
00:01:09.091 --> 00:01:12.960
very, very quickly. And the prognosticators, the people that are trying to,

20
00:01:13.260 --> 00:01:16.470
you know, have some sort of an idea of where this is all going,

21
00:01:16.680 --> 00:01:21.480
no one really knows and change is happening at such a rapid pace that it

22
00:01:21.481 --> 00:01:22.470
scares everybody.

23
00:01:22.680 --> 00:01:26.010
So they're looking to define things and they're looking for control and they're

24
00:01:26.011 --> 00:01:29.640
looking to be the person who's got it figured out because nobody's got it

25
00:01:29.940 --> 00:01:32.160
figured out, right? It's madness. The earth is heating.

26
00:01:32.161 --> 00:01:36.570
The ice caps are melting. The fish are disappearing. People are eating dolphins.

27
00:01:36.780 --> 00:01:39.060
It's madness. It's madness out there.

28
00:01:39.061 --> 00:01:41.460
The garbage patch has grown and grown and grown.

29
00:01:41.550 --> 00:01:42.930
And if it wasn't for someone like you,

30
00:01:43.170 --> 00:01:46.800
who's actually acting and doing something about it,

31
00:01:47.080 --> 00:01:50.700
it would just get worse. You have a workable solution. You should be applauded.

32
00:01:51.300 --> 00:01:55.860
Hmm. Yeah. that guy. You heard me right. Jamie?

33
00:01:55.980 --> 00:01:57.630
<v 1>I didn't think it was that guy.</v>

34
00:01:58.200 --> 00:02:01.980
<v 0>Mark. them hard, right? Yeah. Yeah. Jamie agrees. Consensus.</v>

35
00:02:02.550 --> 00:02:04.950
Consensus is that guy. Yeah, but.

36
00:02:05.520 --> 00:02:07.440
<v 1>It doesn't create clicks. So maybe we need that.</v>

37
00:02:07.760 --> 00:02:10.740
<v 0>Guys trying to, how to make a living or she, or they're,</v>

38
00:02:10.980 --> 00:02:13.320
or they they're trying to make a living. You know what I mean?

39
00:02:13.321 --> 00:02:15.630
I understand it's like in this day and age,

40
00:02:15.631 --> 00:02:17.940
like you have to have things you have to write about.

41
00:02:18.270 --> 00:02:21.990
So they write about things and it might not necessarily be, um, you know,

42
00:02:22.050 --> 00:02:23.700
honest. So how do you.

43
00:02:24.000 --> 00:02:26.670
<v 1>Incentivize the truth? That's the.</v>

44
00:02:26.670 --> 00:02:28.890
<v 0>Question I get again,</v>

45
00:02:28.891 --> 00:02:32.940
I think we're in this transitionary phase and I also think technology is going

46
00:02:32.941 --> 00:02:37.560
to make a lot of what we're concentrating on

47
00:02:38.250 --> 00:02:42.120
obsolete. I think, um, we are really,

48
00:02:42.121 --> 00:02:45.540
really close to some crazy breakthroughs in terms of the distribution of

49
00:02:45.541 --> 00:02:46.201
information.

50
00:02:46.201 --> 00:02:49.860
That's going to make it obsolete and people aren't going to care as much about

51
00:02:49.861 --> 00:02:51.960
click baity things because you know,

52
00:02:51.961 --> 00:02:56.340
you're going to be able to feel things from digitally created media.

53
00:02:56.790 --> 00:03:00.520
And we think we're very, very close to augmented becoming an, uh,

54
00:03:00.521 --> 00:03:02.830
an essential part of people's lives. You know,

55
00:03:02.831 --> 00:03:06.550
the same way your phone is becoming a central part of your life. 20 years ago,

56
00:03:06.551 --> 00:03:10.300
no one carried a phone around. There was a very rare and, you know,

57
00:03:10.510 --> 00:03:14.170
1999, I mean, a small percentage of people had phones on them.

58
00:03:14.350 --> 00:03:16.420
Now it's a hundred percent, right?

59
00:03:18.010 --> 00:03:22.300
All this stuff is happening at this exponentially increasing rate when they

60
00:03:22.301 --> 00:03:26.290
implement augmented reality and who was telling us that Apple's like somewhere

61
00:03:26.291 --> 00:03:29.200
around 2021, man, it might've been, I've been looking that up.

62
00:03:29.410 --> 00:03:33.100
You mentioned it once or twice. Definitely did. Um, might've been you. Um,

63
00:03:33.130 --> 00:03:37.240
but some other folks that brought it up to that, Apple's really close.

64
00:03:37.390 --> 00:03:37.661
And they're,

65
00:03:37.661 --> 00:03:41.890
they're in the process right now of developing some sort of augmented reality

66
00:03:41.891 --> 00:03:46.750
goggles. And there'll be like glasses. Like, you know, you put on a pair of, uh,

67
00:03:46.900 --> 00:03:47.830
you know, just like this,

68
00:03:48.670 --> 00:03:51.940
but you'll be seeing all these things in front of you.

69
00:03:51.941 --> 00:03:55.330
You'll be able to move them around. You'll be able to see navigation,

70
00:03:55.840 --> 00:03:58.510
you'll turn it on and off. It'll probably work on Siri.

71
00:03:58.511 --> 00:03:59.530
You'll be able to talk to it.

72
00:04:00.460 --> 00:04:04.300
And you're going to be able to get video and information written as podcasts,

73
00:04:04.330 --> 00:04:07.180
all these things, music it's going to come through this.

74
00:04:07.630 --> 00:04:11.980
And w probably this is one step in this

75
00:04:12.160 --> 00:04:16.540
ever increasing trend of us getting further and further immersed in

76
00:04:16.541 --> 00:04:21.400
technology. And augmented reality will lead to some sort of, uh,

77
00:04:21.910 --> 00:04:25.360
impossible to determine virtual reality where it's indistinguishable from

78
00:04:25.361 --> 00:04:27.250
regular reality where we're like,

79
00:04:28.030 --> 00:04:30.850
we're like 50 years away from literally being in the matrix.

80
00:04:31.120 --> 00:04:34.270
<v 1>Yeah. Yeah. So I,</v>

81
00:04:34.930 --> 00:04:39.580
I think it's under appreciated how much our behavior is also guided

82
00:04:39.581 --> 00:04:43.870
by technology. I mean, of course we have our genes or genotype,

83
00:04:43.900 --> 00:04:48.580
which kind of lies at the most fundamental level of how our behaviors are

84
00:04:48.581 --> 00:04:53.110
formed. Um, that's why there is such a thing as, as, as a human nature,

85
00:04:53.560 --> 00:04:57.190
but then there's this whole sort of cultural layer,

86
00:04:57.400 --> 00:05:01.780
but that we humans created around us, which, um,

87
00:05:02.170 --> 00:05:05.890
I called a technosphere maybe other people have different names for it,

88
00:05:05.920 --> 00:05:09.070
but it's indeed recently read. We,

89
00:05:09.090 --> 00:05:14.080
we interact with something like 30,000 inventions or 30,000 technologies

90
00:05:14.320 --> 00:05:18.670
through our entire lives. That's a huge amount. And I think, um,

91
00:05:19.660 --> 00:05:24.130
that environment that shapes your behavior, it, um,

92
00:05:24.160 --> 00:05:28.900
decides what kind of genes are expressed.

93
00:05:29.440 --> 00:05:32.290
And the interesting thing is that it's not just the natural environment,

94
00:05:32.291 --> 00:05:36.790
but it's an environment we create. So, so probably, um,

95
00:05:37.570 --> 00:05:41.950
now when you think about people being born thousands of years ago,

96
00:05:42.430 --> 00:05:47.320
their genes were very, very similar to the people today, yet. Um,

97
00:05:47.350 --> 00:05:51.010
how they behave is completely different. You know, look again at violence.

98
00:05:51.100 --> 00:05:55.780
And why is that the case it's thanks to these

99
00:05:55.781 --> 00:05:58.580
inventions, not just physical inventions,

100
00:05:58.581 --> 00:06:02.930
but also cultural inventions and institutions that we create it that,

101
00:06:03.920 --> 00:06:08.460
um, you know, shapes our behavior. And, um, you know,

102
00:06:08.600 --> 00:06:13.580
probably the human nature is the human behavior

103
00:06:13.581 --> 00:06:18.200
is very hard to change unless it actually, um, you know,

104
00:06:18.290 --> 00:06:21.050
benefits what we do look at, you know, smartphones,

105
00:06:21.051 --> 00:06:23.480
how fast that happened versus, uh,

106
00:06:23.510 --> 00:06:28.040
how long it takes for smoking to, to go away. You know, one is,

107
00:06:29.180 --> 00:06:33.650
um, one is kind of incentivizing the continued use of it,

108
00:06:34.310 --> 00:06:39.290
um, you know, through addictive products while, uh, with, um, with,

109
00:06:39.500 --> 00:06:43.190
uh, smartphones it's, again, it's, it's something that you, you want to use. So,

110
00:06:44.150 --> 00:06:46.730
um, so I just wonder whether, you know,

111
00:06:46.731 --> 00:06:51.710
that interaction between humans and the technology that we

112
00:06:51.711 --> 00:06:53.330
create kind of invent, uh,

113
00:06:54.050 --> 00:06:58.940
incentivize inventors to become morally better and better because they are,

114
00:06:58.970 --> 00:07:02.540
did he lose me already or no? So

115
00:07:04.220 --> 00:07:06.170
the question is, um.

116
00:07:06.380 --> 00:07:08.930
<v 0>Well people are incentivized primarily by profit, right?</v>

117
00:07:09.170 --> 00:07:14.030
<v 1>Right. Um, but the, the behavior that people, um,</v>

118
00:07:14.450 --> 00:07:19.250
you know, express is kind of shaped by the world they live in and, you know,

119
00:07:19.251 --> 00:07:20.330
who knows, um,

120
00:07:20.510 --> 00:07:25.430
maybe a person today is it's more incentivized

121
00:07:25.760 --> 00:07:25.761
to,

122
00:07:25.761 --> 00:07:29.270
to do good things because of the environment that has been created rather than a

123
00:07:29.271 --> 00:07:30.104
thousand years.

124
00:07:30.650 --> 00:07:33.170
<v 0>No, I think that's absolutely the case.</v>

125
00:07:33.350 --> 00:07:37.310
And I hope that people's ability to express themselves through social media,

126
00:07:37.311 --> 00:07:38.840
all those often negative and.

127
00:07:39.620 --> 00:07:44.120
Sometimes also it can give you a sense of the moral landscape of the

128
00:07:44.121 --> 00:07:44.954
culture.

129
00:07:45.020 --> 00:07:49.940
Like not just the people on the far fringes that are the most angry

130
00:07:49.941 --> 00:07:51.290
and vehement about things,

131
00:07:51.291 --> 00:07:56.090
but people that have objective real rational thoughts. Like the,

132
00:07:56.120 --> 00:07:56.840
the,

133
00:07:56.840 --> 00:08:01.520
the fact that you were able to read that article and objectively assess whether

134
00:08:01.521 --> 00:08:03.200
or not someone has any good points or not.

135
00:08:03.470 --> 00:08:06.080
If we could all do that about everything, you know,

136
00:08:06.081 --> 00:08:08.780
if people had that sort of perspective, instead of being so reactionary,

137
00:08:08.781 --> 00:08:13.580
instead of being so angry about things, just look at criticisms,

138
00:08:13.581 --> 00:08:14.600
look at possibilities,

139
00:08:14.601 --> 00:08:19.370
look at all these different things and then shape technology to fit within our

140
00:08:19.371 --> 00:08:23.870
ethical and moral boundaries. So there's PR there's,

141
00:08:24.890 --> 00:08:29.450
and also there it's very profitable, right? Because if, if things don't feel,

142
00:08:29.451 --> 00:08:31.880
if you don't have like a guilty feeling about buying something,

143
00:08:31.881 --> 00:08:34.970
like every time I get a plastic straw, now I feel guilty. Right.

144
00:08:34.971 --> 00:08:37.310
If there was something that you,

145
00:08:37.340 --> 00:08:40.520
that people where they innovate to the point where you don't feel guilty

146
00:08:40.521 --> 00:08:41.390
supporting products,

147
00:08:41.391 --> 00:08:44.060
and you felt like this company has the same sort of ethics and ideas that you

148
00:08:44.061 --> 00:08:49.010
have, that's all good. And I think we're moving more towards that, but again,

149
00:08:49.390 --> 00:08:52.970
we're dealing with a very short window of time where human beings have had to

150
00:08:52.971 --> 00:08:56.850
adapt to this credible amount of change that takes place during a small period

151
00:08:56.851 --> 00:08:57.660
of time.

152
00:08:57.660 --> 00:08:58.980
<v 1>Yeah. It's kind of the,</v>

153
00:09:00.000 --> 00:09:04.470
where one way to look at problems is that it's kind of this chasms between,

154
00:09:05.130 --> 00:09:08.970
you know, human nature, human behavior and how we want the world to be.

155
00:09:09.150 --> 00:09:12.330
And indeed, yeah, social media is that's the case,

156
00:09:12.780 --> 00:09:16.980
but similarly for environmental problems, we, humans are,

157
00:09:17.110 --> 00:09:19.740
are driven by, you know, certain things and, you know,

158
00:09:20.790 --> 00:09:24.990
self-interests was definitely a big part of it. And yet, um,

159
00:09:25.950 --> 00:09:26.040
you know,

160
00:09:26.040 --> 00:09:30.990
that's not creating the worlds right now that we want to live in because

161
00:09:31.320 --> 00:09:32.420
the, the,

162
00:09:32.520 --> 00:09:37.020
the technosphere the technology that is the interface between the

163
00:09:37.021 --> 00:09:39.870
worlds are so sort of nature and human nature,

164
00:09:39.871 --> 00:09:43.410
that interface is not compatible with both.

165
00:09:43.530 --> 00:09:47.700
So you either have something that's compatible with human nature.

166
00:09:47.701 --> 00:09:52.560
So it's like a, you know, a big car with a V8 engine. Uh,

167
00:09:52.590 --> 00:09:56.790
but that's not compatible with, with nature or, um, yeah,

168
00:09:56.810 --> 00:10:00.150
something that's compatible with nature, which is probably walking,

169
00:10:00.151 --> 00:10:03.780
but it's not really compatible with human nature because we're lazy ingredient.

170
00:10:03.960 --> 00:10:06.810
<v 0>It's cold outside and you got to get somewhere in a snow storm. Exactly.</v>

171
00:10:06.810 --> 00:10:11.340
<v 1>So ideally what we do is rather than trying to change humans,</v>

172
00:10:11.370 --> 00:10:14.160
which I don't think is very futile activity,

173
00:10:14.161 --> 00:10:17.160
because there is such a thing as human nature, we have genes.

174
00:10:17.460 --> 00:10:19.770
We have this evolutionary history, uh,

175
00:10:19.800 --> 00:10:23.280
rather than trying to change that I think is much more effective to change the,

176
00:10:23.440 --> 00:10:27.750
the technology around us that, um, enables our,

177
00:10:28.320 --> 00:10:31.860
our inner desires and behaviors to, um,

178
00:10:31.950 --> 00:10:34.470
be positive rather than the negative.

179
00:10:34.950 --> 00:10:37.110
<v 0>I agree with you. I think it's going to be difficult though,</v>

180
00:10:37.111 --> 00:10:42.090
to get that same sort of a positive result.

181
00:10:43.560 --> 00:10:44.010
Um,

182
00:10:44.010 --> 00:10:48.780
w when it comes to our addiction to technology or a addiction to smartphones in

183
00:10:48.781 --> 00:10:51.990
particular, I mean, for a long time, it was like televisions, right?

184
00:10:52.320 --> 00:10:54.810
Like people talked about how much kids watch TV,

185
00:10:54.960 --> 00:10:58.530
kids watch TV eight hours a day. It's so much, it's so bad.

186
00:10:59.160 --> 00:11:00.600
You don't really hear that anymore. Right.

187
00:11:00.750 --> 00:11:05.280
You hear about phones and this is a sort of an UN discussed

188
00:11:05.580 --> 00:11:09.330
rapid shift in what we waste our time doing. Right.

189
00:11:09.390 --> 00:11:12.060
And most of it is what you hear people talking about.

190
00:11:12.061 --> 00:11:14.430
And most of the use of these phones, I would,

191
00:11:15.330 --> 00:11:19.620
I'd be willing to bet a giant chunk of that social media. Yes. Right.

192
00:11:20.250 --> 00:11:23.550
<v 1>Yeah. And I suppose that's, again,</v>

193
00:11:23.760 --> 00:11:28.380
this sort of infantile stage of that technology I.

194
00:11:28.560 --> 00:11:30.270
<v 0>An adolescent before.</v>

195
00:11:33.240 --> 00:11:34.110
So, uh.

196
00:11:34.170 --> 00:11:37.230
<v 1>I think you're right. Yeah. I mean, it's, it's, again,</v>

197
00:11:37.680 --> 00:11:42.480
probably we can engineer social media and, um, you know,

198
00:11:42.570 --> 00:11:47.370
our information technology to incentivize people to

199
00:11:47.430 --> 00:11:50.700
do good things. But indeed now it's, it's probably, uh,

200
00:11:50.790 --> 00:11:55.570
incentivizing the use of scrolling through timelines because you watch more ads.

201
00:11:55.960 --> 00:11:56.591
<v 0>I also,</v>

202
00:11:56.591 --> 00:12:01.330
I think it's our bodies and our minds and the way we view the world,

203
00:12:01.331 --> 00:12:05.680
it's, we're not designed to live in this digital realm.

204
00:12:06.220 --> 00:12:08.950
This is a completely new thing for the species.

205
00:12:08.980 --> 00:12:12.400
And I think we don't really know how to handle the dopamine rush that we get

206
00:12:12.760 --> 00:12:16.750
from clicking on Instagram and scrolling through your feeds and checking your

207
00:12:16.751 --> 00:12:21.160
DMS and reading your emails and constantly interacting with people and checking,

208
00:12:21.161 --> 00:12:23.860
did he text me back? Oh, what'd he say, oh, well, that's interesting.

209
00:12:24.100 --> 00:12:26.650
What about this and that, and this and that. You're just all day,

210
00:12:26.830 --> 00:12:29.980
all day interacting with some digital device we're not made for this.

211
00:12:30.160 --> 00:12:32.050
We're supposed to go outside. Yeah.

212
00:12:32.350 --> 00:12:36.880
<v 1>And then you have, you know, very bright engineers, somewhere in a big,</v>

213
00:12:36.881 --> 00:12:41.380
shiny building, um, AB testing all day to see whether, you know,

214
00:12:41.381 --> 00:12:45.910
a red dots on a certain icon in the social media app

215
00:12:46.240 --> 00:12:49.640
makes people click more or less. And that's yeah.

