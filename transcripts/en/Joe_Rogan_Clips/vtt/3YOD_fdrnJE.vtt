WEBVTT

1
00:00:01.020 --> 00:00:05.700
<v 0>The Joe Rogan experience. How much is media shifting now?</v>

2
00:00:05.730 --> 00:00:06.350
Like you've,

3
00:00:06.350 --> 00:00:09.930
you've obviously been a journalist for a long time at com how much are things

4
00:00:09.931 --> 00:00:12.300
changing in the light of the internet?

5
00:00:13.160 --> 00:00:15.140
<v 1>Well, a lot, and this is what I mean, I have a new book out now.</v>

6
00:00:15.141 --> 00:00:19.040
That's really about this, right? Why the business has changed? What's it called?

7
00:00:19.510 --> 00:00:23.480
Hayden, Inc. Yeah, it's out, it's out now. And, uh, it's,

8
00:00:23.570 --> 00:00:27.800
it's really about how the press, the business model of the press has changed.

9
00:00:27.801 --> 00:00:29.420
I mean, it's something that you talk about a lot.

10
00:00:29.660 --> 00:00:33.350
I hear you on your show all the time, talking about how, um,

11
00:00:33.830 --> 00:00:36.500
news agencies are always trying to push narratives on people,

12
00:00:36.501 --> 00:00:39.530
trying to get people wound up and upset. Uh,

13
00:00:39.680 --> 00:00:44.240
and that is a conscious business strategy that we didn't have maybe 30 years

14
00:00:44.241 --> 00:00:44.991
ago. You know,

15
00:00:44.991 --> 00:00:49.520
you think about Walter Cronkite or what the news was like back in the day you

16
00:00:49.521 --> 00:00:53.660
had the whole family sitting around the table and everybody watched,

17
00:00:53.720 --> 00:00:56.780
it was sort of a unifying experience to watch the news. Hm.

18
00:00:56.930 --> 00:01:00.080
Now you have news for the crazy right wing uncle,

19
00:01:00.081 --> 00:01:02.720
and then you have news for the kid in the shade tee shirt,

20
00:01:02.990 --> 00:01:05.300
and they're different channels. And they're trying to wind these,

21
00:01:05.390 --> 00:01:10.070
these people up, uh, you know, to get them upset constantly and stay there.

22
00:01:10.430 --> 00:01:13.490
And a lot of that has to do with the internet because, um,

23
00:01:13.970 --> 00:01:18.230
before the internet news companies had like a basically free way of making

24
00:01:18.231 --> 00:01:20.240
money, they dominated distribution.

25
00:01:20.510 --> 00:01:23.030
The newspaper was the only thing in town that had, uh, you know,

26
00:01:23.200 --> 00:01:26.540
if you wanted to get a wan ad, it had to be through the local newspaper.

27
00:01:27.110 --> 00:01:29.750
Now with the internet, the internet is the distribution system.

28
00:01:30.230 --> 00:01:34.400
Anybody has access to it, not just the local newspaper. And so there,

29
00:01:34.430 --> 00:01:39.170
the easy money is gone and we have to chase clicks more than we ever had, uh,

30
00:01:39.200 --> 00:01:41.510
had to before we have to chase eyeballs more than we have to.

31
00:01:41.511 --> 00:01:44.750
So we've had to build new money-making strategies and,

32
00:01:44.810 --> 00:01:48.620
and a lot of it has to do with just sort of monetizing anger and division and

33
00:01:48.621 --> 00:01:51.020
all these things. And we just didn't do that before.

34
00:01:51.080 --> 00:01:53.990
And it's had a profound difference on the, on, on.

35
00:01:53.990 --> 00:01:58.010
<v 0>The media, as a writer, have you personally experienced this sort of, uh,</v>

36
00:01:58.070 --> 00:02:02.930
the influence where people have tried to lean you in the direction of clickbait

37
00:02:02.931 --> 00:02:07.790
or perhaps maybe alter titles that make them a little bit disingenuous in order

38
00:02:07.791 --> 00:02:09.380
to get people excited about the story?

39
00:02:09.830 --> 00:02:13.100
<v 1>You know, I, I, my editors at rolling stone are, are, are pretty good in it.</v>

40
00:02:13.101 --> 00:02:16.640
And they gave me a lot of leeway to kind of explore whatever I want to explore,

41
00:02:16.641 --> 00:02:20.570
but I definitely feel a lot of pressure that I didn't feel before in the

42
00:02:20.571 --> 00:02:25.070
business, because, especially in the Trump era and, you know,

43
00:02:25.071 --> 00:02:27.720
I've written a lot about the Russia story, right. But you know,

44
00:02:27.750 --> 00:02:31.190
that's an example of one side's media does,

45
00:02:31.220 --> 00:02:34.580
has one take on it and another size media has another take on it.

46
00:02:34.610 --> 00:02:36.860
And if you are just the journalist and you,

47
00:02:37.490 --> 00:02:40.040
and you want to just sort of report the facts,

48
00:02:40.041 --> 00:02:43.520
you feel a lot of pressure to fit the facts into a narrative that your audience

49
00:02:43.521 --> 00:02:46.550
is going to like. And I had a lot of problem with the Russia story. Cause I,

50
00:02:46.590 --> 00:02:49.660
I thought, you know, I don't like Donald Trump, but I'm like, I don't,

51
00:02:49.880 --> 00:02:53.900
I don't think this guy's James Bond consorting with Russian spies.

52
00:02:53.901 --> 00:02:55.820
I think he's corrupt in other ways.

53
00:02:56.240 --> 00:02:59.740
And there was a lot of blow back on my side of the business. Um,

54
00:02:59.770 --> 00:03:04.600
because you know, people in sort of liberal quote unquote liberal media,

55
00:03:04.601 --> 00:03:05.520
you just have,

56
00:03:05.650 --> 00:03:08.770
there's a lot of pressure to have everybody fit into a certain narrative.

57
00:03:08.771 --> 00:03:11.370
And I think that's really unhealthy for the best. Yes. Yeah.

58
00:03:11.430 --> 00:03:12.480
<v 0>Very unhealthy. Right.</v>

59
00:03:12.690 --> 00:03:15.930
It's cause as soon as people can be manipulated to conforming to that narrative,

60
00:03:15.960 --> 00:03:18.750
then all sorts of stories can be shifted. Oh yeah.

61
00:03:18.960 --> 00:03:20.910
<v 1>Yeah, absolutely. And, and you,</v>

62
00:03:21.360 --> 00:03:25.380
the job used to be about challenging your audience every now and then, right?

63
00:03:25.381 --> 00:03:27.180
Like as you think a certain thing is true. Well,

64
00:03:27.181 --> 00:03:30.090
it's our job to give you the bad news and say that you're wrong about that.

65
00:03:30.091 --> 00:03:34.170
That used to be what the job was to be journalists. Now it's the opposite.

66
00:03:34.200 --> 00:03:38.340
Now we have an audience we're going to tell you exactly what you want to hear

67
00:03:38.341 --> 00:03:40.590
and what you, and we're going to reinforce what you think.

68
00:03:40.950 --> 00:03:42.600
And that's very unhealthy. I mean,

69
00:03:43.020 --> 00:03:47.190
a great example of this was in the summer of 2016,

70
00:03:48.300 --> 00:03:51.990
um, I was covering the campaign. I started to hear, um,

71
00:03:52.080 --> 00:03:55.800
reporters talking about how they didn't want to report poll numbers that showed

72
00:03:55.801 --> 00:04:00.090
the race was close. They thought that that was going to hurt Hillary. Right?

73
00:04:00.210 --> 00:04:01.021
Like we said, in other words,

74
00:04:01.021 --> 00:04:05.010
we had information that the race was close and we're not telling this to

75
00:04:05.011 --> 00:04:08.820
audiences because they wanted to hear that it was going to be a blowout for

76
00:04:08.821 --> 00:04:12.480
Hillary right on. And that didn't help Hillary.

77
00:04:12.481 --> 00:04:16.560
It didn't help the Democrats to not warn people about this. Right. Um,

78
00:04:16.590 --> 00:04:21.510
but it was just because if you turned on MSNBC or CNN and you heard that

79
00:04:21.540 --> 00:04:24.060
Trump was within five points or whatever it was,

80
00:04:24.480 --> 00:04:27.600
that was going to be a bummer for that audience. So we stayed away from it.

81
00:04:28.050 --> 00:04:31.340
And you know, this is the kind of thing that it's,

82
00:04:31.341 --> 00:04:34.350
it's not politically beneficial to anybody. It's just,

83
00:04:34.410 --> 00:04:37.710
we're just trying to keep people glued to the set by telling them what they want

84
00:04:37.711 --> 00:04:40.890
to hear. And that's not the news. That's not the, that's not our job, you know?

85
00:04:41.140 --> 00:04:42.630
Uh, and it, it, it drives me crazy.

86
00:04:43.260 --> 00:04:44.880
<v 2>Yeah. Yeah. It should drive you crazy that.</v>

87
00:04:44.910 --> 00:04:49.080
<v 0>What you said about journalism being used to be something that you're</v>

88
00:04:49.081 --> 00:04:50.810
challenging your reader, you're,

89
00:04:50.811 --> 00:04:55.080
you're giving them this reality that may be uncomfortable, but it's,

90
00:04:55.110 --> 00:04:59.140
it's educational and expands their view of the world this well,

91
00:04:59.141 --> 00:05:00.720
where do they get that? Now? They.

92
00:05:00.720 --> 00:05:04.410
<v 1>Don't, that's the whole problem. Like you get,</v>

93
00:05:04.411 --> 00:05:08.970
you can predict exactly what the, each news organization,

94
00:05:08.971 --> 00:05:12.960
what their take is going to be on any issue by going, oh, I'll just to get,

95
00:05:12.961 --> 00:05:15.570
take an example. When, um,

96
00:05:15.960 --> 00:05:19.650
when the business about the ISIS leader al-Baghdadi being killed,

97
00:05:21.090 --> 00:05:23.580
um, hit the news instantaneously.

98
00:05:23.581 --> 00:05:27.150
You knew that the New York times CNN, the Washington post,

99
00:05:27.151 --> 00:05:30.090
that they were going to write a whole bunch of stories about how Trump was

100
00:05:30.091 --> 00:05:34.410
overplaying, the significance of it, that he, you know, um,

101
00:05:34.740 --> 00:05:37.740
that he was telling lies about it. They were, they made, they,

102
00:05:37.830 --> 00:05:41.070
you knew they were going to make the entire thing about Trump. Uh,

103
00:05:41.100 --> 00:05:41.731
and then meanwhile,

104
00:05:41.731 --> 00:05:44.850
Fox had a completely different spin on about how heroic it was. But,

105
00:05:44.910 --> 00:05:47.130
but newsletter insist didn't have anywhere to go to,

106
00:05:47.131 --> 00:05:50.910
to just simply hear who was this person? Why was he important?

107
00:05:50.940 --> 00:05:54.780
What were the roles of the people in the region? Think, you know, what kind of,

108
00:05:54.810 --> 00:05:56.520
what is this going to mean? Going forward?

109
00:05:56.640 --> 00:06:00.410
Is it actually going to have any impact, you know, is,

110
00:06:00.440 --> 00:06:03.800
are we going to have to continually, um, you know,

111
00:06:03.801 --> 00:06:06.650
is there going to be a new person like this every, every time, right?

112
00:06:06.651 --> 00:06:09.200
Are we actually accomplishing, like you don't get that anywhere.

113
00:06:09.201 --> 00:06:13.700
All you get is Trump has a head on one side and Trump is a hero on the other

114
00:06:13.701 --> 00:06:15.830
side and that's not the news, you know?

115
00:06:16.270 --> 00:06:21.220
<v 0>Yeah. And, but the thing is, it's like the business aspect of it is so weird.</v>

116
00:06:21.460 --> 00:06:24.940
Like you have your guys like Hannity where you can absolutely predict what that

117
00:06:24.941 --> 00:06:26.470
guy's going to say every single time,

118
00:06:26.471 --> 00:06:29.560
you know what side he's on and he's blatant about it.

119
00:06:29.950 --> 00:06:34.660
And when you see someone like that, you go, okay, well, this is okay,

120
00:06:35.110 --> 00:06:38.890
this is, this is peak. Right? So where, where do we go?

121
00:06:39.010 --> 00:06:41.530
Where I see both sides? Where's the, where's the,

122
00:06:41.531 --> 00:06:44.110
where's the middle ground where someone goes, well, this is true.

123
00:06:44.111 --> 00:06:46.600
But you got to say, this is honest too. And this is,

124
00:06:46.780 --> 00:06:48.280
this is what's going on over on this side.

125
00:06:48.460 --> 00:06:51.550
And the Republicans have a point here and you don't, you don't,

126
00:06:51.760 --> 00:06:56.260
there's no mainstream media place where you can go for that. Right? No, there.

127
00:06:56.260 --> 00:06:58.080
<v 1>Isn't. And that's, I mean, I mean,</v>

128
00:06:58.140 --> 00:07:00.040
one of this is one of the things I write about this is one of the reasons why

129
00:07:00.041 --> 00:07:01.840
shows like yours are so popular. I mean, I, I,

130
00:07:01.900 --> 00:07:05.800
I think there's a complete loss of trust that they feel like people are not

131
00:07:05.801 --> 00:07:09.400
being honest with them. Right. And they're not being straight. And you know,

132
00:07:09.401 --> 00:07:14.190
they, they come to people like you and, and a lot of other people, uh,

133
00:07:14.230 --> 00:07:18.160
sort of independent folks who aren't like the quote unquote mainstream media,

134
00:07:19.180 --> 00:07:23.590
um, because they, it's not really thought it's not reporting.

135
00:07:23.591 --> 00:07:25.870
It's not anything. If you can predict a hundred percent what a,

136
00:07:25.871 --> 00:07:29.710
person's going to say, that's not thinking that's not reporting. That's not,

137
00:07:29.711 --> 00:07:30.580
it's just marketing.

138
00:07:31.150 --> 00:07:32.260
<v 0>Like me. That's so disturbing.</v>

139
00:07:32.261 --> 00:07:36.850
I'm a comedian in a cage fighting commentator when people are coming to me

140
00:07:37.090 --> 00:07:37.721
like, this is,

141
00:07:37.721 --> 00:07:41.680
this is the source where you go for unbiased representations of what's going on

142
00:07:41.681 --> 00:07:43.540
in the world. That's crazy. Well, I mean.

143
00:07:44.410 --> 00:07:47.920
<v 1>I mean, I saw your interview with Barry Weiss, right. And you just,</v>

144
00:07:48.010 --> 00:07:51.820
you did a simple base. You didn't go to journalism school, right? No, no.

145
00:07:51.821 --> 00:07:56.530
So she said something about how, um, you know, oh,

146
00:07:56.531 --> 00:08:01.150
she's an Assad toady and you said, what does that mean? You just ask the simple,

147
00:08:01.180 --> 00:08:04.810
basic questions. Right? What does that mean? Where where's that coming from?

148
00:08:04.811 --> 00:08:08.800
How do you know that? Yeah. Like journalism isn't brain surgery.

149
00:08:08.801 --> 00:08:09.634
That's all it is.

150
00:08:09.700 --> 00:08:12.730
It's just asking the simple questions that sort of popped to mind when you,

151
00:08:13.240 --> 00:08:15.340
when you're in a situation, like, where did this happen?

152
00:08:15.341 --> 00:08:18.160
How do we know that that's true. And,

153
00:08:18.340 --> 00:08:22.480
but there's a whole generation of people in the press now who just simply do not

154
00:08:23.140 --> 00:08:26.470
do that, go through the process of just asking simple questions.

155
00:08:26.510 --> 00:08:29.290
How do I know that's true. Like after each story, your report,

156
00:08:29.530 --> 00:08:32.830
you're supposed to kinda like wipe your memory clean and start over.

157
00:08:33.010 --> 00:08:35.800
So just because somebody was banned the last time you covered them,

158
00:08:35.830 --> 00:08:38.800
doesn't mean that they're necessarily going to be the bad guy this time you

159
00:08:38.801 --> 00:08:39.770
cover them. Right.

160
00:08:40.150 --> 00:08:44.530
You have to continually test your assumptions and ask yourself,

161
00:08:44.531 --> 00:08:47.590
is this true? Is that true? Is this true? How do we know this?

162
00:08:47.980 --> 00:08:50.530
And we've just stopped doing that. Like the,

163
00:08:50.531 --> 00:08:55.440
it's just the morass of like pre-written takes on things

164
00:08:55.500 --> 00:09:00.480
and it's, it's really, really bad. Uh, and you can see why audiences are, are,

165
00:09:00.530 --> 00:09:03.360
are fleeing from this stuff. They, they just don't have the impact they used to.

166
00:09:03.380 --> 00:09:03.650
Well, it's.

167
00:09:03.650 --> 00:09:04.670
<v 0>Really interesting that this,</v>

168
00:09:04.730 --> 00:09:09.470
a lot of this is this unpredicted consequence of having these open platforms

169
00:09:09.471 --> 00:09:12.590
like Facebook and like where, where people are getting their news.

170
00:09:12.591 --> 00:09:16.940
And then the algorithm sort of directs them towards things that are going to

171
00:09:17.090 --> 00:09:21.860
them off, which I don't even think necessarily was initially the plan.

172
00:09:21.861 --> 00:09:25.820
I think the plan is to accelerate engagement, right? So they find out what,

173
00:09:26.060 --> 00:09:29.300
what w what you're engaging with, what stories you're engaging with.

174
00:09:29.570 --> 00:09:32.840
And then they give you more of that. Like Ari, my friend, Ari,

175
00:09:32.841 --> 00:09:35.840
Shaffir actually tried this out.

176
00:09:36.200 --> 00:09:41.120
And what he did was he went on YouTube and only looked up puppy videos.

177
00:09:41.840 --> 00:09:43.910
And that's all he looked at for like weeks.

178
00:09:44.270 --> 00:09:48.410
And then YouTube only started recommending puppy videos to him.

179
00:09:48.830 --> 00:09:52.580
So it's not necessarily that Facebook wants you to be outraged,

180
00:09:52.610 --> 00:09:54.440
but that when you are outraged,

181
00:09:54.441 --> 00:09:57.050
whether it's over abortion or war or whatever the subject is,

182
00:09:57.230 --> 00:10:01.130
you're going to engage more. And their algorithm favors you engaging more.

183
00:10:01.550 --> 00:10:04.730
So if you were engaging more about something very positive, you know,

184
00:10:04.731 --> 00:10:06.440
if you're all about yoga and meditation,

185
00:10:06.470 --> 00:10:09.800
your algorithm would probably favor yoga and meditation,

186
00:10:09.980 --> 00:10:11.450
because those are the things that you engage with.

187
00:10:11.960 --> 00:10:16.730
But it's natural for people to be off and to look for things that are

188
00:10:17.090 --> 00:10:19.130
knowing, especially if you're done working and you're like, ah,

189
00:10:19.170 --> 00:10:21.380
this world sucks. What's going on. That sucks worse.

190
00:10:21.650 --> 00:10:25.460
And then you go to your Facebook and, oh, Jesus, look at this border crisis,

191
00:10:25.490 --> 00:10:29.450
right? Oh, Jesus. Look at this while, here's the problem with these liberal.

192
00:10:29.451 --> 00:10:33.760
They don't know. And the you engage and then that's your life. And then it's,

193
00:10:33.761 --> 00:10:36.170
it's saying, oh, I know how to get mad, all fired up.

194
00:10:36.200 --> 00:10:39.200
I'm going to send them some abortion stories. Whoa. Right.

195
00:10:39.230 --> 00:10:40.250
And then that's your feed.

196
00:10:40.400 --> 00:10:41.810
<v 1>Right? Yeah, exactly. But the,</v>

197
00:10:42.100 --> 00:10:44.960
but there's so many economic incentives that go in there. Right.

198
00:10:44.961 --> 00:10:49.010
They know that the more that you engage, the longer that you're on,

199
00:10:49.400 --> 00:10:52.640
the more ads you're going to see. Right.

200
00:10:52.641 --> 00:10:56.570
So that same dynamic that Facebook and the social media companies

201
00:10:57.290 --> 00:11:00.530
figured out, which is that if you keep feeding something, somebody,

202
00:11:00.531 --> 00:11:01.910
something that, you know,

203
00:11:01.911 --> 00:11:05.030
has been proven to spin that person up and get them wound up,

204
00:11:05.360 --> 00:11:08.390
that they're gonna, they're going to come back for more of it.

205
00:11:08.391 --> 00:11:09.530
And they're going to keep coming back.

206
00:11:09.531 --> 00:11:14.420
And actually you can expand their desire to, to see that stuff by,

207
00:11:14.450 --> 00:11:18.980
by making them sort of more angry overall. And they will,

208
00:11:19.010 --> 00:11:21.440
they will come back and they will spend more and more and more time. Well,

209
00:11:21.441 --> 00:11:23.810
the news companies figured out the same thing, and they're just,

210
00:11:23.870 --> 00:11:28.120
they're just funneling stuff at you that they know you're gonna, you're,

211
00:11:28.121 --> 00:11:31.490
you're going to just be in an endless cycle of sort of impotent mute,

212
00:11:31.491 --> 00:11:35.570
rage all the time, but it's kind of addicting, you know, and they know that.

213
00:11:35.800 --> 00:11:38.270
And, and, and it's, it's sort of like the tobacco companies, they,

214
00:11:38.271 --> 00:11:40.640
they know it's a bad, it's a product that's bad for you.

215
00:11:41.090 --> 00:11:43.850
And they just keep giving it to you because you know, it makes money for them.

216
00:11:44.480 --> 00:11:45.140
Yeah.

217
00:11:45.140 --> 00:11:49.430
<v 0>And it's just the thing about it is all of it is about</v>

218
00:11:49.460 --> 00:11:52.490
ads. Total, how many clicks? They get an ads.

219
00:11:52.550 --> 00:11:56.560
If they just said you can have a social media company, but you can't have ads.

220
00:11:57.070 --> 00:12:02.020
There's a new federal law. No more ads on Facebook. No more ads on YouTube,

221
00:12:02.500 --> 00:12:05.350
nowhere ads on Twitter. No more ads on Instagram. Good.

222
00:12:05.350 --> 00:12:07.600
<v 2>Luck, right? Yeah. Totally.</v>

223
00:12:07.960 --> 00:12:11.230
<v 0>Collapse. Yup. Yeah. But that seems to be what it is. It's like,</v>

224
00:12:11.231 --> 00:12:16.120
they figured out that your data is worth a tremendous amount of money and the

225
00:12:16.121 --> 00:12:19.810
way they can utilize that money is to sell advertising.

226
00:12:20.340 --> 00:12:22.640
<v 1>Yeah. No, they, they get it coming and going, because they're,</v>

227
00:12:22.641 --> 00:12:25.110
they're not only selling you ads or, or,

228
00:12:25.140 --> 00:12:27.780
but they're also collecting the information about your habits,

229
00:12:27.781 --> 00:12:30.420
which they can then sell, sell again. So it's, it's a,

230
00:12:30.440 --> 00:12:34.620
it's a dual revenue stream. You know, the media companies,

231
00:12:34.980 --> 00:12:38.310
they're basically, they're just consumer businesses where they're,

232
00:12:38.370 --> 00:12:42.290
they're trading attention for ad space, right. So if they can get you to,

233
00:12:42.291 --> 00:12:44.040
to watch four hours of television a day,

234
00:12:44.041 --> 00:12:47.340
they have that many ad slots that they can show you and they know how much money

235
00:12:47.341 --> 00:12:49.470
they're going to make, you know, but the, the,

236
00:12:49.471 --> 00:12:52.590
the social media companies get it two ways. They're they, they get it by,

237
00:12:53.100 --> 00:12:55.320
you know, attracting your eyeballs and then also selling it,

238
00:12:55.321 --> 00:12:59.670
selling your habits to the other, the next set of advertisers, which you know,

239
00:12:59.671 --> 00:13:00.391
is very insidious.

240
00:13:00.391 --> 00:13:04.380
But what's interesting about this is that most people don't think about this as

241
00:13:04.381 --> 00:13:06.570
a consumer business, right? Like Americans,

242
00:13:06.571 --> 00:13:09.660
these are very conscious of like what they put in their bodies. You know,

243
00:13:09.661 --> 00:13:12.510
they won't eat too many candy. Well, depending on who they are. Right.

244
00:13:12.511 --> 00:13:14.370
But people at least look at what the calories are,

245
00:13:14.371 --> 00:13:18.270
but they don't think about the news that way or social media, what that,

246
00:13:18.271 --> 00:13:21.420
what they put in their brains. And it's also a consumer product. Yeah.

247
00:13:21.421 --> 00:13:21.810
It really.

248
00:13:21.810 --> 00:13:25.800
<v 0>Is. I've, I've gone over that many times with people that that's a diet,</v>

249
00:13:26.220 --> 00:13:29.280
this is your diet. You have a mental diet, as well as you have a physical,

250
00:13:29.700 --> 00:13:32.700
like food diet absolutely have an information diet.

251
00:13:32.790 --> 00:13:36.090
And a lot of people are just eating with their brain. It's.

252
00:13:36.090 --> 00:13:39.810
<v 1>The worst kind of junk food. It's like, it's like a cigarette sandwich,</v>

253
00:13:39.900 --> 00:13:43.440
the stuff it's so bad and it's getting worse. It.

254
00:13:43.440 --> 00:13:44.790
<v 0>Is, it is getting worse. And it's,</v>

255
00:13:44.791 --> 00:13:47.160
what's weird is that this is a ten-year-old problem.

256
00:13:47.161 --> 00:13:50.760
And no one saw it coming and it's kind of overtaken politics.

257
00:13:50.761 --> 00:13:52.890
It's overtaking social discourse.

258
00:13:53.220 --> 00:13:56.340
Everybody's wrapped up in social media conversations.

259
00:13:56.341 --> 00:14:00.510
They carry them on over to the dinner table and it gets people in arguments at

260
00:14:00.511 --> 00:14:05.220
work and all this stuff, no one saw coming these that no one saw the,

261
00:14:05.221 --> 00:14:08.790
this outrage economy from, you know,

262
00:14:08.791 --> 00:14:12.330
social media sites from things like Facebook, no one saw that no one,

263
00:14:12.360 --> 00:14:16.260
no one ever predicted that your data was going to be so valuable. No,

264
00:14:16.261 --> 00:14:17.094
the saw that.

265
00:14:17.310 --> 00:14:19.320
<v 1>I don't think anybody. I mean,</v>

266
00:14:19.321 --> 00:14:22.920
I think some people in the tech business probably saw early on the potential,

267
00:14:22.980 --> 00:14:25.770
the potential for this, but, you know,

268
00:14:25.771 --> 00:14:30.270
in terms of other other businesses, like the news media and also politics,

269
00:14:30.271 --> 00:14:30.481
I mean,

270
00:14:30.481 --> 00:14:34.380
you have to think about the impact of this on politics has been enormous.

271
00:14:34.490 --> 00:14:36.600
I mean, you know, I covered Donald Trump.

272
00:14:36.960 --> 00:14:41.040
Trump really was just all about whatever you're off about.

273
00:14:41.041 --> 00:14:42.690
I'm right there with you, you know,

274
00:14:42.900 --> 00:14:45.630
and people are just sort of off about lots of things these days,

275
00:14:45.631 --> 00:14:49.410
because they're doing this all day long, you know, and if you, if you can,

276
00:14:49.860 --> 00:14:52.190
if you can, uh, take advantage of that,

277
00:14:52.191 --> 00:14:53.780
then you're going to have a lot of success. And I think,

278
00:14:53.810 --> 00:14:57.110
I think a lot of people haven't figured that out and some of these things are

279
00:14:57.111 --> 00:15:01.940
real causes. Like people are upset about real things. Um, but it's just,

280
00:15:02.510 --> 00:15:03.410
uh, you're absolutely right.

281
00:15:03.411 --> 00:15:05.360
People did not see this coming and they didn't prepare.

282
00:15:05.360 --> 00:15:06.193
<v 0>For it.</v>

283
00:15:06.390 --> 00:15:10.660
Weird that it's one of the biggest sources of income online and people didn't

284
00:15:10.661 --> 00:15:11.740
see it coming. I mean,

285
00:15:11.741 --> 00:15:16.150
Facebook is generating billions of dollars and now potentially

286
00:15:16.151 --> 00:15:18.640
shifting global policy ticks.

287
00:15:19.060 --> 00:15:22.680
<v 1>Yeah. And, uh, you know, the, the whole issue of,</v>

288
00:15:22.810 --> 00:15:27.610
of a couple of companies like Facebook having control over what you do and do

289
00:15:27.611 --> 00:15:31.900
not see is an enormous problem that nobody, nobody really cares about.

290
00:15:31.901 --> 00:15:33.790
I've tried to write about it a few times.

291
00:15:33.791 --> 00:15:36.730
I've written a couple of features about it and about how,

292
00:15:37.120 --> 00:15:41.650
what a serious problem this is. Like, if you look at other countries like, um,

293
00:15:41.800 --> 00:15:44.440
Israel, uh, China, there,

294
00:15:44.470 --> 00:15:46.450
there are a number of companies where you've seen this,

295
00:15:46.480 --> 00:15:49.450
this pattern of internet platforms,

296
00:15:49.780 --> 00:15:53.770
liaising with the government to decide what people can and cannot see.

297
00:15:54.100 --> 00:15:56.770
And they'll, they'll say, well, we don't want to see, you know,

298
00:15:56.800 --> 00:16:00.790
Palestinian protest movements. So we don't want to see, um, you know, the,

299
00:16:00.950 --> 00:16:04.060
the Venezuelan channel, tell us soar, look, we want to take that off.

300
00:16:04.490 --> 00:16:07.870
You think about how that could end up happening in the United States.

301
00:16:07.871 --> 00:16:10.480
And it is already a little bit happening it's a little.

302
00:16:10.480 --> 00:16:13.750
<v 0>Bit, but it seems to be happening only in terms of like leaning towards the</v>

303
00:16:13.751 --> 00:16:16.330
progressive side, which people are okay with. Because I think,

304
00:16:16.360 --> 00:16:19.090
especially in the light of Donald Trump being in office,

305
00:16:19.091 --> 00:16:21.640
this is acceptable censorship. Yeah. But they're,

306
00:16:21.660 --> 00:16:23.740
I think they're wrong about that. They're wrong about that too.

307
00:16:24.280 --> 00:16:25.750
And terribly dangerous.

308
00:16:25.780 --> 00:16:29.830
<v 1>It's very short-sighted. Yes. And they, and, and I think there's,</v>

309
00:16:29.860 --> 00:16:34.330
there's also this thing that happens with, um, people where they think, oh,

310
00:16:34.340 --> 00:16:36.400
this is never going to happen to me. You know,

311
00:16:36.401 --> 00:16:40.450
like you can do that bad thing to this person that I don't like, but, you know,

312
00:16:40.451 --> 00:16:43.360
as long as it's never going to happen to me, but they're wrong. I mean,

313
00:16:43.361 --> 00:16:46.270
history shows that always does happen to you, you know? And that's,

314
00:16:46.630 --> 00:16:50.560
so we're giving these companies an enormous amount of power to decide all kinds

315
00:16:50.561 --> 00:16:53.410
of things. What we, what we look at, um, what,

316
00:16:53.440 --> 00:16:57.100
what kind of political ideas we can be exposed to? Um, you know,

317
00:16:57.490 --> 00:16:58.900
I think it's very, very dangerous.

318
00:16:59.710 --> 00:17:01.630
<v 0>Biased interpretation of what something is.</v>

319
00:17:01.660 --> 00:17:06.610
That was what people talked about when the initial Patriot act was enacted.

320
00:17:06.640 --> 00:17:11.290
When people were like, Hey, this might be fine with Obama in office, right.

321
00:17:11.650 --> 00:17:16.630
Maybe Obama is not going to enact some of the worst clauses of this

322
00:17:16.631 --> 00:17:21.520
and use it on people. Or the, um, was the NDA. Is that what it was?

323
00:17:21.590 --> 00:17:25.030
Yeah. Where some of the things were just completely unconstitutional,

324
00:17:25.060 --> 00:17:26.350
but don't worry. We're not going to use those,

325
00:17:26.620 --> 00:17:31.600
but you're setting these tools aside for whatever president

326
00:17:31.601 --> 00:17:34.720
we have. Like, what if we have a guy who out Trump's Trump, right? I mean,

327
00:17:34.721 --> 00:17:36.280
we never thought we'd have a Trump, right.

328
00:17:36.460 --> 00:17:38.830
What if we have a next level guy post Trump?

329
00:17:39.070 --> 00:17:42.640
What if there's some sort of catastrophe,

330
00:17:42.670 --> 00:17:44.470
tragedy attack,

331
00:17:44.560 --> 00:17:48.340
something that really gets people fired up and they vote in someone who takes it

332
00:17:48.341 --> 00:17:50.640
up to another level, and then he has these tools.

333
00:17:50.641 --> 00:17:52.680
And then he uses these tools on his political enemies,

334
00:17:52.710 --> 00:17:54.620
which is entirely it's possible. Well.

335
00:17:54.740 --> 00:17:57.090
<v 1>I mean, we've already seen that a little bit. I mean,</v>

336
00:17:57.320 --> 00:17:58.670
people don't want to bring this up,

337
00:17:59.030 --> 00:18:02.450
but in a lot of the stories that have come out about Trump,

338
00:18:02.480 --> 00:18:06.680
they're coming from leaks of classified information that are coming from those

339
00:18:06.890 --> 00:18:10.000
war on terror programs that were instituted after 9 11, 5,

340
00:18:10.001 --> 00:18:14.390
those sort of fives and amendments act, the NSA programs to collect data.

341
00:18:14.391 --> 00:18:16.040
Like they're, they're unmasking people.

342
00:18:16.041 --> 00:18:19.130
Like we have a lot of evidence now that there was a lawsuit,

343
00:18:19.131 --> 00:18:23.690
a couple that came out about a month ago that showed that the FBI

344
00:18:23.691 --> 00:18:27.260
was doing something like 60,000 searches a month,

345
00:18:27.530 --> 00:18:28.790
at one point where they're on,

346
00:18:29.090 --> 00:18:33.380
they were asking the NSA for the ability to unmask names and that sort of thing.

347
00:18:34.010 --> 00:18:37.910
So we're, I mean, these tools are incredibly powerful.

348
00:18:37.911 --> 00:18:40.340
They're incredibly dangerous, but people thought after nine 11,

349
00:18:40.430 --> 00:18:43.610
they were scared. So, you know, we want to protect ourselves.

350
00:18:43.611 --> 00:18:47.960
So that's okay for now, you know, uh, we'll, we'll pull it back later, but they,

351
00:18:48.230 --> 00:18:50.300
but you never do pull it back. Right. You know what I mean?

352
00:18:50.340 --> 00:18:54.020
It always ends up being used by somebody in the wrong way.

353
00:18:54.021 --> 00:18:56.750
And I think we're starting to see that that's going to be a problem.

354
00:18:57.080 --> 00:19:01.760
<v 0>Yeah. I'm real concerned about places like Google and Facebook</v>

355
00:19:02.150 --> 00:19:07.130
altering the path of free speech and leaning

356
00:19:07.190 --> 00:19:10.970
people in certain directions and silencing people that have opposing viewpoints

357
00:19:11.360 --> 00:19:14.930
and the fact that they think that they're doing this for good,

358
00:19:14.931 --> 00:19:17.000
because this is how they see the world.

359
00:19:17.300 --> 00:19:21.140
And they don't understand that you have to let these ideas play out in the

360
00:19:21.141 --> 00:19:24.380
marketplace of free speech and free ideas. If you don't do that,

361
00:19:24.710 --> 00:19:27.740
if you don't do that, if you don't let people debate the merits, the pros,

362
00:19:27.741 --> 00:19:30.710
the cons, what's wrong. What's right. If you don't do that,

363
00:19:30.740 --> 00:19:32.930
then you don't get real discourse. If you don't get real discourse,

364
00:19:33.140 --> 00:19:36.770
you're essentially, you've got some sort of intellectual dictatorship going on.

365
00:19:37.070 --> 00:19:39.140
And because it's a progressive dictatorship,

366
00:19:39.170 --> 00:19:42.830
you think it's okay because it's people who want everybody to be inclusive. And,

367
00:19:43.190 --> 00:19:46.700
you know, I mean, this is, this is a weird time for that.

368
00:19:46.701 --> 00:19:49.010
It's a really weird time for that. Because as you said,

369
00:19:49.280 --> 00:19:50.750
people are so shortsighted,

370
00:19:51.050 --> 00:19:54.440
they don't understand that these like the first amendments in place for a very

371
00:19:54.441 --> 00:19:57.950
good reason, it's set up a long time ago. Cause they did the math,

372
00:19:58.070 --> 00:20:00.020
they saw where it was going. And they were like, look,

373
00:20:00.021 --> 00:20:02.060
we have to have the ability to express ourselves.

374
00:20:02.180 --> 00:20:06.800
We have to have the ability to freely express thoughts and ideas and challenge

375
00:20:07.010 --> 00:20:08.960
people that are in a position of power. Because if we don't,

376
00:20:09.140 --> 00:20:11.210
we wind up exactly where we came from.

377
00:20:11.840 --> 00:20:12.611
<v 1>Yeah, no. And,</v>

378
00:20:12.611 --> 00:20:17.420
and courts continually reaffirmed that idea that the,

379
00:20:17.600 --> 00:20:18.320
the,

380
00:20:18.320 --> 00:20:22.430
the way to deal with bad speech was with more speech and they did it over and

381
00:20:22.431 --> 00:20:25.700
over and over again. You know, we, we, the,

382
00:20:25.701 --> 00:20:28.400
the legal standard for speech still,

383
00:20:28.401 --> 00:20:33.260
I think remains that unless it's directly inciting

384
00:20:33.261 --> 00:20:34.220
violence, you can, you can, like,

385
00:20:34.221 --> 00:20:36.980
you can have speech that incites violence generally,

386
00:20:36.981 --> 00:20:38.150
and even in the Supreme court,

387
00:20:38.151 --> 00:20:41.840
even upheld that you can have speech that's that comes from, you know,

388
00:20:41.841 --> 00:20:46.130
material that was stolen illegally that's okay. Um, but we had a very,

389
00:20:46.131 --> 00:20:50.860
very high bar for speech always. And, you know, the, the libel cases,

390
00:20:51.100 --> 00:20:55.870
the cases for defamation, um, you know, that also established a very,

391
00:20:55.871 --> 00:20:57.580
very high standard for punishing speech.

392
00:20:58.150 --> 00:21:01.300
But now all of a sudden people have a completely different idea about it.

393
00:21:01.301 --> 00:21:02.230
It's like w you know,

394
00:21:02.770 --> 00:21:06.490
forget about the fact that this was a fundamental concept in American society

395
00:21:06.491 --> 00:21:10.720
for, you know, 230 years, or would it be, they just want to change it, you know,

396
00:21:10.721 --> 00:21:12.280
without thinking about the consequences.

397
00:21:12.630 --> 00:21:15.690
<v 0>I swear, a guy like Trump could be almost like,</v>

398
00:21:16.830 --> 00:21:21.810
it's almost like a Trojan horse in a way, like, if you wanted to play 3d chess,

399
00:21:22.230 --> 00:21:26.340
which you would do, you'd get a guy who's just so gracious and so outrageous.

400
00:21:26.370 --> 00:21:29.220
And then so many people oppose him, get that guy,

401
00:21:29.280 --> 00:21:32.010
let him get into a position of power and then sit back,

402
00:21:32.160 --> 00:21:33.690
watch the outrage bubble,

403
00:21:33.810 --> 00:21:36.960
and then take advantage of that and funnel people into certain directions.

404
00:21:37.140 --> 00:21:38.910
I mean, I don't think that's what's happening,

405
00:21:39.240 --> 00:21:42.660
but if I was super tinfoil Hattie,

406
00:21:42.870 --> 00:21:45.540
that's how I would go about it. I would say, this is what you want.

407
00:21:45.541 --> 00:21:48.420
If you really want to change things for your direction,

408
00:21:48.690 --> 00:21:52.020
put someone that opposes it, that's disgusting.

409
00:21:52.500 --> 00:21:55.440
And that way people just a rational,

410
00:21:55.470 --> 00:21:58.200
intelligent person is never going to side with him.

411
00:21:58.440 --> 00:22:00.120
So they're going to side with the people that oppose him,

412
00:22:00.121 --> 00:22:03.240
and then you could sneak a lot of in that maybe they wouldn't agree with in any

413
00:22:03.241 --> 00:22:04.110
other circumstance.

414
00:22:04.410 --> 00:22:08.610
<v 1>Yeah. Trump's election is sort of like another nine 11, right? Like, you know,</v>

415
00:22:08.611 --> 00:22:12.030
nine 11 happened all of a sudden people who weren't in favor of the government,

416
00:22:12.031 --> 00:22:15.480
being able to go through your library records or listen to your phone calls.

417
00:22:15.481 --> 00:22:18.690
And all of a sudden they were like, oh, Jesus, I'm so freaked out. Like, yeah,

418
00:22:18.720 --> 00:22:20.580
fine. When Trump got elected,

419
00:22:20.581 --> 00:22:24.570
all of a sudden people suddenly had very different ideas about speech, right?

420
00:22:24.600 --> 00:22:28.800
Like they, you know, Hey, that guy is so bad. Um, you know,

421
00:22:29.070 --> 00:22:33.810
that maybe we should consider banning X, Y, and Z, you know? And, uh, yeah.

422
00:22:34.110 --> 00:22:35.280
It's if.

423
00:22:36.890 --> 00:22:37.530
<v 2>If he.</v>

424
00:22:37.530 --> 00:22:40.860
<v 1>Was conceived as a, as a, as a way to discredit, uh,</v>

425
00:22:40.861 --> 00:22:43.620
the first amendment and some other ideas that would, that would,

426
00:22:43.621 --> 00:22:45.720
that would be a brilliant 3d chess move.

427
00:22:46.380 --> 00:22:48.420
<v 2>Super sneaky. That's.</v>

428
00:22:48.420 --> 00:22:51.810
<v 0>Like China level, many steps ahead, right? Yeah, exactly.</v>

