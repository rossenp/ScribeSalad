WEBVTT

1
00:00:00.960 --> 00:00:01.680
<v 0>The Joe Rogan.</v>

2
00:00:01.680 --> 00:00:06.510
<v 1>Experience. So particulate pollution has gotten so much better. In fact,</v>

3
00:00:06.540 --> 00:00:11.400
one wrinkle of, uh, climate change and global warming,

4
00:00:11.910 --> 00:00:16.140
is that the particular, it's the suit in the atmosphere in the sixties,

5
00:00:16.170 --> 00:00:19.920
50 sixties and seventies was apparently what kept things a little bit cooler

6
00:00:19.980 --> 00:00:22.530
because it refracted sunlight and heat, right?

7
00:00:23.400 --> 00:00:27.930
So the irony is you clean up the air and you allow more warming. Yeah,

8
00:00:27.960 --> 00:00:31.680
exactly. Anyway, there's a global warming is a very complicated issue.

9
00:00:31.920 --> 00:00:35.010
This is another example where when people reduce it to the headlines and then

10
00:00:35.011 --> 00:00:38.730
divide people into tribes, it's exactly the opposite of what you want.

11
00:00:39.120 --> 00:00:42.000
<v 0>Example, because it's clearly a right versus left thing too.</v>

12
00:00:42.001 --> 00:00:45.150
In some people's circles. If you're on the right,

13
00:00:45.151 --> 00:00:48.440
you're supposed to say it's exaggerated and it's a hoax, it's this it's that.

14
00:00:48.720 --> 00:00:50.910
That's not my concern. My concern is jobs.

15
00:00:50.911 --> 00:00:54.540
My concern is that like you re repeat the talking points and if it's on the

16
00:00:54.541 --> 00:00:56.460
left, it's how dare you.

17
00:00:57.570 --> 00:01:00.060
<v 1>I think the biggest, uh, that was,</v>

18
00:01:00.300 --> 00:01:03.660
that was not the best creditor that was actually, now that I know it's her,

19
00:01:03.661 --> 00:01:08.400
it was not a bad imitation. Um, I think one of Obama's biggest mistakes.

20
00:01:08.580 --> 00:01:11.280
He plainly wanted to address climate change global warming,

21
00:01:11.430 --> 00:01:15.090
but he did it in a kind of standard left, uh,

22
00:01:15.120 --> 00:01:18.600
Democrat way by calling it global warming,

23
00:01:18.601 --> 00:01:22.110
by saying that there were bad actors,

24
00:01:22.890 --> 00:01:25.980
which is true. The thing that astonishes me,

25
00:01:25.981 --> 00:01:30.780
that Democrats haven't done is talk about it in a language that every

26
00:01:30.781 --> 00:01:33.900
Republican, every conservative, every hunter Fisher would respond to,

27
00:01:33.901 --> 00:01:37.170
which is pollution, which is what it is by the way, why,

28
00:01:37.230 --> 00:01:41.490
why it became a conversation about a much bigger,

29
00:01:41.491 --> 00:01:44.610
much more abstract, much more difficult to understand and act on.

30
00:01:44.611 --> 00:01:48.180
Problem is strange to me because, um, you know,

31
00:01:48.181 --> 00:01:52.320
humankind comes together. We came to look polio vaccine.

32
00:01:52.380 --> 00:01:53.940
It wasn't like everybody was working on it,

33
00:01:53.941 --> 00:01:55.620
but there were enough people concerned about it.

34
00:01:55.621 --> 00:01:58.650
And then you had a president who said, Hey, March of dimes,

35
00:01:58.651 --> 00:02:02.340
let's have everybody raise money, come up with a vaccine.

36
00:02:02.400 --> 00:02:06.000
And then interestingly, you know, salt with the vaccine.

37
00:02:06.390 --> 00:02:09.750
So it's just interesting about the way medicine works today. He basically said,

38
00:02:09.751 --> 00:02:12.570
no, no, no, I don't want to patent. I don't want to own this patent.

39
00:02:12.600 --> 00:02:15.870
He could have become a billionaire. You know, this was, this is,

40
00:02:16.320 --> 00:02:18.780
this is the way we've have thought in our past,

41
00:02:18.781 --> 00:02:21.090
as humans about solving big problems,

42
00:02:21.690 --> 00:02:25.080
we seem to have gotten away from that a little bit. And I think that's where,

43
00:02:25.081 --> 00:02:27.150
to me, the tribalism is the most dangerous.

44
00:02:27.151 --> 00:02:30.300
It's not about the political charades. I don't care about that.

45
00:02:30.301 --> 00:02:33.540
I don't think that's particularly damaging where I think it's damaging is by

46
00:02:33.541 --> 00:02:37.680
dividing yourself into these tribes that are so exclusive and Hey,

47
00:02:37.710 --> 00:02:39.180
they have these purity tests.

48
00:02:39.420 --> 00:02:43.440
What we're doing is we're actually diminishing our collective ability to come

49
00:02:43.441 --> 00:02:44.251
together to solve problems.

50
00:02:44.251 --> 00:02:48.810
The good news is there's a million people out there at academic institutions and

51
00:02:48.811 --> 00:02:53.670
garages groups of one and two people who are working on

52
00:02:54.060 --> 00:02:58.990
solutions that are, that keep coming, you know, human humans and cool species.

53
00:03:00.100 --> 00:03:00.670
Yeah. We're really.

54
00:03:00.670 --> 00:03:03.670
<v 0>Are. Yeah. Yeah. And I think it's interesting, but Jonas Salk,</v>

55
00:03:03.700 --> 00:03:06.760
when he did create that vaccine, the world was a different place.

56
00:03:06.970 --> 00:03:09.850
And there wasn't this pharmaceutical industry that we have today that's,

57
00:03:10.240 --> 00:03:10.511
you know,

58
00:03:10.511 --> 00:03:15.070
has such a strong ability to influence the way people look at things through

59
00:03:15.190 --> 00:03:16.180
advertisements.

60
00:03:16.181 --> 00:03:19.570
And just through the way they influence politicians were there's,

61
00:03:19.630 --> 00:03:20.530
it's a different world.

62
00:03:20.531 --> 00:03:24.250
So to compare the bounty that was awaiting Jonas Salk for coming up with the

63
00:03:24.251 --> 00:03:27.670
polio vaccine, it's just a different world. The world's different,

64
00:03:27.790 --> 00:03:28.660
didn't he like,

65
00:03:28.661 --> 00:03:31.540
wasn't there some controversy that he didn't give credit to the other people

66
00:03:31.541 --> 00:03:32.590
that helped him with the vaccine.

67
00:03:33.070 --> 00:03:33.903
<v 1>There was an.</v>

68
00:03:34.810 --> 00:03:36.160
<v 0>Okay with money, but not good with.</v>

69
00:03:37.450 --> 00:03:40.350
<v 1>I haven't read this in a long time, so I can't, I can't speak to it, but,</v>

70
00:03:40.600 --> 00:03:43.120
but even like, I've been, um, do you like Richard Fineman? You know,

71
00:03:43.121 --> 00:03:46.520
Richard Scheinman so, so, you know, when he talks about, I,

72
00:03:46.521 --> 00:03:48.250
I love hearing him talk about, uh,

73
00:03:48.251 --> 00:03:51.010
when he was drafted to work on the Manhattan project and you think about it,

74
00:03:51.011 --> 00:03:54.490
America was, it was an existential fear.

75
00:03:56.110 --> 00:03:58.690
<v 0>Sure. I mean, we wound up using it. They couldn't use it on us.</v>

76
00:03:59.740 --> 00:04:00.610
<v 1>And, uh,</v>

77
00:04:01.750 --> 00:04:04.870
and when you hear Fineman talk about all the complication of that,

78
00:04:04.871 --> 00:04:07.000
we have an enormous scientific challenge.

79
00:04:07.060 --> 00:04:10.690
We have an enormous competition against the Germans who were trying to do the

80
00:04:10.691 --> 00:04:14.470
same thing. And then even if we win, then we have another whole challenge,

81
00:04:14.471 --> 00:04:17.790
which is the moral challenge. But,

82
00:04:17.791 --> 00:04:20.190
but there's a way of thinking about those things. And again,

83
00:04:20.440 --> 00:04:24.100
measuring the cost and the benefits that people who might disagree aggressively

84
00:04:24.101 --> 00:04:27.280
and they did about the Manhattan project can sit down and say, okay, here's,

85
00:04:27.310 --> 00:04:30.370
here's what we're going to do. What's, what's the lesser of the two evils.

86
00:04:30.670 --> 00:04:32.920
And I feel like right now, I don't know,

87
00:04:32.921 --> 00:04:35.710
it's just as much progress as we've had.

88
00:04:35.980 --> 00:04:40.480
I feel we've gotten worse at looking at the lesser of two evil paths that

89
00:04:40.481 --> 00:04:42.850
looking at weighing costs and benefits.

90
00:04:43.450 --> 00:04:47.200
<v 0>The lesser of two evils in that regard is it dropped the bombs and the stop,</v>

91
00:04:47.201 --> 00:04:48.034
the war,

92
00:04:48.370 --> 00:04:52.870
or is the lesser of two years never dropped the bomb and stop the war later.

93
00:04:53.680 --> 00:04:57.550
<v 1>Yeah. I mean, look, there's a million books been written about this.</v>

94
00:04:59.920 --> 00:05:03.910
I could make an argument in, in either case. I mean, Japan, we were very,

95
00:05:03.911 --> 00:05:07.750
very scared of Japan. Japan had shown a lot of, um,

96
00:05:07.810 --> 00:05:11.380
ability to punish the United States. Um,

97
00:05:11.860 --> 00:05:15.920
even though Germany was out, uh, America's still felt very fragile. So I,

98
00:05:15.921 --> 00:05:20.650
I totally get the argument that it was meant to be, uh, you know, uh,

99
00:05:20.710 --> 00:05:23.140
I get it too, on the other hand, um,

100
00:05:23.170 --> 00:05:26.680
you're picking some pretty big cities to drop it on and you're picking too. So.

101
00:05:26.770 --> 00:05:29.720
<v 0>You know, you're killing mostly civilians. Yeah. Yeah.</v>

102
00:05:29.800 --> 00:05:32.830
<v 1>So it's hard to imagine that decision would be made today,</v>

103
00:05:32.831 --> 00:05:35.770
but as you just said about, you know, polio vaccine, different, different case,

104
00:05:35.771 --> 00:05:37.840
but roughly same time, um, you know,

105
00:05:37.841 --> 00:05:41.770
it's very hard to project your morals on to, uh, you know, 50 years from now.

106
00:05:41.771 --> 00:05:44.800
We may have a very different view about MMA. For instance,

107
00:05:44.801 --> 00:05:45.700
it's very hard to project.

108
00:05:45.880 --> 00:05:50.530
<v 0>Yeah. Well that think that's far more intense and extreme than MMA.</v>

109
00:05:50.620 --> 00:05:54.070
And when we're talking about new king people literally out of existence,

110
00:05:54.400 --> 00:05:59.330
but I think that just the fact that these brilliant scientists were forced into

111
00:05:59.331 --> 00:06:00.260
that moral dilemma,

112
00:06:00.440 --> 00:06:04.880
like one of my favorite videos online it's Oppenheimer when he's discussing what

113
00:06:04.881 --> 00:06:05.714
he said,

114
00:06:05.750 --> 00:06:09.920
when the first atomic bomb was detonated and he quoted the Bhagavad Gita and he

115
00:06:09.921 --> 00:06:14.600
said, I am become death destroyer of worlds. And is this to say that to see him,

116
00:06:14.720 --> 00:06:17.930
have you seen it, the video, please, let me, let me play this for you. Play,

117
00:06:17.990 --> 00:06:22.070
play Oppenheimer right after describing what it was like.

118
00:06:22.460 --> 00:06:25.160
Cause it's so eerie because here you have one of the most brilliant scientists

119
00:06:25.161 --> 00:06:27.770
ever who completed this fantastic project. Yeah.

120
00:06:28.070 --> 00:06:32.960
The Manhattan project created bombs that literally where nuclear

121
00:06:32.961 --> 00:06:36.380
weapons never happened before in all of human history, as far as we know,

122
00:06:36.560 --> 00:06:40.430
and here the guy that did it that knew that knew that that was going to be the

123
00:06:40.431 --> 00:06:45.350
death of untold amounts of people. Listen to this, listen to what he says.

124
00:06:45.680 --> 00:06:47.240
<v 2>The world would not be the same.</v>

125
00:06:50.480 --> 00:06:52.460
Two people laughed.

126
00:06:54.620 --> 00:06:58.250
Few people cried. Most people were silent.

127
00:07:01.730 --> 00:07:06.680
I remembered the lion from the Hindu scripture. The Bhagavad Gita

128
00:07:10.250 --> 00:07:11.083
Vishnu

129
00:07:13.610 --> 00:07:16.250
is trying to persuade the prince that

130
00:07:18.050 --> 00:07:22.100
he should do his duty and to impress him

131
00:07:22.970 --> 00:07:27.350
takes on his multi-armed form and says,

132
00:07:28.430 --> 00:07:31.970
now I am become death. The destroyer of worlds,

133
00:07:35.060 --> 00:07:37.400
I suppose we all thought that one way or another.

134
00:07:38.360 --> 00:07:43.130
<v 0>Dude. Imagine being that guy. I mean, here's a guy, first of all,</v>

135
00:07:43.190 --> 00:07:45.500
it was quoting the Bhagavad Gita in 1945,

136
00:07:46.160 --> 00:07:50.600
a little ahead of just praying pretty incredible or 40 46. When did they,

137
00:07:50.690 --> 00:07:52.760
when did they first detonate.

138
00:07:53.030 --> 00:07:53.863
<v 1>40? Uh,</v>

139
00:07:54.260 --> 00:07:58.730
the tests were I think 44 and Hiroshima and Nagasaki

140
00:07:58.731 --> 00:07:59.564
45, correct?

141
00:07:59.990 --> 00:08:04.220
<v 0>Somewhere in that range of being this guy who,</v>

142
00:08:05.120 --> 00:08:06.830
you know, he's just a brilliant scientist.

143
00:08:06.831 --> 00:08:10.280
He's not supposed to be the guy who destroys a half a million people in one

144
00:08:10.580 --> 00:08:14.180
moment, one brief flash of light and vaporizes,

145
00:08:14.780 --> 00:08:15.770
a half a million people.

146
00:08:16.100 --> 00:08:16.940
<v 1>He, um,</v>

147
00:08:17.090 --> 00:08:20.690
he went to the school in New York city called the ethical culture Fieldston

148
00:08:20.691 --> 00:08:22.640
school, which is where my kids went.

149
00:08:22.760 --> 00:08:27.710
So Oppenheimer is kind of a patron Saint for

150
00:08:29.180 --> 00:08:34.160
having the brains to do something almost unimaginable and

151
00:08:34.161 --> 00:08:38.210
having the ethics encouraged to know that what he'd done

152
00:08:38.240 --> 00:08:42.530
was unacceptable on some levels.

153
00:08:43.550 --> 00:08:45.500
Um, you know, on the other hand, but look,

154
00:08:45.530 --> 00:08:50.510
if we're talking about costs and benefits, let's think about nuclear power,

155
00:08:50.540 --> 00:08:54.470
nuclear bombs as a deterrent against others down the road, right?

156
00:08:54.471 --> 00:08:58.440
So you have to say killed a lot of people. How many lives did it save?

157
00:08:58.860 --> 00:09:00.870
Impossible to say sound like a Republican.

158
00:09:00.870 --> 00:09:01.740
<v 0>Sir. I know.</v>

159
00:09:01.830 --> 00:09:03.630
<v 1>I know, but I mean, no argument.</v>

160
00:09:03.630 --> 00:09:04.350
<v 0>Right.</v>

161
00:09:04.350 --> 00:09:08.040
<v 1>Then you also have to talk about, then let's also talk about nuclear power,</v>

162
00:09:08.160 --> 00:09:12.570
which was the by-product of this, right? And there are those who could argue.

163
00:09:12.571 --> 00:09:15.090
And I would probably, uh,

164
00:09:15.540 --> 00:09:20.310
aid this argument to say that if the us had continued on the

165
00:09:20.311 --> 00:09:23.220
path of nuclear power in the seventies,

166
00:09:23.221 --> 00:09:27.870
rather than totally flipping out after well, after three mile island,

167
00:09:27.871 --> 00:09:30.630
then later Chernobyl, which was a much worse action in the three mile island.

168
00:09:31.500 --> 00:09:36.150
If, if that had continued, what we'd have now is probably much,

169
00:09:36.180 --> 00:09:39.960
much, much cleaner, cheaper, safer nuclear fuel.

170
00:09:40.260 --> 00:09:44.190
And instead what happened because we basically stopped building nuclear plants.

171
00:09:44.191 --> 00:09:48.120
Instead for the next 40 years, we burnt a crap load of coal.

172
00:09:48.450 --> 00:09:51.390
That's been terrible for the environment for lives.

173
00:09:51.391 --> 00:09:55.380
A lot of lives lost in mining coal, but then the pollution. So, so, you know,

174
00:09:55.381 --> 00:09:56.940
actions have consequences.

175
00:09:57.000 --> 00:10:01.710
What seems to be all benefit often has a lot of costs and life is complicated,

176
00:10:01.711 --> 00:10:06.600
but I think the more that we can measure and weigh

177
00:10:06.601 --> 00:10:11.040
things sensibly, the less screaming there is. I just, you know,

178
00:10:11.041 --> 00:10:12.480
I love changing my mind.

179
00:10:12.481 --> 00:10:16.770
I love hearing somebody make an argument that makes me say, oh, you know,

180
00:10:16.771 --> 00:10:19.590
the way I thought about that before I see why I thought it,

181
00:10:19.620 --> 00:10:21.600
I don't feel like a fool for having thought it. But wow,

182
00:10:21.630 --> 00:10:25.650
now that now that you've laid out some facts and laid out some counterfactuals,

183
00:10:25.830 --> 00:10:29.820
I, I appreciate the opportunity to change my mind. I enjoy that.

184
00:10:29.821 --> 00:10:30.654
I don't know why.

185
00:10:30.990 --> 00:10:33.780
<v 0>Well, I think we're so often married to our ideas.</v>

186
00:10:33.781 --> 00:10:36.630
Like our ideas are a part of us and we're losing,

187
00:10:37.020 --> 00:10:39.750
if our ideas that we've been discussing are incorrect.

188
00:10:39.751 --> 00:10:43.080
If our assumptions were incorrect, it's a value judgment against us. Right.

189
00:10:43.110 --> 00:10:44.430
You know, I think, um,

190
00:10:44.880 --> 00:10:49.650
the nuclear thing is interesting because I think one of the problems with what

191
00:10:49.651 --> 00:10:53.820
happened was the design of like Fukushima, where they can't shut it off.

192
00:10:53.821 --> 00:10:55.680
It's freaked people out, rightly so. And.

193
00:10:55.680 --> 00:10:57.670
<v 1>It's in there, it's built in a bad spot. It's.</v>

194
00:10:57.890 --> 00:11:00.930
<v 0>A little spot and the backup plan sucked everything's wrong.</v>

195
00:11:00.931 --> 00:11:03.180
And now they have this nuclear hotspot that they're, you know,

196
00:11:03.181 --> 00:11:05.070
it's going to be like that for a hundred thousand years. Right.

197
00:11:05.280 --> 00:11:08.880
There's a better way. And they never had a chance to find that better way,

198
00:11:08.910 --> 00:11:12.150
right. Because they've had a better way for everything else in the later ones,

199
00:11:12.330 --> 00:11:15.480
the ones that they're operational now are far better than the Fukushima one.

200
00:11:15.690 --> 00:11:18.540
And they could have gotten way, way, way, way, way better. And that is,

201
00:11:18.810 --> 00:11:21.570
it's one of those things that doesn't seem like it makes sense. Like,

202
00:11:21.571 --> 00:11:24.570
wait a minute, nuclear is clean, right. It's really clean.

203
00:11:24.930 --> 00:11:26.580
It's really clean if it's done. Right.

204
00:11:26.581 --> 00:11:28.950
And if they allow them to innovate over those 40 years,

205
00:11:29.100 --> 00:11:31.500
we have got to some place where it's like super efficient,

206
00:11:31.620 --> 00:11:35.490
just like everything else. If you look at a 1970 car, right.

207
00:11:35.491 --> 00:11:39.960
And then you look at a car from 2020, how about a Tesla zero emissions. Right?

208
00:11:40.020 --> 00:11:43.410
Like maybe they would figure out how to really knock it down to.

209
00:11:44.070 --> 00:11:47.610
<v 1>There are still a lot of people working on next, next, next,</v>

210
00:11:47.611 --> 00:11:50.820
next generation nuclear power, including bill gates is involved in.

211
00:11:51.060 --> 00:11:54.970
And including some that are working with using as fuel, what's called,

212
00:11:54.971 --> 00:11:56.890
spent fuel in a traditional nuclear actor,

213
00:11:56.920 --> 00:12:00.640
which takes care of two big problems at once. Right. Right, right. So look,

214
00:12:00.670 --> 00:12:03.970
yeah, yeah. Reasons to be cheerful, bunkers filled with.

215
00:12:04.390 --> 00:12:07.510
That's eventually going to crack through the bunker and toxify everything around

216
00:12:07.511 --> 00:12:12.010
it. Maybe you could also theoretically use that as literally as fuel.

217
00:12:12.160 --> 00:12:12.700
So.

