WEBVTT

1
00:00:01.250 --> 00:00:02.120
<v 0>Hello, freak bitches.</v>

2
00:00:03.320 --> 00:00:06.720
<v 1>I do think we could be living in a simulation as Elon Musk famously, uh,</v>

3
00:00:06.990 --> 00:00:09.640
suggested we could be all living in a computer simulation.

4
00:00:09.680 --> 00:00:10.640
I don't think it's likely,

5
00:00:10.641 --> 00:00:13.160
but it's completely plausible given what we know right now.

6
00:00:13.250 --> 00:00:16.880
<v 0>Right? The real issue is that one day we most likely will have something.</v>

7
00:00:16.980 --> 00:00:21.790
As long as technology continues to exponentially advance will one day have

8
00:00:21.791 --> 00:00:24.430
something that's indiscernible from the reality,

9
00:00:24.431 --> 00:00:26.950
they'll be able to interface more than likely. Yeah.

10
00:00:26.951 --> 00:00:31.110
They'll be able to interface with your senses with the way the mine perceives

11
00:00:31.111 --> 00:00:33.950
reality and create something yep.

12
00:00:34.020 --> 00:00:38.790
That passes that uncanny valley and literally feels like, like real life,

13
00:00:38.820 --> 00:00:40.070
like the matrix, or I think.

14
00:00:40.190 --> 00:00:42.180
<v 1>That we might be very far or we might be pretty close.</v>

15
00:00:42.220 --> 00:00:44.420
I think it's hard to tell. Cause there's so much we don't know right now.

16
00:00:44.421 --> 00:00:46.420
It's not like this is coming in the next 10 years,

17
00:00:46.560 --> 00:00:49.060
but it could be a thousand years, but yeah, absolutely. And that it could,

18
00:00:49.240 --> 00:00:53.340
you know, the, the thought experiment that, that helps people accept this is,

19
00:00:53.760 --> 00:00:56.940
you know, a single neuron in your brain. You have something like, I don't know,

20
00:00:56.941 --> 00:00:58.460
a hundred billion neurons in your brain,

21
00:00:58.920 --> 00:01:02.130
but every single one is not that complicated. It takes of inputs.

22
00:01:02.131 --> 00:01:05.970
It puts in some outputs, it might be work to figure out exactly what it does,

23
00:01:05.990 --> 00:01:07.730
but it's not mysterious. Right.

24
00:01:08.230 --> 00:01:13.130
So we can imagine replacing one neuron in your brain with

25
00:01:13.410 --> 00:01:16.290
a solid state micro, uh,

26
00:01:16.350 --> 00:01:18.810
device that does exactly the same thing as that neuron.

27
00:01:19.550 --> 00:01:22.720
And you would be the same person roughly. So speaking. Right.

28
00:01:22.721 --> 00:01:25.600
Mm-hmm &lt;affirmative&gt; so, okay. So if you believe that, then do that with two,

29
00:01:25.740 --> 00:01:29.640
two neurons and do it with three neurons and you're gradually building up who

30
00:01:29.641 --> 00:01:33.520
you are, but just replacing your brain with something that is, uh, machine made.

31
00:01:34.140 --> 00:01:35.440
And if you believe that can happen,

32
00:01:35.441 --> 00:01:38.520
there's no reason to think that machines can't be as human as we want them to

33
00:01:38.521 --> 00:01:39.520
be. Do you.

34
00:01:39.540 --> 00:01:41.920
<v 0>Follow any array curves while stuff?</v>

35
00:01:42.790 --> 00:01:45.550
<v 1>A little bit. I don't think that, uh, he's an especially deep thinker,</v>

36
00:01:45.551 --> 00:01:46.384
but he's a good.

37
00:01:47.350 --> 00:01:51.310
<v 0>&lt;Laugh&gt; how dare you. He's right now pounding on his desk. It could be this.</v>

38
00:01:53.370 --> 00:01:56.270
You don't think he's an especially deep thinker. I mean, he is a brilliant guy.

39
00:01:56.470 --> 00:01:59.710
He's invented a bunch of really fascinating things. Yeah.

40
00:01:59.710 --> 00:02:04.220
<v 1>He likes to extrapolate without, uh, limit, uh, pretty.</v>

41
00:02:04.500 --> 00:02:05.301
<v 0>Fearlessly &lt;laugh&gt; well,</v>

42
00:02:05.301 --> 00:02:07.740
he must have irritated you for you to jump right out with,

43
00:02:07.820 --> 00:02:09.380
I don't think he's a particularly deep thinker.

44
00:02:09.400 --> 00:02:11.500
<v 1>No, but I do think he serves a role. I think it, you know, there,</v>

45
00:02:11.501 --> 00:02:15.420
there are people who, uh, make us think more deeply, right?

46
00:02:15.421 --> 00:02:18.620
Mm-hmm &lt;affirmative&gt; like I remember seeing a panel discussion with, uh,

47
00:02:18.621 --> 00:02:19.421
Murray Galman,

48
00:02:19.421 --> 00:02:23.460
who was a famous physicist and Isaac OV this was like 30 years ago and, uh,

49
00:02:24.130 --> 00:02:27.890
ASMO was still alive. And there was some someone else who was on the panel.

50
00:02:27.970 --> 00:02:29.410
I forget it was another physicist.

51
00:02:29.750 --> 00:02:34.130
And the amazing thing was the science fiction author was by far the most

52
00:02:34.131 --> 00:02:38.410
conservative thinker up there in terms of what he thought would really be

53
00:02:38.560 --> 00:02:40.410
happening a thousand years from now, right.

54
00:02:40.411 --> 00:02:42.890
The physicist had these way out ideas and asthma was like, yeah,

55
00:02:42.891 --> 00:02:45.280
I don't think it's gonna be all that different. Right.

56
00:02:45.780 --> 00:02:49.400
And it's just hard to predict the future accurately. And there's a,

57
00:02:49.480 --> 00:02:53.320
a role served both by trying to be as realistic as possible and,

58
00:02:53.480 --> 00:02:56.440
and careful and BA and saying, what are the probabilities and so forth?

59
00:02:56.680 --> 00:02:59.640
Mm-hmm &lt;affirmative&gt; and there's another role served by just being the, uh,

60
00:02:59.641 --> 00:03:02.560
provocative and saying, well, maybe this crazy thing is gonna happen.

61
00:03:02.561 --> 00:03:05.880
Maybe we'll cure death in the next a hundred years. And uh,

62
00:03:05.910 --> 00:03:08.360
life will change for everybody dramatically. That seems.

63
00:03:08.360 --> 00:03:09.280
<v 0>Fairly likely, right.</v>

64
00:03:09.760 --> 00:03:12.080
They're they'll be gonna figure out some way reverse aging.

65
00:03:12.150 --> 00:03:15.040
<v 1>Yeah. I think that's plausible. There's no biological reason. Why not KW.</v>

66
00:03:15.360 --> 00:03:16.720
<v 0>Got some weird motivations too.</v>

67
00:03:17.140 --> 00:03:21.230
He literally wants to sort of rebuild his father.

68
00:03:21.940 --> 00:03:22.950
Yeah. Yeah.

69
00:03:23.370 --> 00:03:27.590
He wants to be able to piece together some sort of an artificial intelligence

70
00:03:27.591 --> 00:03:32.230
version of his father and, and go back and see him. Yep. That's deep.

71
00:03:32.630 --> 00:03:35.070
<v 1>&lt;Laugh&gt; that's a word for it. Yeah. Yeah. &lt;laugh&gt;.</v>

72
00:03:35.390 --> 00:03:40.100
<v 0>So that's, &lt;laugh&gt; um, he fit, he thinks it there's gonna come a time,</v>

73
00:03:40.540 --> 00:03:41.860
uh, inside his lifetime,

74
00:03:41.861 --> 00:03:45.460
hopefully that you'll be able to download consciousness.

75
00:03:45.770 --> 00:03:49.900
That consciousness is going to be something you you'll transfer sort of like,

76
00:03:50.220 --> 00:03:51.060
uh, you know, code.

77
00:03:51.610 --> 00:03:55.180
<v 1>Yeah. I think that there's, there's a difference between, you know,</v>

78
00:03:55.181 --> 00:03:59.420
what is potentially possible given arbitrary amounts of time and resource

79
00:03:59.990 --> 00:04:03.410
and what is realistic in the relatively near term by the near term,

80
00:04:03.411 --> 00:04:05.370
we mean 50 or a hundred years. Right. Right.

81
00:04:05.430 --> 00:04:10.410
And I think that people like Ray Cowell are far, uh, exaggerating,

82
00:04:10.411 --> 00:04:13.010
what's gonna be possible the next 50 or a hundred years because they

83
00:04:13.011 --> 00:04:16.010
underestimate how little we know about how the brain works,

84
00:04:16.190 --> 00:04:19.490
how important it is for the brain to be in our bodies. Right.

85
00:04:19.491 --> 00:04:21.360
Mm-hmm &lt;affirmative&gt; &lt;affirmative&gt; we, you know, one of the,

86
00:04:21.660 --> 00:04:24.320
one of the breakthroughs in artificial intelligence over the last couple decades

87
00:04:24.460 --> 00:04:28.360
was to realize that if you try to build an artificial intelligent computer,

88
00:04:29.150 --> 00:04:32.000
it's, it becomes much more realistic. If you give it a body,

89
00:04:32.300 --> 00:04:35.000
if you give it a face and it can interact with people. I mean,

90
00:04:35.001 --> 00:04:38.680
we underestimate the extent to which having a body is an important part of how

91
00:04:38.681 --> 00:04:40.480
we think and who we are. And these, the,

92
00:04:40.550 --> 00:04:44.350
this is just like such baby steps in understanding this stuff, that,

93
00:04:44.351 --> 00:04:47.630
to imagine that in the matter of decades we'll have it all figured out and have

94
00:04:47.830 --> 00:04:50.070
downloadable consciousness is, is, is not realistic to me.

95
00:04:50.410 --> 00:04:54.950
<v 0>Do you think that we have to absolutely understand the exact way that the human</v>

96
00:04:54.951 --> 00:04:58.790
brain works in order to replicate its possibilities? No.

97
00:04:58.910 --> 00:05:00.470
<v 1>I mean, I think that, uh,</v>

98
00:05:00.650 --> 00:05:05.380
we probably won't that probably won't be the way that we make artificial

99
00:05:05.381 --> 00:05:07.140
consciousness or artificial intelligence.

100
00:05:07.141 --> 00:05:09.500
Like we won't just be reconstructing human beings.

101
00:05:09.600 --> 00:05:12.780
Mm-hmm &lt;affirmative&gt; like when we made cars, we didn't reconstruct horses.

102
00:05:12.950 --> 00:05:15.140
Right. We just did it in a very, very different way.

103
00:05:15.200 --> 00:05:17.940
And cars are much better than horses in, in various ways.

104
00:05:18.560 --> 00:05:22.770
Not as good in other ways, uh, like going on Hills, going up Hills, but also,

105
00:05:23.170 --> 00:05:25.730
you know, in the early days of cars, one big complaint was,

106
00:05:26.270 --> 00:05:29.330
but if I'm drunk and passed out, it won't take me home. I'll buy this out.

107
00:05:29.570 --> 00:05:31.330
&lt;laugh&gt; is that a complaint? Yeah, that was a complaint.

108
00:05:31.530 --> 00:05:32.363
<v 0>And its true. That's hilarious.</v>

109
00:05:32.410 --> 00:05:35.210
Cuz the horse just take you home and knows how to get there. Exactly. Finally.

110
00:05:35.570 --> 00:05:37.090
<v 1>Will get the artificial and nobody gets mad.</v>

111
00:05:37.091 --> 00:05:38.970
Self-driving cars will be able to do it. Finally.

112
00:05:39.290 --> 00:05:40.050
<v 0>If you're driving a horse drunk.</v>

113
00:05:40.050 --> 00:05:42.680
<v 1>Nobody gets a. Nope. You're passed out. That's right.</v>

114
00:05:43.080 --> 00:05:45.640
<v 0>It had to be like a big adjustment for people. Yeah, exactly. No more.</v>

115
00:05:45.640 --> 00:05:46.473
<v 1>Driving drunk.</v>

116
00:05:46.580 --> 00:05:50.480
And so the idea that the way that we'll make artificial intelligence is to sort

117
00:05:50.481 --> 00:05:54.200
of mimic a human being is just crazy. Now that that's not how it's gonna be.

118
00:05:54.390 --> 00:05:55.520
<v 0>What do you think it's gonna be like?</v>

119
00:05:56.230 --> 00:06:00.440
<v 1>Well, I don't know. There are, there are the problems that can be solved. Um,</v>

120
00:06:00.900 --> 00:06:04.680
by things that we design are just a different set of problems than the thing

121
00:06:04.681 --> 00:06:09.280
that evolution naturally made us do, right? Like evolution built a very,

122
00:06:09.281 --> 00:06:14.200
very general purpose machine that is inefficient and irrational in all

123
00:06:14.201 --> 00:06:16.910
sorts of ways. Like anyone's pocket cow calculator,

124
00:06:17.310 --> 00:06:20.710
since the 1970s can multiply numbers way better than your brain can. Right?

125
00:06:20.980 --> 00:06:25.150
Your brain has enormously more computational capacity than a pocket calculator.

126
00:06:25.250 --> 00:06:26.430
Why can't we multiply numbers?

127
00:06:27.020 --> 00:06:30.710
That ability was not important back when we were evolving these things. Right?

128
00:06:31.050 --> 00:06:32.950
So the set of things that it is easy to.

129
00:06:33.010 --> 00:06:36.350
<v 0>Design is just very, very different than what the brain does. So who knows,</v>

130
00:06:36.390 --> 00:06:39.060
I don't know exactly what it'll be, what it will look like.

131
00:06:39.420 --> 00:06:43.060
I actually think that the more important thing will be blurring the distinction

132
00:06:43.061 --> 00:06:45.300
between human beings and machines, you know, the,

133
00:06:45.320 --> 00:06:49.580
the crossovers there's another one of Elon Musk's project is called the neural

134
00:06:49.900 --> 00:06:52.980
link. The idea of, you know, basically a neural lace,

135
00:06:52.981 --> 00:06:55.980
something that is just interfacing with your brain very, very,

136
00:06:55.981 --> 00:06:59.810
very fast so that you have of access to the entire internet or whatever

137
00:06:59.960 --> 00:07:01.930
peripherals you want in real time.

138
00:07:02.110 --> 00:07:04.690
So like Wikipedia is part of your memory essentially,

139
00:07:04.790 --> 00:07:06.490
and that's just who you are and how you walk around.

140
00:07:06.730 --> 00:07:09.090
Then you can multiply numbers as fast as you want to.

141
00:07:09.360 --> 00:07:13.330
That seems like very likely yeah. That augmented, augmented reality,

142
00:07:13.490 --> 00:07:15.170
I think that's coming. Yeah. Yeah. And,

143
00:07:15.490 --> 00:07:19.480
and some sort of a weird symbiotic connection to the net into

144
00:07:20.110 --> 00:07:23.120
electronics, whether it's a wearable thing or maybe even an embedded.

145
00:07:23.130 --> 00:07:26.120
<v 1>Thing. Yeah, absolutely. I think it will be embedded eventually. Yeah.</v>

146
00:07:26.640 --> 00:07:27.560
It's more efficient. Will you.

147
00:07:28.740 --> 00:07:30.760
<v 0>You be one of the first adopters, are you gonna wait a little.</v>

148
00:07:30.760 --> 00:07:34.280
<v 1>While? Uh, I'm never a first generation adopter even of iPhones, much less,</v>

149
00:07:34.420 --> 00:07:36.600
you know, things embedded in my body. But look, I, we,

150
00:07:36.640 --> 00:07:40.390
I just adopted two kittens. They get microchip implants, right? Oh yeah.

151
00:07:40.391 --> 00:07:43.550
When you adopt a cat that gets a microchip inside so you can't lose it. Yeah.

152
00:07:43.690 --> 00:07:46.590
And so to think that we're very far away from doing that,

153
00:07:46.710 --> 00:07:48.950
I don't think is right. I think that we're gonna be doing things.

154
00:07:49.150 --> 00:07:51.870
<v 0>I was just looking at a, a, a laptop bag.</v>

155
00:07:52.300 --> 00:07:55.070
It's a laptop bag that also is, uh,

156
00:07:55.250 --> 00:08:00.060
you have a passport bag and a laptop bag and then a carry on and then

157
00:08:00.420 --> 00:08:04.180
a check and stove, you know, on a, for airplane, airplane luggage.

158
00:08:04.440 --> 00:08:05.940
And all of it is Bluetooth.

159
00:08:06.200 --> 00:08:10.780
And all of it is location coordinated with GPS so that if somehow or another,

160
00:08:10.781 --> 00:08:11.740
your bag gets lost,

161
00:08:11.741 --> 00:08:15.020
you literally can go on a computer and it'll show you where your bag is. Yeah.

162
00:08:15.700 --> 00:08:17.660
It's like, whoa. Yeah. How long are we gonna,

163
00:08:17.770 --> 00:08:18.940
when are we gonna do that with people?

164
00:08:19.670 --> 00:08:23.690
<v 1>And I feel, you know, I, I, I'm a big believer in privacy rights and so forth.</v>

165
00:08:24.030 --> 00:08:27.370
<v 0>Her dilemma, do I let my employer microchipping? What is this? There you go.</v>

166
00:08:27.520 --> 00:08:31.010
<v 2>There's a company in Wisconsin that had microchip day on August 1st.</v>

167
00:08:31.030 --> 00:08:33.530
And they implanted microchips in people.

168
00:08:34.250 --> 00:08:39.050
<v 0>I worked that dude with his ears in that those earplugs and the lower</v>

169
00:08:39.190 --> 00:08:43.280
ear lobe, the you ain't doing nothing, buddy. You ain't sticking that in me.

170
00:08:43.281 --> 00:08:44.360
Look what you've done to your ears.

171
00:08:44.720 --> 00:08:49.480
Fellow Wisconsin company becomes first us employee. What? And these.

172
00:08:49.480 --> 00:08:52.400
<v 1>Are very simple, just location tracking things. Right. But why would you.</v>

173
00:08:52.400 --> 00:08:55.560
<v 0>Allow the company to do that? I wouldn't allow my company to tattoo me.</v>

174
00:08:55.561 --> 00:08:58.400
Why would I allow my company to microchip me? It's for tracking.

175
00:08:58.401 --> 00:09:01.640
And you can also like, they can buy stuff in the cafeteria and whatnot.

176
00:09:01.740 --> 00:09:03.400
You know what else you could do? Use your wallet.

177
00:09:04.540 --> 00:09:06.120
<v 1>You say that, but you know, like gross.</v>

178
00:09:06.700 --> 00:09:09.200
Do you turn off all cookies when you have your laptop on and.

179
00:09:09.200 --> 00:09:11.920
<v 0>Your browser? Yes I do. No do no, I don't. You don't because it's little.</v>

180
00:09:11.920 --> 00:09:13.960
<v 1>Convenient. And these guys say like they get,</v>

181
00:09:14.280 --> 00:09:16.510
they can buy their snacks with their mic.

182
00:09:16.790 --> 00:09:17.710
Gocha they just walk up through a machine.

183
00:09:17.710 --> 00:09:21.830
<v 0>And get a snack. But here's where therefore separation every loss, this laptop,</v>

184
00:09:22.150 --> 00:09:25.630
I can go like that and then I can walk out the door and that laptop stays here

185
00:09:26.070 --> 00:09:28.910
and all the cookies and all my browser history stays here.

186
00:09:29.270 --> 00:09:31.390
Mm-hmm &lt;affirmative&gt; it. Doesn't, it's not, that's what they want you to think.

187
00:09:32.560 --> 00:09:34.190
Jesus. Now you're freaking me out.

