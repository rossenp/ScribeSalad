WEBVTT

1
00:00:01.060 --> 00:00:02.360
<v 0>The Joe Rogan experience.</v>

2
00:00:02.660 --> 00:00:05.480
Did you see what happened with Neil DEROS Tyson that Neil DEROS Tyson got in

3
00:00:05.481 --> 00:00:08.800
trouble for tweeting something the other day, uh, after the mass shootings,

4
00:00:08.801 --> 00:00:12.480
right? People were at him because it pull the, pull the, the tweet up,

5
00:00:12.481 --> 00:00:17.480
cuz it's pretty interesting cuz it's it's just accurate and people were

6
00:00:17.481 --> 00:00:21.230
angry and they're saying he's using his platform. Irresponsive believe. No,

7
00:00:21.570 --> 00:00:26.390
he was trying to let people know that although these shootings are

8
00:00:26.510 --> 00:00:27.343
a tragedy,

9
00:00:27.580 --> 00:00:31.990
they are a small number of deaths and there's so many other deaths that happen

10
00:00:32.310 --> 00:00:33.790
here. It goes the past 48 hours,

11
00:00:34.370 --> 00:00:38.030
the USA horrifically lost 34 people to mass shootings. On average,

12
00:00:38.130 --> 00:00:42.380
across 48 hours, we also lose 500 hundred to medical errors,

13
00:00:42.590 --> 00:00:46.660
300 to the flu, 250 to suicide, 200 to car accidents,

14
00:00:46.870 --> 00:00:48.780
40 to homicides via handgun.

15
00:00:49.070 --> 00:00:52.540
Often our emotions respond more to spectacle than to data.

16
00:00:53.040 --> 00:00:57.900
Now that is not a bad tweet, but I saw a scientist who was writing.

17
00:00:58.300 --> 00:01:01.140
I am unfollowing him. He is using his platform.

18
00:01:01.460 --> 00:01:05.370
It responsibly a lot of virtue signaling really? Because what,

19
00:01:05.371 --> 00:01:09.930
what he's saying is not that there's anything wrong with, uh, you know,

20
00:01:10.200 --> 00:01:13.970
feeling ho horrified by these tragedies. I mean,

21
00:01:14.000 --> 00:01:18.850
he's saying we horrifically horrifically lost 34 people to mass

22
00:01:19.050 --> 00:01:19.630
shootings,

23
00:01:19.630 --> 00:01:24.080
but he's saying it's interesting that there's people dying left and right all

24
00:01:24.081 --> 00:01:27.800
throughout this country all day long, just not at the hands of one person.

25
00:01:28.060 --> 00:01:31.000
So we look at it differently. And he's just saying,

26
00:01:31.710 --> 00:01:33.680
he's just giving you data as a.

27
00:01:33.880 --> 00:01:38.040
<v 1>Scientist. And that's, that's exactly what it is. He's a scientist. Yes.</v>

28
00:01:38.460 --> 00:01:41.720
And so a scientist can separate the emotion,

29
00:01:42.690 --> 00:01:44.430
but people don't people find, people.

30
00:01:44.430 --> 00:01:47.230
<v 0>Find his apology cause hispology is even more interesting. People.</v>

31
00:01:47.340 --> 00:01:51.990
<v 1>Will consider a mass shooting much worse than,</v>

32
00:01:52.970 --> 00:01:55.070
you know, heart attacks. Yes.

33
00:01:55.070 --> 00:01:57.070
<v 0>Yeah. So listen to this.</v>

34
00:01:57.090 --> 00:02:01.150
So this is the other thing yesterday I posted in reaction to the horrific mass

35
00:02:01.310 --> 00:02:03.300
shootings in America, over the previous 48 hours,

36
00:02:03.301 --> 00:02:07.780
killing 34 people spawned mixed in highly critical responses.

37
00:02:07.880 --> 00:02:12.300
If you missed it, I offered a short list of largely preventable causes of death.

38
00:02:12.390 --> 00:02:16.060
Along with their average two day death toll in the United States,

39
00:02:16.130 --> 00:02:20.300
they significantly exceeded the death toll from the two days of mass shootings,

40
00:02:20.301 --> 00:02:23.260
including the number of people, 40 who on average,

41
00:02:23.610 --> 00:02:26.210
I from handgun homicides every two days,

42
00:02:26.890 --> 00:02:31.290
I then noted that we tend to react emotionally to spectacular incidences of

43
00:02:31.291 --> 00:02:32.000
death.

44
00:02:32.000 --> 00:02:36.530
With the implication that more common causes of death trigger

45
00:02:36.870 --> 00:02:38.330
milder responses within us.

46
00:02:38.670 --> 00:02:43.050
My intent was to offer objectively true information that might help shape

47
00:02:43.051 --> 00:02:46.840
conversations and reactions to preventable ways we die.

48
00:02:46.970 --> 00:02:51.400
Where I miscalculated was that I genuinely believed that the tweet would be

49
00:02:51.401 --> 00:02:54.040
helpful to anyone trying to save lives in America.

50
00:02:54.390 --> 00:02:58.680
What I learned from the range of reactions that, uh, is that for many people,

51
00:02:59.310 --> 00:03:03.960
some information, my tweet in particular can be true, but unhelpful,

52
00:03:04.290 --> 00:03:08.720
especially at a time when many people are either still in shock or trying to

53
00:03:08.721 --> 00:03:11.360
heal or both. So if you are one of those people,

54
00:03:11.920 --> 00:03:16.640
I apologize for not knowing in advance what effect my tweet could have had on

55
00:03:16.641 --> 00:03:17.474
you.

56
00:03:17.620 --> 00:03:22.350
I'm therefore thank thankful for the candor and depth of critical

57
00:03:22.750 --> 00:03:25.390
reactions shared in my Twitter feed as an educator,

58
00:03:25.790 --> 00:03:30.430
I personally value knowing with precision and accuracy, what reaction,

59
00:03:30.950 --> 00:03:33.670
anything that I say or right will instill in my audience.

60
00:03:33.890 --> 00:03:38.390
And I got this one wrong respectfully submitted. And then it says,

61
00:03:38.391 --> 00:03:42.380
Neil degrasta Tyson. Now, even that people are saying not enough,

62
00:03:42.600 --> 00:03:45.020
not good enough, a response it's almost like today.

63
00:03:45.021 --> 00:03:49.460
There's certain people today that D they don't give a whether or not you're,

64
00:03:49.461 --> 00:03:52.660
you're saying something with sincerity, whether you are sorry, they don't,

65
00:03:52.850 --> 00:03:55.540
sorry's not enough. Like they don't. They just want, yeah.

66
00:03:55.541 --> 00:03:57.580
They wanna be mad at you. And even if you're sorry,

67
00:03:57.960 --> 00:04:02.090
if you admit you made a mistake, no forgiveness. There's no road to redemption.

68
00:04:02.240 --> 00:04:04.570
There's no, there's no, I get what you were doing.

69
00:04:05.850 --> 00:04:10.330
<v 1>I think that it depends on, you know, as far as is the sorry,</v>

70
00:04:10.331 --> 00:04:11.290
enough or whatever,

71
00:04:11.720 --> 00:04:14.810
depends on the pattern of the person now with Neil Degrass Tyson,

72
00:04:14.880 --> 00:04:18.170
this is what I think happened in here. This is my opinion. He's a scientist.

73
00:04:18.630 --> 00:04:22.480
So he gives information. So he saw this and said, oh, wait a minute.

74
00:04:22.950 --> 00:04:26.520
This many people died from medical mistakes. Yes. This many. And,

75
00:04:26.540 --> 00:04:30.520
and people didn't react. What he, doesn't where he messed up with the timing.

76
00:04:30.980 --> 00:04:34.760
Yes. You don't say that the day after the mass shooting, you say it, uh,

77
00:04:34.761 --> 00:04:37.080
maybe a week later or something like, Hey, you know,

78
00:04:37.081 --> 00:04:40.990
people die in a lot of weight, blah, blah, blah. Now what he in is,

79
00:04:41.710 --> 00:04:43.990
I didn't know that I'm a scientist, right? I didn't,

80
00:04:44.030 --> 00:04:45.870
I wasn't aware of the emotional impact.

81
00:04:46.560 --> 00:04:48.990
Thank you for telling me the emotional impact.

82
00:04:49.090 --> 00:04:52.270
And I'm sorry that I hurt people's feelings,

83
00:04:52.271 --> 00:04:54.310
which to me is totally legitimate.

84
00:04:54.310 --> 00:04:57.670
Especially coming from who he is and what, what,

85
00:04:58.550 --> 00:05:03.100
I would think the scientific mindset now, there were some people like, yeah,

86
00:05:03.101 --> 00:05:07.220
exactly what you said. They decided you're a terrible person for saying this.

87
00:05:07.221 --> 00:05:10.380
So that apology isn't enough. There's nothing he could do. Right.

88
00:05:10.480 --> 00:05:14.780
But see there there's certain people, right? There's nothing you can do.

89
00:05:15.600 --> 00:05:18.580
That's gonna change their mind. You know, you, you look at,

90
00:05:18.810 --> 00:05:20.700
look at the Obama birth certificate, right.

91
00:05:20.720 --> 00:05:22.970
So even when the birth certifi came out,

92
00:05:22.971 --> 00:05:25.290
there's a certain percentage of the people that they're still like, well, no,

93
00:05:25.291 --> 00:05:26.091
that's fake, right?

94
00:05:26.091 --> 00:05:30.650
Like there was no way he was ever gonna be American to these people.

95
00:05:30.830 --> 00:05:34.890
Or there there's a certain, you know, you, you travel,

96
00:05:36.070 --> 00:05:37.490
you know, like I, you know,

97
00:05:38.520 --> 00:05:41.490
there's certain groups like you ain't gonna get 'em right. They,

98
00:05:41.491 --> 00:05:44.200
their minds made up and they're in their bubble. They're in there,

99
00:05:44.600 --> 00:05:48.320
whatever it is. And they they're surrounded by like-minded people.

100
00:05:48.350 --> 00:05:51.120
Like you said, like, this is the disadvantage of the internet, right?

101
00:05:51.121 --> 00:05:54.080
The advantage of the internet is all of this information.

102
00:05:54.340 --> 00:05:58.320
The disadvantage is you find people who only think like you and you only talk to

103
00:05:58.321 --> 00:06:01.320
them and you build this bubble. Yeah. Well, well, yes.

104
00:06:01.340 --> 00:06:04.880
So there's a group of people, a percentage. I don't know.

105
00:06:04.881 --> 00:06:05.960
However you want to describe it.

106
00:06:06.550 --> 00:06:10.960
That decided the moment he said that this is an unfeeling, horrible person,

107
00:06:11.270 --> 00:06:13.280
blah, blah, blah. I mean, I said, you know my thing with the,

108
00:06:13.281 --> 00:06:16.990
with the mass shootings, I'm like, listen, we don't care.

109
00:06:18.010 --> 00:06:22.750
We say, we, we care individual. Like, it's very sad for that family.

110
00:06:22.780 --> 00:06:23.910
It's very, to me, the,

111
00:06:23.911 --> 00:06:27.390
the worst part of a mass shooting is somebody went to Walmart that day.

112
00:06:27.391 --> 00:06:29.270
They didn't know it was gonna be the last day of their life.

113
00:06:29.420 --> 00:06:33.630
They're never gonna see their family. Again, somebody went out to a club in,

114
00:06:33.810 --> 00:06:38.100
in Ohio. They didn't know. They never come. That's the,

115
00:06:38.160 --> 00:06:42.140
the tragic part. And that is said, but in the grand scheme of things,

116
00:06:42.800 --> 00:06:44.980
we don't do anything. We don't, you know, we,

117
00:06:45.080 --> 00:06:47.540
we say it doesn't work before we even try it. Like,

118
00:06:47.541 --> 00:06:51.060
there's nothing we do that changes. Even after Vegas,

119
00:06:51.170 --> 00:06:53.500
when they said they were gonna ban, what was it called?

120
00:06:53.501 --> 00:06:58.170
The bump stop or whatever it was that thing that helped shoot faster.

121
00:06:58.510 --> 00:07:01.650
Mm-hmm &lt;affirmative&gt; and then ultimately they didn't even ban that. You know?

122
00:07:02.270 --> 00:07:06.770
So, so as a society, we say we care,

123
00:07:07.350 --> 00:07:10.850
but we don't. Cuz we don't change anything. You gotta change something.

124
00:07:10.851 --> 00:07:13.770
Nothing changes if nothing changes. Right. Well where we're.

125
00:07:13.770 --> 00:07:16.090
<v 0>Getting way off track here. But with neilgras Tyson,</v>

126
00:07:16.590 --> 00:07:21.280
the outrage thing DIDT rage me. No, no, I, it I't outrage you either, right? No.

127
00:07:21.280 --> 00:07:24.240
<v 1>Because he's a scientist. What he said said just is truth. Yeah.</v>

128
00:07:24.500 --> 00:07:27.480
And data is true and data is delivered without emotion. Yeah.

129
00:07:27.481 --> 00:07:32.360
And why people were upset is because he delivered truth with no

130
00:07:32.361 --> 00:07:33.680
emotion and he, but he.

131
00:07:33.680 --> 00:07:37.360
<v 0>Did, he said horrifically, I mean, he was talking about the tragedy. Yeah.</v>

132
00:07:37.380 --> 00:07:40.390
The thing is that just people looking to be upset. Oh.

133
00:07:40.520 --> 00:07:43.550
<v 1>Absolutely. Yeah, yeah. Absolutely. People look for,</v>

134
00:07:44.770 --> 00:07:46.830
for something to be mad about. Yeah.

135
00:07:47.090 --> 00:07:50.350
Or something to be outraged about and that, you know,

136
00:07:50.351 --> 00:07:52.670
what that takes away from it takes away from real outrage.

137
00:07:53.170 --> 00:07:55.790
<v 0>Yes. Right? Yeah. It, it diminishes.</v>

138
00:07:55.790 --> 00:08:00.540
<v 1>It. If you're outraged every day, then you know, then, okay.</v>

139
00:08:00.560 --> 00:08:03.460
So what's really outrageous, right. If, if I'm gonna be upset,

140
00:08:03.680 --> 00:08:07.100
if I'm gonna be upset at a scientist for giving me scientific data,

141
00:08:08.130 --> 00:08:10.260
what do I want? Yeah.

142
00:08:10.420 --> 00:08:13.140
<v 0>I guess the timing was the issue. The timing was the issue.</v>

143
00:08:13.141 --> 00:08:16.340
It wasn't an issue with me. I mean, I get what he's doing. I'm not a moron.

144
00:08:16.580 --> 00:08:17.980
I mean, it was just, it's simple.

145
00:08:18.450 --> 00:08:23.410
He's he was just giving you all sorts of different horrific deaths

146
00:08:23.411 --> 00:08:27.370
that occur all throughout the country. And I think on that same weekend,

147
00:08:27.371 --> 00:08:30.130
there was some unprecedented number of people that were shot and killed in

148
00:08:30.131 --> 00:08:32.330
Chicago. Like Chicago is.

149
00:08:32.330 --> 00:08:35.930
<v 1>A Chicago's in it's, but war, you know, war zone, it's a war zone. Yeah. And,</v>

150
00:08:35.990 --> 00:08:39.530
and it, the thing is, you know, people say, well, what about the gun laws?

151
00:08:39.590 --> 00:08:42.520
And it's yeah. But they, all you gotta do is go to Indiana.

152
00:08:42.590 --> 00:08:46.040
Like you go two hours away and you can get whatever you want. Just, you know,

153
00:08:46.100 --> 00:08:47.880
that's the mm-hmm &lt;affirmative&gt;, that's the thing.

154
00:08:47.900 --> 00:08:51.800
So just driving a car to a gun show. So just as a city, Chicago's like, look,

155
00:08:51.860 --> 00:08:54.640
we we're trying, but we can't. Right. You know,

156
00:08:54.641 --> 00:08:59.600
what are we gonna set up borders and check every car coming into Illinois. God.

157
00:08:59.940 --> 00:09:02.520
<v 0>And you'd have to go in every house. Exactly. It's.</v>

158
00:09:02.520 --> 00:09:06.280
<v 1>Not even you enough. So you can't, you can't do that. Yeah. It it's. But yeah,</v>

159
00:09:06.340 --> 00:09:10.000
his thing, I think it was a time and I'm like you, that didn't offend me.

160
00:09:10.600 --> 00:09:12.880
I get what he was saying. And then this is it's.

161
00:09:13.010 --> 00:09:15.870
<v 0>Can't offend you. If you're a rational human, it can't.</v>

162
00:09:15.870 --> 00:09:16.830
<v 1>Offend you, you know,</v>

163
00:09:16.831 --> 00:09:21.710
you know what bothers me when people pretend to be ignorant of something

164
00:09:21.770 --> 00:09:26.350
and they're not how so? Well, when people like, like, for example,

165
00:09:26.351 --> 00:09:30.390
with this, like there's some people like, oh, we can use this to,

166
00:09:30.570 --> 00:09:35.540
to create some dislike against Neil as Tyson know or

167
00:09:35.660 --> 00:09:39.100
whatever. It's generally it's generally politicians do it. Right. When they,

168
00:09:39.130 --> 00:09:42.260
they say something and they're like, oh, I didn't know.

169
00:09:42.261 --> 00:09:45.420
That was offensive around. It's like, yes you did.

170
00:09:46.000 --> 00:09:49.580
But you know your followers didn't you see what I mean?

171
00:09:49.610 --> 00:09:51.860
Like you can talking like, gimme an example talking. I'm trying,

172
00:09:51.861 --> 00:09:55.340
trying to think of an example. Okay. Well, well with the mass shootings, right?

173
00:09:56.210 --> 00:09:56.411
So the,

174
00:09:56.411 --> 00:10:00.930
so the whole thing of saying that Trump's tweets had nothing to do with it.

175
00:10:01.990 --> 00:10:06.690
Yes they did. Yes. Now they, they didn't directly, but yeah,

176
00:10:06.710 --> 00:10:11.690
it did normalize. And this guy used the same language of the,

177
00:10:11.790 --> 00:10:13.330
you know, invasion, et cetera.

178
00:10:13.790 --> 00:10:18.120
So you can't say that it's completely a unrelated, you know what I mean?

179
00:10:18.180 --> 00:10:22.320
You can't, you can't don't pre and, and again, it's not saying direct, but,

180
00:10:22.460 --> 00:10:26.000
but you can't pretend there's no connection. Right, right. There's a direct.

181
00:10:26.000 --> 00:10:27.040
<v 0>And areion are people connect.</v>

182
00:10:27.040 --> 00:10:31.000
<v 1>Cause it's quotes. There are people who are intelligent enough to know that,</v>

183
00:10:31.580 --> 00:10:35.320
but they'll say no, no there's no. And it's like, yes, there is. And you know,

184
00:10:35.321 --> 00:10:38.590
there are, don't faint. It gnorance, that's what I mean. Right. But you.

185
00:10:38.590 --> 00:10:38.711
<v 0>Know,</v>

186
00:10:38.711 --> 00:10:43.110
the guy in Dayton who really up one was that guy's an Elizabeth Warren supporter

187
00:10:43.130 --> 00:10:47.350
who actually wrote about gun control. I mean, he was just a horrific suit.

188
00:10:47.940 --> 00:10:50.030
Homicidal, yeah. Psychopath.

189
00:10:50.290 --> 00:10:53.790
<v 1>And there, there, listen, there, you have to have some,</v>

190
00:10:54.520 --> 00:10:58.940
there has to be something wrong with you to do that. You like, even, you know,

191
00:10:59.400 --> 00:11:02.420
in, regardless of your political beliefs,

192
00:11:02.750 --> 00:11:04.860
there has to be something wrong with you,

193
00:11:05.000 --> 00:11:09.740
but you can be sparked or you can be egged on. Sure.

194
00:11:10.210 --> 00:11:14.860
Yeah. By the words of, of leaders or political people or powerful people and it,

195
00:11:14.861 --> 00:11:18.090
people of influence perfect way to put it. Yeah. People of influence.

196
00:11:18.091 --> 00:11:22.970
And that's why I think people of influence have to be responsible in what

197
00:11:22.971 --> 00:11:27.250
they say. Yeah. You know, but, but this, but Neil Degrass, Tyson,

198
00:11:27.360 --> 00:11:28.370
this is a different thing.

199
00:11:28.450 --> 00:11:32.530
I think this is just a case of a scientist pointing out information

200
00:11:33.640 --> 00:11:35.970
without, you know, like, like, uh, you know,

201
00:11:35.971 --> 00:11:37.930
you were talking about earlier dorks, nerd,

202
00:11:38.320 --> 00:11:41.560
whatever you wanna say to where it's like, yeah, well this is just information.

203
00:11:41.700 --> 00:11:45.800
I'm not trying to be emotional. And people are like, well, you have to be right.

204
00:11:45.801 --> 00:11:47.760
Exactly. You have to connect. Exactly. You,

205
00:11:47.830 --> 00:11:49.800
even though you're from a lab environment,

206
00:11:49.801 --> 00:11:52.480
you have to understand not everyone is, but that's.

207
00:11:52.480 --> 00:11:54.400
<v 0>A, that's a, one of the more uncomfortable.</v>

208
00:11:54.400 --> 00:11:57.640
<v 1>Things about today with social media is that there's a bunch of people that are</v>

209
00:11:57.641 --> 00:12:00.600
really just, they're just authoritarian.

210
00:12:00.670 --> 00:12:05.600
They're they're they demand certain types of behavior and they do so under the

211
00:12:05.601 --> 00:12:07.400
guise of compassion they do.

212
00:12:07.401 --> 00:12:10.640
So under like they're trying to enforce the way people communicate,

213
00:12:10.641 --> 00:12:14.390
like the Neil Degrass Tyson thing. There's you there's,

214
00:12:14.420 --> 00:12:19.310
there's no indication whatsoever that he was minimizing the no, the deaths.

215
00:12:19.410 --> 00:12:22.670
No, but people pretending that he's doing so yeah, there,

216
00:12:22.671 --> 00:12:25.110
there were some people who believe he did,

217
00:12:25.130 --> 00:12:28.590
but there were a lot of people pretending to be right. Offending, pretending.

218
00:12:28.590 --> 00:12:31.150
<v 0>To be upset. They're finding a nice target. Right? Yeah.</v>

219
00:12:32.230 --> 00:12:34.100
<v 1>I, you know, &lt;affirmative&gt;, there's.</v>

220
00:12:34.100 --> 00:12:37.500
<v 0>Too many voices, Alonzo, too many words, too many people out there.</v>

221
00:12:38.050 --> 00:12:39.700
<v 1>Spewing again, that's the, the,</v>

222
00:12:39.720 --> 00:12:43.380
as we spew the positive and thenegativeity of the internet. Yeah. Yeah.

223
00:12:43.381 --> 00:12:47.020
That's the positive and the negative, the positive is everyone has a voice.

224
00:12:47.120 --> 00:12:51.380
The negative is everyone has a voice. Yeah. &lt;laugh&gt; but this no there's no.

225
00:12:51.920 --> 00:12:56.850
And, and everyone's like, and, and the thing is the,

226
00:12:57.950 --> 00:13:00.970
the middle, the calmest voices, the,

227
00:13:01.110 --> 00:13:04.490
the reasonable voices are the least heard. Of course.

228
00:13:04.760 --> 00:13:09.450
<v 0>Yeah. Well that's Facebook, right? Facebook's algorithm favors outrage. Yeah.</v>

229
00:13:09.451 --> 00:13:13.290
So if you are on Facebook and you get upset about abortion,

230
00:13:14.100 --> 00:13:15.880
the kinda you're gonna get in your feet. Right.

231
00:13:15.900 --> 00:13:17.880
If you get mad about climate change, there you go.

232
00:13:17.881 --> 00:13:20.200
You're gonna get a lot of climate change talk. Yeah. That that's,

233
00:13:20.201 --> 00:13:23.920
what's gonna show up and God, that makes people nuts.

234
00:13:24.240 --> 00:13:26.600
<v 1>I mean, oh a again, and this is living in the bubble.</v>

235
00:13:26.700 --> 00:13:30.920
You don't get to hear the other side. Yeah. You know, and, and, and,

236
00:13:31.900 --> 00:13:35.430
and it's not even the, it's the reasonable part at the other side.

237
00:13:35.431 --> 00:13:38.630
Mm-hmm &lt;affirmative&gt;, you know what I mean? Like politically, yeah. I'm left.

238
00:13:38.990 --> 00:13:42.790
I have friends who I call reasonable Republicans and I could talk to a

239
00:13:42.791 --> 00:13:45.950
reasonable Republican. Yeah. I can't talk to a crazy Republican. Right.

240
00:13:46.170 --> 00:13:47.150
You know, can't, if you,

241
00:13:47.151 --> 00:13:51.710
if you say that the mass shootings are based on transgender marriage &lt;laugh&gt;

242
00:13:52.220 --> 00:13:55.940
then we can't talk. We, we got nothing. I I'm sorry, I can't work with you.

243
00:13:55.990 --> 00:13:56.440
Right.

244
00:13:56.440 --> 00:14:00.300
But if you say that taxes should be lower to stimulate the economy and blah,

245
00:14:00.301 --> 00:14:02.940
blah, blah. Yeah. We could talk about that. You know, you know what I mean?

246
00:14:03.060 --> 00:14:06.980
Yeah. That's, that's, that's the difference. Um, and you know, and just,

247
00:14:07.010 --> 00:14:10.940
just like on the left look, I, I believe in the environment, this and that,

248
00:14:10.941 --> 00:14:14.020
but now if you're asking me to give up gasoline, we may have a problem.

249
00:14:14.660 --> 00:14:18.010
I may not be real ready to go that far, just yet.

