WEBVTT

1
00:00:00.360 --> 00:00:03.320
<v 0>Um, but to get back to artificial intelligence.</v>

2
00:00:03.321 --> 00:00:06.600
So the idea is that there's two camps.

3
00:00:06.601 --> 00:00:11.200
There's there's one camp that thinks that the exponential increase in

4
00:00:11.201 --> 00:00:14.760
technology and that once artificial intelligence becomes sentient,

5
00:00:15.100 --> 00:00:19.920
it could eventually improve upon its own design and literally become a

6
00:00:19.940 --> 00:00:22.430
God a short amount of time.

7
00:00:23.130 --> 00:00:27.590
And then there's the other school of thought that thinks that is so far outside

8
00:00:27.591 --> 00:00:32.150
of the realm of what is possible today that even the

9
00:00:32.151 --> 00:00:36.510
speculation of this eventually taking place is kind of ludicrous to imagine.

10
00:00:37.160 --> 00:00:37.660
Right?

11
00:00:37.660 --> 00:00:41.300
<v 1>Exactly. And the balance needs to be truck,</v>

12
00:00:41.301 --> 00:00:44.460
because I think I'd like to talk about sort of the short term threats that are

13
00:00:44.461 --> 00:00:49.100
there. And that's really important to think about, but the long term threats,

14
00:00:49.680 --> 00:00:53.700
if they come to fruition will overpower everything. Right.

15
00:00:53.720 --> 00:00:55.740
That's really important to think about.

16
00:00:56.360 --> 00:01:00.580
But what happens is if you think too much about the, uh,

17
00:01:00.610 --> 00:01:02.450
encroaching due of humanity,

18
00:01:03.000 --> 00:01:06.770
there's some aspect to it that is paralyzing

19
00:01:07.860 --> 00:01:12.850
where you almost, it turns you off from actually thinking about the,

20
00:01:12.980 --> 00:01:15.210
these ideas. There's something so appealing.

21
00:01:15.600 --> 00:01:17.290
It's like a black hole that pulls you in.

22
00:01:18.030 --> 00:01:20.890
And if you notice folks like Sam Harrison,

23
00:01:21.150 --> 00:01:24.720
so on a large amount of the time, you know,

24
00:01:25.670 --> 00:01:29.800
they're talking about the negative stuff about something that's far away

25
00:01:30.620 --> 00:01:32.600
not to say it's not wrong to talk about it,

26
00:01:32.601 --> 00:01:37.160
but they spend very little time about the potential positive impacts in the near

27
00:01:37.161 --> 00:01:41.800
term and also the negative impacts in the near term. So let's go over those yep.

28
00:01:41.920 --> 00:01:43.870
Fairness. So the, we,

29
00:01:43.970 --> 00:01:48.710
the more and more we put decisions about our lives

30
00:01:48.820 --> 00:01:50.790
into the hands of artificial intelligence systems,

31
00:01:50.900 --> 00:01:54.750
whether you get a loan or, uh,

32
00:01:55.030 --> 00:01:58.830
in the autonomous vehicle context, or in terms of, um,

33
00:01:59.310 --> 00:02:03.540
recommending jobs for you on LinkedIn or all these kind of the things,

34
00:02:04.320 --> 00:02:07.220
the idea of fairness becomes of bias in,

35
00:02:07.221 --> 00:02:12.100
in these machine learning systems becomes a really big threat because the

36
00:02:12.101 --> 00:02:14.940
way current neuro, uh,

37
00:02:15.120 --> 00:02:19.740
the way current artificial intelligence systems function is they train on data.

38
00:02:20.400 --> 00:02:21.740
So there's no way to,

39
00:02:21.741 --> 00:02:26.650
for them to somehow &lt;affirmative&gt; gain a greater intelligence than

40
00:02:26.651 --> 00:02:30.290
our, than the data we provide them with. So we provide them with actual data.

41
00:02:30.510 --> 00:02:35.050
And so they carry over. If we're not careful the biases in that data, the,

42
00:02:35.950 --> 00:02:39.130
the discrimination that's inherent our current society as,

43
00:02:39.150 --> 00:02:42.090
as represented by the data. So they'll, they'll just carry that forward.

44
00:02:42.640 --> 00:02:43.130
Like how.

45
00:02:43.130 --> 00:02:43.963
<v 0>So?</v>

46
00:02:44.880 --> 00:02:47.680
<v 1>Uh, so there's people working in this more. So to see, uh,</v>

47
00:02:47.740 --> 00:02:52.600
to show really the negative impacts in terms of getting

48
00:02:52.840 --> 00:02:57.800
a loan or whether to say whether this particular human being should

49
00:02:57.801 --> 00:03:02.560
be convicted or not of a crime or there's there's

50
00:03:02.730 --> 00:03:05.840
ideas there that can carry, you know,

51
00:03:06.100 --> 00:03:08.160
in our criminal system there's discrimination.

52
00:03:09.220 --> 00:03:14.200
And if you use data from that criminal system to then assist deciders judges,

53
00:03:15.140 --> 00:03:20.110
juries lawyers in making this incriminating in making a decision of what kind of

54
00:03:20.111 --> 00:03:22.910
penalty a person gets, they're gonna carry that forward.

55
00:03:23.650 --> 00:03:28.150
So you mean like racial, economic biases, racial, economic, yeah. Um,

56
00:03:28.510 --> 00:03:33.310
geographical. And that's a sort of, I don't study that exact problem,

57
00:03:33.610 --> 00:03:34.443
but it's, it's,

58
00:03:34.490 --> 00:03:39.380
you're aware of it because of the tools we're using it own only.

59
00:03:40.720 --> 00:03:45.460
So the two ways that I'd like to talk about neural networks with

60
00:03:45.970 --> 00:03:48.740
okay. With Joe, &lt;laugh&gt; sure let's do it. Okay.

61
00:03:49.360 --> 00:03:54.340
So the current approaches are, there's been a lot of,

62
00:03:55.060 --> 00:03:58.900
uh, demonstrated improvements, exciting new improvements,

63
00:03:59.120 --> 00:04:02.130
and our advancements of our artificial intelligence.

64
00:04:02.190 --> 00:04:05.130
And those are for the most part have to do with neural networks.

65
00:04:05.131 --> 00:04:09.850
Something that's been around since the 1940s has gone to two AI

66
00:04:09.851 --> 00:04:10.251
winters,

67
00:04:10.251 --> 00:04:14.410
where everyone was super hyped and then super bummed and super hyped again,

68
00:04:14.411 --> 00:04:16.530
and bummed again. And now we're in this other hype cycle.

69
00:04:17.510 --> 00:04:18.970
And what neural networks are,

70
00:04:19.230 --> 00:04:23.640
is these collections of interconnected, simple compute units.

71
00:04:23.641 --> 00:04:27.480
They're all similar. It's kind of like it's inspired by our own brain.

72
00:04:27.500 --> 00:04:29.440
We have a bunch of little neurons interconnected.

73
00:04:30.060 --> 00:04:34.240
And the idea is these interconnections are really dominant and random,

74
00:04:34.740 --> 00:04:36.360
but if you feed it with some data,

75
00:04:36.990 --> 00:04:40.400
they'll learn to connect just like they're doing our brain in a,

76
00:04:40.550 --> 00:04:41.710
that interprets that data.

77
00:04:42.180 --> 00:04:45.030
They form representations of that data and can make decisions.

78
00:04:45.970 --> 00:04:49.590
But there's only two ways to train those neural networks that we have. Now.

79
00:04:50.770 --> 00:04:53.470
One is we have to provide a large data set.

80
00:04:54.010 --> 00:04:56.750
If you want that neural network to tell the difference between a cat and a dog,

81
00:04:57.210 --> 00:05:01.620
you have to give it 10,000 images of a cat and 10,000 of a dog.

82
00:05:02.880 --> 00:05:04.460
You need to give it those images.

83
00:05:05.400 --> 00:05:10.180
And who tells you what a picture of a cat and a dog is it's humans.

84
00:05:10.440 --> 00:05:11.660
So it has to be annotated.

85
00:05:12.400 --> 00:05:15.460
So as teachers of these artificial intelligence systems,

86
00:05:15.461 --> 00:05:16.620
we have to collect this data.

87
00:05:17.280 --> 00:05:20.500
We have to invest significant amount of effort and

88
00:05:22.050 --> 00:05:25.210
annotate that data. And then we teach neural networks, uh,

89
00:05:25.230 --> 00:05:26.330
to make that prediction.

90
00:05:27.070 --> 00:05:32.010
The what's not obvious there is how poor of a

91
00:05:32.011 --> 00:05:35.530
method there is to achieve any kind of greater degree of intelligence.

92
00:05:35.790 --> 00:05:40.410
You're just not able to get very far besides very specific narrow

93
00:05:41.580 --> 00:05:45.120
tasks of cat versus dog, or, uh,

94
00:05:45.121 --> 00:05:48.200
should I give this person a loan or not these kind of simple,

95
00:05:48.300 --> 00:05:53.080
simple tasks I would argue autonomous vehicles are actually beyond the scope of

96
00:05:53.081 --> 00:05:53.960
that kind of approach.

97
00:05:54.700 --> 00:05:58.760
And then the other realm of where neural networks can be trained

98
00:06:00.060 --> 00:06:01.840
is if you can simulate that world.

99
00:06:02.780 --> 00:06:07.640
So if the world is simple enough or is conducive to

100
00:06:07.660 --> 00:06:11.040
be formalized sufficiently to where you can simulate it.

101
00:06:11.041 --> 00:06:15.720
So a game of chess is just it's it's there's rules, a game of go there's rules.

102
00:06:15.721 --> 00:06:17.910
So you can, they had the big,

103
00:06:18.230 --> 00:06:23.190
exciting thing about Google deep mind is that they were able to beat the world

104
00:06:23.470 --> 00:06:27.590
champion by doing something called competitive self play, uh,

105
00:06:27.600 --> 00:06:30.030
which is they have two systems play against each other.

106
00:06:30.340 --> 00:06:33.550
They don't need the human, they play against each other, but that only works.

107
00:06:33.551 --> 00:06:37.260
And that's a beautiful idea and super powerful and really interesting and

108
00:06:37.261 --> 00:06:41.660
surprising, but that only works on things like games and simulation.

109
00:06:42.560 --> 00:06:45.540
So now, if I wanted to, uh,

110
00:06:45.670 --> 00:06:48.420
sorry to be going to analogies of like UFC, for example,

111
00:06:49.200 --> 00:06:54.140
if I wanted to train a system to become the world

112
00:06:54.420 --> 00:06:59.290
champion, uh, be, uh, what's his name? I'm that up? Right?

113
00:07:00.490 --> 00:07:04.050
I could play the UFC game. I, I could create Sy.

114
00:07:04.490 --> 00:07:08.770
I create two neural networks that play use competitive self play to play in that

115
00:07:08.771 --> 00:07:12.970
virtual world. And they could become state of the art,

116
00:07:13.030 --> 00:07:17.890
the best fighter ever in that game. But transferring that to the physical world,

117
00:07:18.300 --> 00:07:19.440
we don't know how to do that.

118
00:07:20.500 --> 00:07:23.560
We don't know how to teach systems to do stuff in the real world.

119
00:07:23.900 --> 00:07:27.720
So some of the stuff that freaks you out often is Boston dynamics, robots. Ugh.

120
00:07:28.070 --> 00:07:28.903
Yeah.

121
00:07:29.090 --> 00:07:32.640
<v 0>Every day I go to the Instagram page and I just go, what the are you guys doing?</v>

122
00:07:33.400 --> 00:07:34.760
So, uh, engineering, our demise.

123
00:07:35.670 --> 00:07:39.310
<v 1>Mark Reber, CEO is spoke with the class I taught.</v>

124
00:07:40.130 --> 00:07:43.430
He is, uh, he calls himself a bad boy of robotics.

125
00:07:43.930 --> 00:07:46.310
So he's having a little fun with it. He should.

126
00:07:46.310 --> 00:07:49.190
<v 0>Definitely stop doing that. Don't call yourself a bad boy of anything.</v>

127
00:07:49.610 --> 00:07:51.630
That's true. How old is he? &lt;laugh&gt;.

128
00:07:53.750 --> 00:07:57.030
<v 1>&lt;Laugh&gt; okay. He's one of the greatest robotics of our generation. That's.</v>

129
00:07:57.030 --> 00:07:58.990
<v 0>Great. That's wonderful. However, you can't call yourself.</v>

130
00:07:59.430 --> 00:08:01.500
Don't call yourself a bad boy, bro. Okay.

131
00:08:01.640 --> 00:08:06.460
<v 1>So you &lt;laugh&gt;, so you're not the bad boy of MMA. Definitely not.</v>

132
00:08:07.460 --> 00:08:08.293
<v 0>&lt;Laugh&gt;.</v>

133
00:08:09.700 --> 00:08:10.533
<v 1>Um.</v>

134
00:08:11.200 --> 00:08:13.660
<v 0>But I'm not even the bad man. Bad man.</v>

135
00:08:13.900 --> 00:08:18.540
&lt;laugh&gt; definitely not a bad boy. Okay. It's so silly.

136
00:08:20.120 --> 00:08:24.690
<v 1>Yeah. The, those robots are actually functioning in the physical world.</v>

137
00:08:24.691 --> 00:08:25.491
That's what I'm talking about.

138
00:08:25.491 --> 00:08:30.490
Mm-hmm &lt;affirmative&gt; and they are using something called oh,

139
00:08:30.491 --> 00:08:33.250
was I think coined, I don't know, seventies or eighties,

140
00:08:33.390 --> 00:08:35.530
the term good old fashioned AI,

141
00:08:36.200 --> 00:08:40.800
meaning there is nothing like going on that you would

142
00:08:41.040 --> 00:08:42.360
consider artificially intelligent,

143
00:08:43.010 --> 00:08:46.440
which is usually connected to learning.

144
00:08:47.300 --> 00:08:51.080
So these systems aren't learning, it's not like you drop the puppy into the,

145
00:08:51.110 --> 00:08:54.520
into the world and it kind of stumbles around and figures stuff out and learns

146
00:08:54.700 --> 00:08:57.280
is better and better and better and better. That's the scary part.

147
00:08:57.281 --> 00:08:59.320
That's the imagination that, that,

148
00:08:59.321 --> 00:09:03.760
that's what we imagine is we put something in this world at first it's like

149
00:09:04.000 --> 00:09:05.600
harmless, it falls all over the place.

150
00:09:05.620 --> 00:09:07.040
And all of a sudden it figures something out.

151
00:09:07.620 --> 00:09:11.640
And like Elon Musk says like travels faster than whatever than you can only see

152
00:09:11.641 --> 00:09:14.990
with proli. There's no learning component on it. There.

153
00:09:15.940 --> 00:09:20.750
This is just purely there's hydraulics and electric motors.

154
00:09:21.130 --> 00:09:25.350
And there is 20 to 30 degrees of freedom and it's

155
00:09:26.680 --> 00:09:31.150
doing hard coded control algorithms to control the task of how do you move

156
00:09:31.151 --> 00:09:34.790
efficiently through space mm-hmm &lt;affirmative&gt; so this is the task Bodis work on

157
00:09:35.500 --> 00:09:40.020
a really, really hard problem is taking a robotic manipulation,

158
00:09:40.021 --> 00:09:44.660
taking arm, grabbing a water bottle and lifting it super hard,

159
00:09:46.020 --> 00:09:49.500
somewhat unsolved to this point and learning to do that.

160
00:09:49.560 --> 00:09:51.180
We really don't know how to do that.

161
00:09:51.440 --> 00:09:52.240
<v 0>The right,</v>

162
00:09:52.240 --> 00:09:55.380
but this is what we're talking about essentially is the convergence of these

163
00:09:55.960 --> 00:09:59.530
robotic systems with artificially intelligence systems. That's right.

164
00:09:59.750 --> 00:10:03.050
And as artificially intelligence, intelligence systems evolve,

165
00:10:03.590 --> 00:10:07.250
and then this convergence becomes complete.

166
00:10:07.270 --> 00:10:11.530
You're going to have the ability to do things like the computer,

167
00:10:11.531 --> 00:10:15.450
that beat humans that go that's right. You're gonna have creativity.

168
00:10:15.750 --> 00:10:17.120
You you're going to have, uh,

169
00:10:17.280 --> 00:10:22.240
a complex understanding of language and expression. And you're gonna have,

170
00:10:23.160 --> 00:10:27.440
I mean, perhaps even engineered things like emotions like jealousy and anger,

171
00:10:28.120 --> 00:10:31.000
I mean, it's, it's entirely possible that as you were saying,

172
00:10:32.090 --> 00:10:36.400
we're gonna have systems that could potentially be biased the way human beings,

173
00:10:36.960 --> 00:10:37.700
right.

174
00:10:37.700 --> 00:10:41.990
Towards people of certain economic groups or certain geographic groups.

175
00:10:42.170 --> 00:10:46.430
And you would use that data that they have to discriminate just like human

176
00:10:46.431 --> 00:10:47.630
beings discriminate. That's right.

177
00:10:48.380 --> 00:10:52.670
What if you have all that in an artificially intelligent robot that has

178
00:10:53.630 --> 00:10:54.870
autonomy and that has the ability to move,

179
00:10:55.220 --> 00:10:57.420
this is what people are totally concerned and with,

180
00:10:57.421 --> 00:11:01.500
and terrified of is that all of these different systems that are currently in

181
00:11:01.530 --> 00:11:04.740
semi crude states, they can't pick up a water bottle yet.

182
00:11:04.970 --> 00:11:08.380
They can't really do much other than they can do back flips, but they, you know,

183
00:11:08.420 --> 00:11:12.580
I mean, I'm sure you've seen more, the more recent Boston dynamic ones,

184
00:11:12.900 --> 00:11:15.620
par core. Yeah. I saw that one the other day. It's get,

185
00:11:15.621 --> 00:11:18.170
they're getting better and better and better. And it's,

186
00:11:18.560 --> 00:11:22.010
it's increasing every year, every year there's they have new abilities.

