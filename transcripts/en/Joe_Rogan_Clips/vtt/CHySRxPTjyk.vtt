WEBVTT

1
00:00:00.670 --> 00:00:02.880
<v 0>It's uh, AI is, uh,</v>

2
00:00:02.881 --> 00:00:06.160
either people are really excited about it or they're really terrified of it.

3
00:00:06.170 --> 00:00:08.600
Those are the sort, it seems to be the two responses.

4
00:00:08.601 --> 00:00:13.200
Either people have this dismal view of these robots taking over the world,

5
00:00:13.340 --> 00:00:17.800
or they think it's going to be some amazing sort of symbiotic relationship with

6
00:00:17.801 --> 00:00:20.400
that we have with these things. That's gonna evolve human,

7
00:00:20.510 --> 00:00:23.910
human beings past the, the monkey stage that we're at right now.

8
00:00:24.100 --> 00:00:28.830
<v 1>Yeah. And I, I, I tend to be on the ladder more positive side. Good of,</v>

9
00:00:29.170 --> 00:00:31.470
of this dichotomy. But I,

10
00:00:31.550 --> 00:00:35.830
I think one thing that has struck me in recent years is

11
00:00:36.500 --> 00:00:38.630
many people are now, you know,

12
00:00:38.950 --> 00:00:42.860
mentally confronting all the issues running AI for the first time. And I,

13
00:00:43.140 --> 00:00:43.381
I mean,

14
00:00:43.381 --> 00:00:48.380
I've been working on AI for three decades and I first started thinking

15
00:00:48.381 --> 00:00:51.220
about AI when I was a little kid in the early,

16
00:00:51.810 --> 00:00:56.740
late sixties and early seventies when I saw AI in robots on the original star

17
00:00:56.770 --> 00:00:57.120
Trek.

18
00:00:57.120 --> 00:01:01.810
So I guess I've had a lot of cycles to process of

19
00:01:02.410 --> 00:01:03.190
positives and,

20
00:01:03.190 --> 00:01:07.970
and negatives of it where it's now like suddenly most of the world is thinking

21
00:01:07.971 --> 00:01:10.850
through all this for the first, for the first time. And, you know,

22
00:01:10.851 --> 00:01:15.810
when you first wrap your brain around the idea that there may be creatures

23
00:01:16.110 --> 00:01:19.810
10,000 or a million times smarter than, than human beings at first,

24
00:01:19.811 --> 00:01:22.400
this is a bit of a shocker, right? Yeah. And then know, I mean,

25
00:01:22.401 --> 00:01:25.560
it takes a while to internalize this into your worldview.

26
00:01:26.190 --> 00:01:28.520
<v 0>Well, it's that, there's also,</v>

27
00:01:29.000 --> 00:01:33.680
I think there's a problem with the term artificial intelligence, cuz it's, it's,

28
00:01:33.790 --> 00:01:38.120
it's intelligent, it's there, it's a real thing. Yeah. Like it's not artificial,

29
00:01:38.121 --> 00:01:42.950
it's not like a fake diamond or a fake Ferrari. It's a real thing. And it.

30
00:01:43.700 --> 00:01:44.870
<v 1>It's not a great term. Right.</v>

31
00:01:44.890 --> 00:01:49.710
And there's been many attempts to replace it with synthetic

32
00:01:49.711 --> 00:01:53.710
intelligence for, for example. Right. But for better or worse,

33
00:01:53.740 --> 00:01:57.270
like AI is there it's part of the popular imagination.

34
00:01:57.370 --> 00:02:01.670
It seems it's an imperfect word, but it's, it's not going away. Well.

35
00:02:01.790 --> 00:02:03.500
<v 0>I, I, my question is, is like,</v>

36
00:02:03.600 --> 00:02:08.380
are we married to this idea of intelligence and of life being biological,

37
00:02:08.950 --> 00:02:13.740
being carbon-based tissue and cells and blood and or

38
00:02:13.890 --> 00:02:17.140
insects or mammals or fish, or are we married to that too much?

39
00:02:17.280 --> 00:02:22.260
Do you think that it's entirely possible that what human beings are doing

40
00:02:22.480 --> 00:02:24.130
and what people are at the,

41
00:02:24.310 --> 00:02:28.490
the tip of AI right now that are really pushing the technology?

42
00:02:28.680 --> 00:02:32.570
What they're doing is really creating a new lifefor that it's,

43
00:02:32.600 --> 00:02:37.450
it's going to be a new thing that just the same way we recognize wasps and

44
00:02:37.730 --> 00:02:42.090
buffalos and artificial intelligence is just gonna be a life form that emerges

45
00:02:42.091 --> 00:02:44.720
from the creativity and ingenuity of human Inc. Well.

46
00:02:45.400 --> 00:02:50.000
<v 1>In, in indeed. So, I mean I've long been, uh, an advocate of a philosophy.</v>

47
00:02:50.040 --> 00:02:52.440
I think of as, as patternism like,

48
00:02:52.470 --> 00:02:57.040
it's the pattern of organization that appears to be the, the,

49
00:02:57.041 --> 00:02:59.640
the critical thing and the min the, you know,

50
00:02:59.641 --> 00:03:02.960
the individual cells and going down further,

51
00:03:02.990 --> 00:03:07.560
like the molecules and particles in our body are, are turning over all the time.

52
00:03:07.820 --> 00:03:11.600
So it's not the specific combination of elementary particles,

53
00:03:11.601 --> 00:03:13.920
which makes me who I am or makes you who you are.

54
00:03:14.350 --> 00:03:18.040
It's a pattern by which they're organized and the patterns by which they change

55
00:03:18.940 --> 00:03:20.630
over time. So, I mean,

56
00:03:20.650 --> 00:03:25.230
if we can create digital systems or quantum computers or femto

57
00:03:25.470 --> 00:03:29.790
computers or whatever it is manifesting the patterns of organization that

58
00:03:29.791 --> 00:03:33.990
constitute intelligence, I mean, then, then there you are, there,

59
00:03:33.991 --> 00:03:37.950
there there's intelligence, right? So that, that's not to say that, you know,

60
00:03:37.951 --> 00:03:42.180
consciousness and ex experience is just about patterns of organization.

61
00:03:42.180 --> 00:03:44.260
There may be more dimensions to it, but when,

62
00:03:44.730 --> 00:03:47.700
when you look at what constitutes intelligence thinking, cognition,

63
00:03:47.701 --> 00:03:51.140
problem solving, you know, it's the pattern of organization, not,

64
00:03:51.160 --> 00:03:55.500
not the specific material as, as, as far as we can tell.

65
00:03:55.640 --> 00:04:00.610
So we can see no reason based on all the science that we know so

66
00:04:00.630 --> 00:04:01.440
far,

67
00:04:01.440 --> 00:04:05.970
that you couldn't make anelligent system out of some other form of matter,

68
00:04:05.971 --> 00:04:09.970
rather than the, the specific types of itemss and molecules that, that,

69
00:04:09.971 --> 00:04:11.330
that make up human beings.

70
00:04:11.670 --> 00:04:15.330
And it seems that we're were well on the way to being able to do so.

71
00:04:15.840 --> 00:04:18.450
<v 0>When you're studying inte when you're studying intelligence,</v>

72
00:04:18.451 --> 00:04:20.520
you're studying artificial intelligence, do,

73
00:04:20.620 --> 00:04:25.560
did you spend any time studying the patterns that insects seem to

74
00:04:25.590 --> 00:04:28.880
cooperatively behave with, like, uh,

75
00:04:28.881 --> 00:04:33.720
how leaf cutter ants build these elaborate structures underground and, you know,

76
00:04:34.010 --> 00:04:37.800
wasps build these giant colonies. And did you study how.

77
00:04:38.040 --> 00:04:40.040
<v 1>I did actually, yes. So I, I,</v>

78
00:04:40.710 --> 00:04:45.190
I sort of grew up with the philosophy of complex

79
00:04:45.260 --> 00:04:50.030
systems, which was championed by the, by the Santa Fe Institute in, in,

80
00:04:50.031 --> 00:04:55.030
in the 1980s and the whole concept that there's an interdisciplinary complex

81
00:04:55.590 --> 00:04:59.990
systems, science, which includes, you know, biology, cosmology,

82
00:05:00.080 --> 00:05:03.700
psychology, ciology, there's sort of universal patterns of,

83
00:05:04.400 --> 00:05:07.140
of selforganization and, you know, ANTSS,

84
00:05:07.720 --> 00:05:12.660
and auntt colonies have long been a paradigm case for that. And I, I, I, I,

85
00:05:12.820 --> 00:05:16.060
I used to play with the ant colonies in my backyard when, when I,

86
00:05:16.061 --> 00:05:19.180
when I was a kid and you'd lay down food and certain patterns,

87
00:05:19.181 --> 00:05:23.090
you'd see how their ants are laying down phones, and the colonies are,

88
00:05:23.091 --> 00:05:25.210
are organizing it in a, in a certain way.

89
00:05:25.270 --> 00:05:29.650
And that's an interesting self-izinging complex system

90
00:05:30.390 --> 00:05:34.690
on its own. It's lacking some types of adaptive intelligence that,

91
00:05:34.760 --> 00:05:38.010
that human minds and human societies have, but,

92
00:05:38.011 --> 00:05:41.010
but it has also interesting selfganizing patterns.

93
00:05:41.440 --> 00:05:46.040
This me of the novel SOIs by Stan slle,

94
00:05:46.041 --> 00:05:48.440
which was published in, in, in the sixties,

95
00:05:48.450 --> 00:05:53.280
which was really quite, quite a deep, novel, much deeper than the,

96
00:05:53.281 --> 00:05:57.200
the movie that was made of it. Did you ever read the book? SOIs no. So what.

97
00:05:57.780 --> 00:05:59.640
<v 0>I'm not familiar with the movie either who who's in.</v>

98
00:05:59.640 --> 00:06:01.040
<v 1>The movie. Um, well, so there was an amazing,</v>

99
00:06:01.041 --> 00:06:04.840
brilliant movie by tarovski the Russian director from the late sixties.

100
00:06:05.110 --> 00:06:07.600
Then there was a, a movie by Steven Soderberg,

101
00:06:07.601 --> 00:06:11.120
which was sort of glmed up and Americanized and, oh, that.

102
00:06:11.120 --> 00:06:12.040
<v 0>Was fairly recent, right?</v>

103
00:06:12.110 --> 00:06:14.520
<v 1>Yeah. 10 years ago. But that, that, that wasn't,</v>

104
00:06:14.540 --> 00:06:18.270
didn't get all the deep points of novel, original novel in essence,

105
00:06:18.340 --> 00:06:22.910
there's this, there's this ocean on coding,

106
00:06:23.130 --> 00:06:24.910
the surface of some alien planet,

107
00:06:25.160 --> 00:06:28.710
which has amazingly complex fractal patterns of organization.

108
00:06:29.130 --> 00:06:30.590
And it's also interactive,

109
00:06:30.591 --> 00:06:35.230
like the patterns of organization on the ocean respond based on, on what you do.

110
00:06:35.730 --> 00:06:37.340
And when people get near ocean,

111
00:06:37.800 --> 00:06:41.820
it causes them to hallucinate things and even causes them to see

112
00:06:42.290 --> 00:06:44.740
simulatra of people from their past, even the,

113
00:06:44.810 --> 00:06:48.780
like the person who they had most harmed or injured in their past appears and

114
00:06:49.020 --> 00:06:49.600
interacts with them.

115
00:06:49.600 --> 00:06:54.060
So clearly this ocean has some type of amazing complexity and

116
00:06:54.061 --> 00:06:57.300
intelligence from the patterns it displays. And from the,

117
00:06:57.301 --> 00:07:01.930
we weird things that reeks in your mind so that the people on earth

118
00:07:02.430 --> 00:07:05.890
try to understand how the ocean is thinking.

119
00:07:05.891 --> 00:07:08.410
They send a scientific expedition there to,

120
00:07:09.130 --> 00:07:13.090
to in interact with that ocean. But it's just so alien,

121
00:07:13.200 --> 00:07:16.930
even though it monkeys with people's minds and clearly is doing complex things,

122
00:07:17.630 --> 00:07:22.280
no two way communication is ever is ever established.

123
00:07:22.340 --> 00:07:27.000
And eventually the, the human expedition gives up and goes home.

124
00:07:27.100 --> 00:07:31.800
So it it's a very Russian ending to the novel. Uh, I, I guess it's not,

125
00:07:32.080 --> 00:07:36.320
I think I saw that, but the, the, the interest,

126
00:07:36.660 --> 00:07:40.310
the interesting message there is, I mean, there can be many,

127
00:07:40.340 --> 00:07:45.310
many kinds of intelligence, right? I mean, human intelligence is one thing.

128
00:07:45.770 --> 00:07:48.950
The intelligence of an event colonly is a different thing.

129
00:07:49.330 --> 00:07:52.110
The intelligence of human society is a different thing.

130
00:07:52.180 --> 00:07:55.470
Ecosystem is a different thing, and there could be many,

131
00:07:55.471 --> 00:07:59.500
many types of AI that we could build. We in many,

132
00:07:59.501 --> 00:08:04.140
many different properties, some could be wonderful to human beings.

133
00:08:04.141 --> 00:08:05.820
Some could be horrible to human beings.

134
00:08:06.170 --> 00:08:09.340
Some could just be alien minds that,

135
00:08:09.450 --> 00:08:14.380
that we can't even relate, relate, relate to very, very well.

136
00:08:14.480 --> 00:08:18.460
So we, we have a very limited conception of what an intelligence is.

137
00:08:19.280 --> 00:08:23.410
If we just think by close analogy to, to human minds, and this,

138
00:08:24.160 --> 00:08:27.970
this is important if you're thinking about engineering or growing artificial

139
00:08:28.060 --> 00:08:31.650
light forms or artificial minds, cuz it, it's not just, can we do this?

140
00:08:31.960 --> 00:08:34.850
It's what kind of mind are, are,

141
00:08:35.070 --> 00:08:38.570
are we going to engineer or evolve? Right. And there's a,

142
00:08:38.571 --> 00:08:40.880
there's a huge spectrum of possibility. These, yeah.

143
00:08:40.880 --> 00:08:45.560
<v 0>That's one of the reasons why I asked you if we had created,</v>

144
00:08:45.620 --> 00:08:49.680
if human beings who had created some sort of an insect and this insect started

145
00:08:49.850 --> 00:08:52.640
organizing and developing these complex colonies,

146
00:08:52.641 --> 00:08:55.000
like a leaf cutter ant and building these structures underground,

147
00:08:55.300 --> 00:08:56.920
people would go crazy. They would panic.

148
00:08:57.350 --> 00:08:59.280
They would think these things are organizing. They're gonna,

149
00:08:59.281 --> 00:09:01.360
they're gonna build up the resources and attack us.

150
00:09:01.361 --> 00:09:04.880
They're gonna try to take over humanity. I mean this, what,

151
00:09:04.910 --> 00:09:08.640
what people are worried about more than anything when it comes to technology,

152
00:09:08.760 --> 00:09:12.800
I think is the idea that we're going to be, uh, irrelevant,

153
00:09:12.990 --> 00:09:14.870
that we're going to be, uh,

154
00:09:15.270 --> 00:09:19.270
antiques and that something new and better is gonna take our place, which is.

155
00:09:19.780 --> 00:09:21.790
<v 1>It's a weird, which is almost inevit. Yeah.</v>

156
00:09:21.820 --> 00:09:22.910
<v 0>It's a weird thing to worry about it.</v>

157
00:09:22.911 --> 00:09:26.510
Cuz it's sort of the history of biological life on earth. I mean,

158
00:09:26.511 --> 00:09:28.670
what we know is there's complex things.

159
00:09:28.671 --> 00:09:31.990
They become more comp go single celled organisms to multi- celled organisms.

160
00:09:31.991 --> 00:09:36.900
There seems to be a pattern leading up to us and us with this unprecedented

161
00:09:36.901 --> 00:09:39.460
ability to change our environment. That's what we can do. Right.

162
00:09:39.480 --> 00:09:42.460
We can manipulate things, poison the environment.

163
00:09:42.520 --> 00:09:45.660
We can blow up entire countries with bombs if we'd like to.

164
00:09:45.661 --> 00:09:50.380
And we can also do wild creative things like send signals through space and land

165
00:09:50.381 --> 00:09:53.060
on someone else's phone. On the other side of the world, almost instantaneously,

166
00:09:53.080 --> 00:09:55.850
we have incredible power, but we're also,

167
00:09:57.060 --> 00:10:00.330
we're also so limited by our biology. Yeah.

168
00:10:00.510 --> 00:10:03.970
The thing I think people are afraid of and I I'm afraid of,

169
00:10:03.971 --> 00:10:07.330
but I don't know if it makes any sense, is that the next

170
00:10:08.900 --> 00:10:13.570
level of life, whatever artificial life is or whatever the, the,

171
00:10:13.630 --> 00:10:17.000
the human Symbio is that it's gonna lack emotions,

172
00:10:17.030 --> 00:10:19.160
it's gonna lack desires and needs.

173
00:10:19.180 --> 00:10:23.480
And all the things that we think are special about us, our creativity,

174
00:10:23.580 --> 00:10:27.200
our desire for attention and love all of our comradery,

175
00:10:27.660 --> 00:10:30.600
all these different things that are sort of programmed into us with,

176
00:10:30.670 --> 00:10:34.880
with our genetics in order to advance our species,

177
00:10:35.430 --> 00:10:38.510
that we we're so connected to these things. But they're,

178
00:10:38.511 --> 00:10:43.190
so they're the reason for war. They're the reason for lies, deception,

179
00:10:43.810 --> 00:10:44.643
th Thiry.

180
00:10:44.700 --> 00:10:49.110
There's so many things that are built into being a person that are responsible

181
00:10:49.111 --> 00:10:52.030
for all the WOS of humanity, but we're afraid to.

182
00:10:52.030 --> 00:10:56.990
<v 1>Lose those. Yeah. I think it, it's almost inevitable by the,</v>

183
00:10:57.060 --> 00:11:00.460
this point that humanity is going to create

184
00:11:02.340 --> 00:11:07.060
synthetic intelligences with tremendously greater general intelligence

185
00:11:07.920 --> 00:11:11.860
and practical capability than human beings have. I mean,

186
00:11:12.220 --> 00:11:15.660
I think I know how to do that with the software I'm working on with my own team,

187
00:11:16.280 --> 00:11:18.650
but if we've fail, you know,

188
00:11:18.651 --> 00:11:22.050
there's a load of other teams who I think are a bit behind us,

189
00:11:22.070 --> 00:11:24.530
but are going in the same direction though. Right? So you guys feel like.

190
00:11:24.530 --> 00:11:25.530
<v 0>You're at the tip of this PI with.</v>

191
00:11:25.530 --> 00:11:27.610
<v 1>This stuff. I do, but I also think</v>

192
00:11:29.150 --> 00:11:31.970
that's not the most important thing from a human perspective.

193
00:11:32.030 --> 00:11:36.330
The most important thing is that humanity as a whole is quite close to this,

194
00:11:36.480 --> 00:11:39.920
this threshold event. Right. So how far do you think it's.

195
00:11:39.920 --> 00:11:40.753
<v 0>Quite close.</v>

196
00:11:41.500 --> 00:11:45.360
<v 1>By my own gut feeling five to 30 years, let's say that's pretty close,</v>

197
00:11:45.500 --> 00:11:49.560
but if I'm wrong and it's a hundred years, like in the historical time scale,

198
00:11:50.030 --> 00:11:51.720
that sort of doesn't matter. It's like,

199
00:11:52.060 --> 00:11:56.840
did the Sumerians create civilization 10,000 or 10,050 years ago? Like what,

200
00:11:56.841 --> 00:11:58.120
what difference does it make? Right. Right.

201
00:11:58.121 --> 00:12:03.120
So I think we're quite close to creating superhuman

202
00:12:03.130 --> 00:12:05.560
artificial general intelligence.

203
00:12:06.180 --> 00:12:11.040
And that's in a way almost inevitable given where we are

204
00:12:11.220 --> 00:12:13.150
now, on the other hand,

205
00:12:14.270 --> 00:12:19.230
I think we still have some agency regarding whether this comes out

206
00:12:19.370 --> 00:12:21.110
in a way that, you know,

207
00:12:21.750 --> 00:12:26.110
respects human values and culture, which are important to us now,

208
00:12:26.111 --> 00:12:27.710
given who and what we are. Yeah.

209
00:12:27.850 --> 00:12:31.790
Or that is essentially indifferent to human values and,

210
00:12:32.030 --> 00:12:36.180
and culture in the same way that we're mostly indifferent to chimpanzee values

211
00:12:36.320 --> 00:12:38.660
and, and culture at, at, at this point,

212
00:12:39.100 --> 00:12:43.140
I mean completely indifferent to insect values and culture. Not completely. But,

213
00:12:43.300 --> 00:12:45.980
uh, if you think about it, I mean, if I'm building a new house,

214
00:12:46.940 --> 00:12:48.460
I will bulldoze a bunch of events,

215
00:12:48.480 --> 00:12:53.020
but yet we get upset if we extinct an insect species. Right. So we, we care,

216
00:12:53.080 --> 00:12:56.210
we carry to some level, but not, but we,

217
00:12:56.211 --> 00:13:00.850
but we would like the superis to care about us more than we care about insects

218
00:13:00.910 --> 00:13:05.170
or, or grade eights. Sure. AB absolutely. Right. And I think this,

219
00:13:05.640 --> 00:13:10.570
this is something we can impact right now. And to, to, to be honest,

220
00:13:12.090 --> 00:13:16.200
I mean, in a certain part of my mind, I, I, I can think, well, like in,

221
00:13:16.220 --> 00:13:19.040
in the end, I don't matter that much.

222
00:13:19.540 --> 00:13:23.120
My four kids don't matter that much. My granddaughter doesn't matter that much.

223
00:13:23.150 --> 00:13:28.080
Like we are patterns of organization in a very long lineage of

224
00:13:28.081 --> 00:13:31.040
patterns of organization, but they matter very much to you. Yeah. And,

225
00:13:31.041 --> 00:13:35.430
and the other, you know, dinosaurs came and went and the, those came and went,

226
00:13:35.431 --> 00:13:37.550
humans may come and go.

227
00:13:37.850 --> 00:13:42.390
The AI that we create may come and go, and that's the nature of the universe.

228
00:13:42.410 --> 00:13:45.870
But on the other hand, of course, in my heart,

229
00:13:45.940 --> 00:13:49.910
from my situated perspective, as an individual human, like if,

230
00:13:49.930 --> 00:13:53.630
if some AI tried to annihilate my,

231
00:13:53.810 --> 00:13:58.100
my 10 month son, I would try to kill that AI. Right? Sure. Like, so a a as,

232
00:13:58.320 --> 00:14:02.420
as a human being situated in this specific, you know,

233
00:14:02.421 --> 00:14:07.100
species place and time, I care a lot about the condition of,

234
00:14:07.160 --> 00:14:10.580
of all of us humans. And so I,

235
00:14:10.780 --> 00:14:15.260
I would like to not only create a powerful general intelligence, but,

236
00:14:15.261 --> 00:14:17.290
but to create one, which, which is,

237
00:14:18.310 --> 00:14:22.370
is going to be beneficial to humans and,

238
00:14:22.371 --> 00:14:26.650
and other life forms on, on the planet, even while in some ways going,

239
00:14:26.780 --> 00:14:30.010
going beyond EV everything that, that we are. Right.

240
00:14:30.110 --> 00:14:34.570
And there can't be any guarantees about something like this. On the other hand,

241
00:14:35.630 --> 00:14:40.520
you Maddy has really never had any guarantees about anything anyway,

242
00:14:40.521 --> 00:14:44.640
right? I mean, since, since, since, since we created civilization,

243
00:14:44.641 --> 00:14:48.760
we've been leaping into the unknown one time after the other, in,

244
00:14:49.000 --> 00:14:53.800
in a somewhat conscious, and selfware way about it from, you know,

245
00:14:53.910 --> 00:14:57.520
agriculture to language, to math, to the industrial revolution,

246
00:14:57.610 --> 00:15:00.880
we're leaping into the unknown all the time,

247
00:15:00.881 --> 00:15:05.600
which is part of why we are where we are today instead of just another

248
00:15:05.940 --> 00:15:07.480
animal species. Right?

249
00:15:07.700 --> 00:15:11.350
So we can't have a guarantee that

250
00:15:12.580 --> 00:15:14.350
Agis artificial general intelligences,

251
00:15:14.351 --> 00:15:18.110
we create are going to do what we consider the right thing,

252
00:15:18.111 --> 00:15:20.870
given our current value systems. On, on the other hand,

253
00:15:22.510 --> 00:15:26.790
I suspect we can bias the odds in the favor of,

254
00:15:26.890 --> 00:15:31.860
of human values and, and culture and that something I've,

255
00:15:32.490 --> 00:15:36.460
I've put a lot of thought and work into alongside the, you know,

256
00:15:36.461 --> 00:15:40.140
the basic algorithms of, of artificial cognition.

