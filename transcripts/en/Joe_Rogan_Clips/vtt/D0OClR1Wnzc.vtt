WEBVTT

1
00:00:00.090 --> 00:00:03.570
<v 0>I honestly think it's coming from a certain not gonna like this.</v>

2
00:00:04.170 --> 00:00:05.790
I think it's coming from his wife. This is.

3
00:00:05.790 --> 00:00:09.980
<v 1>Going around today. The jacket she wore to, uh, uh.</v>

4
00:00:10.040 --> 00:00:11.900
<v 2>Oh, what say it says.</v>

5
00:00:11.900 --> 00:00:13.820
<v 3>I really don't care. Do you? Yeah.</v>

6
00:00:14.030 --> 00:00:17.870
<v 1>And this was to a meeting of D like, uh, immigration. Yeah. Kids.</v>

7
00:00:18.110 --> 00:00:20.210
She's the kids. Yeah. What is that?

8
00:00:20.210 --> 00:00:22.280
<v 3>Is that, is that some fashion jacket.</v>

9
00:00:22.640 --> 00:00:24.230
<v 1>That she wore to that event? Yeah.</v>

10
00:00:24.560 --> 00:00:28.340
<v 3>Stupid thing to wear. I really don't care. Do you?</v>

11
00:00:29.270 --> 00:00:30.830
<v 0>Yeah, I care. Yeah.</v>

12
00:00:30.830 --> 00:00:35.540
<v 3>I do care. But what are we talking about? What an bizarre open-ended question.</v>

13
00:00:35.541 --> 00:00:36.680
I really don't care about.

14
00:00:36.680 --> 00:00:39.980
<v 2>What the tone deafness of these people.</v>

15
00:00:40.100 --> 00:00:41.840
They have no sensitivity.

16
00:00:43.250 --> 00:00:46.310
<v 3>Well critic though, of separating children from their families.</v>

17
00:00:46.550 --> 00:00:50.570
And she actually made a surprise visit to the Mexican border today to check in

18
00:00:50.571 --> 00:00:53.930
on it, to see what's going on. Good for her. Yeah. Good. Well,

19
00:00:53.931 --> 00:00:56.600
she's an immigrant. I mean, this is the irony of the whole thing.

20
00:00:56.601 --> 00:00:57.860
She barely speaks English.

21
00:00:57.861 --> 00:01:02.810
The whole thing is so bizarre is that his own wife is a immigrant.

22
00:01:02.840 --> 00:01:07.190
I mean, clearly the whole thing's crazy. It's it's really strange,

23
00:01:07.220 --> 00:01:11.270
but that jacket, this what? This is the sign of the simulation.

24
00:01:11.271 --> 00:01:11.840
Have you considered.

25
00:01:11.840 --> 00:01:12.770
<v 2>The simulation theory?</v>

26
00:01:14.450 --> 00:01:18.110
<v 0>Yes. Thank you. Real real might be real.</v>

27
00:01:18.710 --> 00:01:23.630
<v 2>I mean, sometimes I do wonder. I mean, it's like, it's like reality,</v>

28
00:01:23.690 --> 00:01:28.550
whatever we choose to call it is becoming so weird that

29
00:01:28.940 --> 00:01:33.500
I often ask myself who is writing this and can't they get a

30
00:01:33.501 --> 00:01:35.070
better writer. You know,

31
00:01:35.420 --> 00:01:39.470
the plot is so absurd and yet, yeah. Okay. I.

32
00:01:39.470 --> 00:01:43.940
<v 3>Began actively considering it when Congressman Wiener kept pulling his Dick out,</v>

33
00:01:44.780 --> 00:01:46.280
like, this is.

34
00:01:46.340 --> 00:01:49.190
<v 0>Just crazy, right. That guy.</v>

35
00:01:49.550 --> 00:01:52.220
<v 3>I'm like, this is too much. And then when Trump won,</v>

36
00:01:52.221 --> 00:01:55.430
I just was sitting down going, imagine if we,

37
00:01:55.431 --> 00:01:58.670
one day someone shuts it off and the lights dim,

38
00:01:58.671 --> 00:02:02.690
and then they turn back on and you realize, well, the game's over.

39
00:02:02.720 --> 00:02:06.620
How did you like it? And you're like, what? That was fake. Yeah. You,

40
00:02:06.621 --> 00:02:10.700
you just went and it only took an hour. How long did it seem like 50 years,

41
00:02:11.180 --> 00:02:12.860
you know? And yeah, you you've,

42
00:02:13.310 --> 00:02:18.290
you're in a simulation with artificial memories implanted into your mind. Well,

43
00:02:18.291 --> 00:02:21.740
the one day that there's the idea is that there's going to be an artificial

44
00:02:21.741 --> 00:02:25.910
reality or a virtual reality is so good that it's indistinguishable. I mean,

45
00:02:25.911 --> 00:02:27.830
this is almost inevitable.

46
00:02:27.890 --> 00:02:30.800
If technology increases at the same rate that it's increasing now,

47
00:02:30.801 --> 00:02:32.990
whether it's 50 years from now or a hundred years from now,

48
00:02:33.200 --> 00:02:35.060
we're going to reach some point in time.

49
00:02:35.480 --> 00:02:38.870
So the real question is when we do reach that, how will we know?

50
00:02:39.680 --> 00:02:41.240
<v 2>Well, what if we're already there?</v>

51
00:02:41.270 --> 00:02:45.470
<v 0>Yes. But if it is a simulation, how would we know? And how do we test it?</v>

52
00:02:45.830 --> 00:02:46.663
How do we test it?

53
00:02:47.750 --> 00:02:49.970
<v 2>But another aspect of this is,</v>

54
00:02:51.230 --> 00:02:54.080
do we really want this, you know, do, I mean,

55
00:02:54.081 --> 00:02:57.500
do we want to be immersed in a virtual reality,

56
00:02:57.530 --> 00:03:02.470
even if we could produce one so sophisticated, we couldn't tell it from,

57
00:03:03.640 --> 00:03:07.750
from whatever this is. Let's assume for the moment that this is reality.

58
00:03:07.751 --> 00:03:11.830
It's not, do we want to migrate into a virtual reality?

59
00:03:12.060 --> 00:03:12.930
<v 3>My concern,</v>

60
00:03:12.931 --> 00:03:16.820
my real concern is that we are the last wave of the biological human I'm.

61
00:03:16.821 --> 00:03:19.680
I'm I'm really, I really do believe I'm concerned.

62
00:03:19.680 --> 00:03:24.630
<v 2>About that too. I'm not sure. I think that that's a good thing. I necessarily.</v>

63
00:03:24.630 --> 00:03:26.460
<v 3>Think it's a good thing for the biological human,</v>

64
00:03:26.730 --> 00:03:30.060
but I feel like if you separate yourself from the idea of good and bad,

65
00:03:30.490 --> 00:03:33.420
the inevitability of innovation and progress,

66
00:03:34.350 --> 00:03:38.760
it's if human beings continue to make more and more complex electronics with

67
00:03:38.800 --> 00:03:40.320
higher and higher capabilities,

68
00:03:40.590 --> 00:03:45.300
it's inevitable that we become symbiotic with these things that we ingrain them

69
00:03:45.301 --> 00:03:48.420
into. Our they're going to become a part of your body.

70
00:03:48.720 --> 00:03:51.930
We're going to replace body parts with more efficient body parts, right?

71
00:03:51.960 --> 00:03:56.340
And we are one day going to create some sort of artificial life. Now,

72
00:03:56.341 --> 00:03:59.970
whether we become a part of that artificial life, we, we, you know,

73
00:03:59.971 --> 00:04:04.320
merge with it, or it just assumes the role of the leader of the earth.

74
00:04:05.070 --> 00:04:09.600
One of those things is likely to happen within the next 500 years. It's just,

75
00:04:10.050 --> 00:04:12.600
and that's, that's a really generous timeframe. Okay.

76
00:04:12.720 --> 00:04:14.550
I think that's a generous timeframe.

77
00:04:15.000 --> 00:04:18.270
<v 2>Well, you know, this, this, I mean,</v>

78
00:04:18.360 --> 00:04:22.740
in some ways this is sort of a, you know, this, this,

79
00:04:22.800 --> 00:04:24.600
this raises the issue about, you know,

80
00:04:24.601 --> 00:04:28.530
one of the things that psychedelics put in front of us front and center is the

81
00:04:28.531 --> 00:04:32.190
fact that we are getting a strange from nature. You know,

82
00:04:32.250 --> 00:04:35.640
that's the main lesson. We're getting a strange from nature.

83
00:04:35.641 --> 00:04:39.690
We have to re understand our relationship and, and, um, um, you know,

84
00:04:39.691 --> 00:04:42.990
become a partner in the symbiosis with nature.

85
00:04:44.460 --> 00:04:49.140
And this projection is the exact opposite of that.

86
00:04:49.470 --> 00:04:53.010
So is that, you know, so maybe, you know, this,

87
00:04:53.011 --> 00:04:57.780
this raises also one issue that we haven't really touched on, but here,

88
00:04:57.810 --> 00:05:01.620
but you know, technology,

89
00:05:01.770 --> 00:05:06.660
which is what this virtual reality stuff is and what any artifact is.

90
00:05:07.020 --> 00:05:11.010
Psychedelics are technology. Molecular biology is technology.

91
00:05:11.011 --> 00:05:12.750
Cybernetics is technology

92
00:05:14.280 --> 00:05:17.730
technology inherently has no moral dimension. You know,

93
00:05:17.790 --> 00:05:20.940
these are not good or bad things, you know,

94
00:05:21.120 --> 00:05:23.760
the way that they're used by humans,

95
00:05:23.790 --> 00:05:28.650
the decisions that humans make in the way that they're going to exploit or

96
00:05:28.651 --> 00:05:32.790
deploy these technologies, that's where the moral dimension comes in.

97
00:05:34.110 --> 00:05:37.080
Mortality comes out of the human heart, you know,

98
00:05:37.110 --> 00:05:41.010
and we are one of our problems. I feel as a species,

99
00:05:41.011 --> 00:05:45.180
we're extremely clever, but we're not wise move.

100
00:05:45.210 --> 00:05:49.140
That's what it is. We're not wise about what we do.

101
00:05:49.141 --> 00:05:53.910
We're not able to step back and say, well, yeah, we can produce, we can,

102
00:05:54.150 --> 00:05:54.540
you know,

103
00:05:54.540 --> 00:05:58.880
download the brain into cybernetics or we can produce an artificial body,

104
00:05:58.881 --> 00:06:01.220
or we can do all this genetic stuff.

105
00:06:01.310 --> 00:06:05.330
Do we ever stop to think about just because we can do something,

106
00:06:05.331 --> 00:06:08.950
should we do something? You know, uh, we, we, uh,

107
00:06:09.030 --> 00:06:13.910
and the arrogance of science, this is also a problem. The scientists will say,

108
00:06:13.911 --> 00:06:17.420
well, they are scientists, right? We couldn't do it.

109
00:06:17.450 --> 00:06:19.160
So let's do it right.

110
00:06:19.200 --> 00:06:23.600
We couldn't do the Hadrian experiment and oh,

111
00:06:23.630 --> 00:06:27.740
maybe it'll collapse the space time continuum, but the very small probability.

112
00:06:28.280 --> 00:06:32.930
So let's do it right. And this is something we have to learn.

113
00:06:32.931 --> 00:06:36.260
I think also the psychedelics are important in that regard too.

114
00:06:36.410 --> 00:06:41.240
They are ways that we can bring our cleverness and our

115
00:06:41.241 --> 00:06:46.130
wisdom into sync so that we have the wisdom not to do

116
00:06:46.131 --> 00:06:49.550
something, even though we might be able to,

117
00:06:49.551 --> 00:06:53.660
we shouldn't do it just because we can do it. You know, we really have to,

118
00:06:53.661 --> 00:06:56.810
as a species, ask ourselves, is this a good idea?

119
00:06:57.440 --> 00:07:00.350
And I think, again, these,

120
00:07:00.560 --> 00:07:05.410
the psychedelics are teaching tools for learning this and

121
00:07:05.420 --> 00:07:09.730
really propagating the message from the community of species. That's.

122
00:07:09.790 --> 00:07:13.840
<v 3>For sure. Going to be a meme with a photo of your face, that we are very clever,</v>

123
00:07:13.841 --> 00:07:16.930
but we're not wise. That's for sure. Going to be a meme. Some,

124
00:07:16.931 --> 00:07:19.990
some dude is right. Or gal is working on that right now.

125
00:07:20.380 --> 00:07:25.210
<v 0>Well, I've been talking about this sentence. So.</v>

126
00:07:25.720 --> 00:07:30.670
<v 3>So striking my real concern with this stuff is that this is inevitable</v>

127
00:07:31.030 --> 00:07:33.910
and this is just like the single celled organism became the multi-celled

128
00:07:33.911 --> 00:07:36.340
organism. And then the thinking,

129
00:07:36.460 --> 00:07:41.440
curious monkey who strives for material possessions is designed to create

130
00:07:41.441 --> 00:07:44.530
artificial life. And this is just what we've, we're sat here.

131
00:07:44.531 --> 00:07:46.420
I've described it as that.

132
00:07:46.421 --> 00:07:50.890
We are the technological butterfly that will

133
00:07:50.920 --> 00:07:52.270
emerge from the cocoon.

134
00:07:52.271 --> 00:07:56.530
And right now we're creating this cocoon that we are this caterpillar,

135
00:07:57.070 --> 00:07:58.330
this technological caterpillar,

136
00:07:58.331 --> 00:08:01.570
and we don't know why we're making this cocoon and that we are going to give

137
00:08:01.571 --> 00:08:04.240
birth to this artificial life, this next stage.

138
00:08:05.740 --> 00:08:09.070
<v 2>And that may be plexity. That may be true too. I mean, I mean,</v>

139
00:08:09.071 --> 00:08:12.520
what you say is true, you know, on some level,

140
00:08:12.820 --> 00:08:16.420
anything that can be done is going to be done,

141
00:08:17.140 --> 00:08:20.800
somebody's going to do it right. But so, you know,

142
00:08:22.300 --> 00:08:26.920
is it good? Is it, is it good? Is it good for the collective?

143
00:08:26.921 --> 00:08:30.340
I mean, you know, there are always megalomaniacs who will say, well,

144
00:08:30.440 --> 00:08:34.960
I can do it. I can start a nuclear war. So why don't we do this? You know?

145
00:08:35.560 --> 00:08:39.820
Um, and that's, that's the tricky part again, I think, you know, uh,

146
00:08:39.910 --> 00:08:43.990
psychedelics are important in giving us a moral compass. I mean,

147
00:08:43.991 --> 00:08:48.250
a wisdom, not, not a set of,

148
00:08:49.120 --> 00:08:52.920
you know, rules about that come out of the religious perception,

149
00:08:52.940 --> 00:08:55.360
a set of rules that come out of the biological.

150
00:08:56.100 --> 00:08:57.990
What is most compatible?

151
00:08:57.991 --> 00:09:01.440
What is most nourishing nourishing for living things?

152
00:09:01.980 --> 00:09:05.790
Presumably we don't want to trash this planet. You know,

153
00:09:05.820 --> 00:09:08.340
we now have the ability to do that. You know,

154
00:09:08.341 --> 00:09:13.230
the forces that we can manipulate for the first time in history

155
00:09:13.260 --> 00:09:17.250
pose a real possibility that we could add life on earth.

156
00:09:19.050 --> 00:09:24.000
I think it's hard, but I think we may be able to do it. I may, for one,

157
00:09:24.030 --> 00:09:25.990
don't want to see that happen out.

158
00:09:25.990 --> 00:09:30.920
<v 3>A bit, a few years back in my comedy act about the origins of the</v>

159
00:09:30.921 --> 00:09:35.360
universe and that what happens is people get so smart that they develop a big

160
00:09:35.361 --> 00:09:37.400
bang machine and that, uh,

161
00:09:37.490 --> 00:09:41.540
someone's sitting around and some guy who's on the autism spectrum was filled up

162
00:09:41.541 --> 00:09:46.370
with SSRS and antidepressants and drinking red bull all day. He just goes, it.

163
00:09:46.371 --> 00:09:49.610
I'll press it. And he hits the button and boom, we start from scratch.

164
00:09:49.611 --> 00:09:53.180
And then every, you know, 14 plus billion years,

165
00:09:53.420 --> 00:09:57.310
someone develops the big bang machine and hits it. And that's the,

166
00:09:57.311 --> 00:10:00.110
the restart of the universe over and over. We obsessed.

167
00:10:00.110 --> 00:10:04.250
<v 2>With this idea that universal simulation. Yeah, I am.</v>

168
00:10:04.251 --> 00:10:07.220
In some ways it's possible. I mean, it's possible.

169
00:10:07.370 --> 00:10:12.090
<v 3>It is. It's inevitable that there will be simulated worlds, right?</v>

170
00:10:12.590 --> 00:10:16.550
I'm not necessarily completely obsessed with the idea that we're living in a

171
00:10:16.551 --> 00:10:18.020
simulation, but I am completely.

172
00:10:18.020 --> 00:10:20.870
<v 2>Obsessed that we are a Relic.</v>

173
00:10:21.260 --> 00:10:23.750
<v 3>And that we are on our way out. I really am.</v>

174
00:10:23.780 --> 00:10:26.720
I really do think that maybe that's one of the reasons why we're so crazy and so

175
00:10:26.721 --> 00:10:27.170
haywire,

176
00:10:27.170 --> 00:10:31.250
it just shows there's no logical progression for our culture that it's as

177
00:10:31.340 --> 00:10:34.670
advanced as we are as much access to information as we have.

178
00:10:34.820 --> 00:10:37.730
We're also as crazy as we have ever been, if not right. Crazier.

179
00:10:37.790 --> 00:10:42.500
<v 2>Yeah. Yeah. Yeah. So the age of the curious monkey is coming to a.</v>

180
00:10:42.500 --> 00:10:46.310
<v 3>Close, I wonder how much of a limitation our biology is too. I mean,</v>

181
00:10:46.580 --> 00:10:50.660
you think about what it took to get to here and all the battles we had to fight

182
00:10:50.740 --> 00:10:54.620
and the animals we had to run from and all these human reward systems that are

183
00:10:54.621 --> 00:10:56.270
ingrained into our DNA.

184
00:10:56.271 --> 00:11:00.050
And that now here we are in a place where we hardly need them. And, uh,

185
00:11:00.060 --> 00:11:04.910
yet we still have them just blowing up and exploding and vomiting all over the

186
00:11:04.911 --> 00:11:09.740
place and these weird ways. And we have them sort of manifesting themselves,

187
00:11:09.780 --> 00:11:13.400
very strange behaviors that aren't, aren't good for anybody.

188
00:11:13.670 --> 00:11:18.490
And this constant need to acquire material possessions and conquer and, and,

189
00:11:18.770 --> 00:11:23.390
and obtain things that this is, this is not tenable.

190
00:11:23.420 --> 00:11:25.550
This is not something that makes sense in the long haul,

191
00:11:25.551 --> 00:11:28.400
but yet we still go down this illogical road.

192
00:11:28.670 --> 00:11:33.020
And that this is really just because this is the best way to fuel

193
00:11:33.021 --> 00:11:33.681
innovation.

194
00:11:33.681 --> 00:11:37.880
Our extreme desire for material possessions is the best way to ensure that

195
00:11:37.881 --> 00:11:40.370
they're going to keep coming up with newer, better things every year,

196
00:11:40.580 --> 00:11:44.270
which will eventually give birth to the electronic butterfly.

197
00:11:46.820 --> 00:11:49.580
Well, I don't know what to say about, I mean, it.

198
00:11:49.580 --> 00:11:53.980
<v 2>May be that may be where we're, where we're headed and that,</v>

199
00:11:54.520 --> 00:11:57.100
you know, it may be that

200
00:11:59.680 --> 00:12:02.330
this is a necessary step. I mean, if,

201
00:12:02.340 --> 00:12:06.910
if our destiny is to actually leave the earth at some point,

202
00:12:06.950 --> 00:12:07.410
if,

203
00:12:07.410 --> 00:12:12.220
if the earth is an incubator for life and we're just

204
00:12:12.250 --> 00:12:16.600
destined to leave it and spread out into the galaxy

205
00:12:17.770 --> 00:12:22.750
and beyond who knows, I mean then maybe this is inevitable that we have to,

206
00:12:22.780 --> 00:12:25.750
that we have to do that if that's what's happening.

207
00:12:25.810 --> 00:12:30.280
But the question is what kind of being will we be when we do that?

208
00:12:30.760 --> 00:12:34.660
You know, we won't be human right. Will be something different than human.

209
00:12:35.280 --> 00:12:38.070
<v 3>Well, I've always wondered about the alien archetype,</v>

210
00:12:38.310 --> 00:12:42.000
that big headed thing with the no genitals and no mouth.

211
00:12:42.030 --> 00:12:46.890
That may be what we think of as being the ultimate form that

212
00:12:47.090 --> 00:12:50.160
the, the human animal takes. Right. If, when we do,

213
00:12:50.550 --> 00:12:54.450
if we do symbiotically merge with technology and electronics at that might be

214
00:12:54.451 --> 00:12:58.890
the form that we take. It's just so strange that that one excepted form,

215
00:12:59.070 --> 00:13:02.370
and I've heard, um, I've heard the idea that this,

216
00:13:02.371 --> 00:13:07.200
this image is something when young eyes from a newborn baby sees a

217
00:13:07.201 --> 00:13:11.550
doctor and see the doctor with a mask and the face,

218
00:13:11.970 --> 00:13:16.170
this is what they see. And that this is imprinted in our mind,

219
00:13:16.171 --> 00:13:19.500
this traumatic experience of the birth and the bright lights and the operating

220
00:13:19.501 --> 00:13:20.190
table.

221
00:13:20.190 --> 00:13:25.020
This is why so many of these alien abduction experiences do take place in these

222
00:13:25.021 --> 00:13:28.950
very clinic clinical sterile environments.

223
00:13:28.950 --> 00:13:31.650
And it seems like a medical procedure as a,

224
00:13:31.660 --> 00:13:35.610
this is a remnant of the birth process. I've heard that experience.

225
00:13:35.640 --> 00:13:39.120
I've heard that explanation, but it also just,

226
00:13:39.510 --> 00:13:43.590
it strikes me that these things are there. If you go, you,

227
00:13:43.770 --> 00:13:48.150
you go from ancient hominids, you go from Australia, Pythagoras,

228
00:13:48.180 --> 00:13:53.040
and then you go to a modern computer programmer who does an

229
00:13:53.041 --> 00:13:55.410
exercise. And you look at their body,

230
00:13:55.590 --> 00:13:59.420
this sort of like doughy thin bodied.

231
00:13:59.421 --> 00:14:01.080
It doesn't move very well.

232
00:14:01.860 --> 00:14:06.690
And then you go back to this muscular ape-like creature that's covered in

233
00:14:06.691 --> 00:14:10.590
hair. They've lost all the hair. They've lost all the muscle. They become thin.

234
00:14:10.591 --> 00:14:14.220
And then where's that going? Well, it's obviously going in that same direction.

235
00:14:14.221 --> 00:14:14.760
It's not going to,

236
00:14:14.760 --> 00:14:18.750
people are not going to get more muscular and harder and Harrier as time goes

237
00:14:18.751 --> 00:14:22.020
on, unless something radical changes and we need to adapt. Right.

238
00:14:22.021 --> 00:14:25.200
So that would be the normal, I mean, the, the path would,

239
00:14:25.260 --> 00:14:29.430
that would be the natural progression that we would eventually have bigger

240
00:14:29.431 --> 00:14:31.380
heads. Cause we have bigger heads in Australia,

241
00:14:31.381 --> 00:14:34.440
pithy kissed and certainly bigger heads than chimps or Bonobos.

242
00:14:34.530 --> 00:14:36.330
And that it just keeps going in that same direction.

243
00:14:38.730 --> 00:14:43.650
<v 2>Well, possibly. Yeah. Or maybe we just leave the biological shell behind,</v>

244
00:14:43.740 --> 00:14:45.930
you know, but then where's it really shot?

245
00:14:46.770 --> 00:14:51.670
What do we are trans human and we, um, no,

246
00:14:51.680 --> 00:14:54.470
I'm, I'm not sure I want to go there.

247
00:14:54.880 --> 00:14:58.300
<v 3>I think I want to go there, but I'm, I'm what I'm thinking is like, what is,</v>

248
00:14:58.360 --> 00:15:02.290
what is it that's making me clean to these ideas? Is it that I love emotions.

249
00:15:02.680 --> 00:15:05.980
I love illogical behavior. Do I love art? Yeah. I love all those things.

250
00:15:06.250 --> 00:15:10.090
I love music and food and all the things that cooking and all the things that

251
00:15:10.330 --> 00:15:13.630
make a person, a person comradery.

252
00:15:13.631 --> 00:15:17.380
But what are those things are those chemical reactions we have with other beings

253
00:15:17.381 --> 00:15:21.910
and natural reward systems that are built in to sort of enhance community and

254
00:15:22.510 --> 00:15:24.760
comradery so that we stay together. So the species survives,

255
00:15:25.280 --> 00:15:27.310
like what if there's something that supplants that,

256
00:15:27.311 --> 00:15:30.550
what if there's something that far surpasses that in terms of pleasure and

257
00:15:30.580 --> 00:15:33.850
connectivity, and we realize that emotions are just these.

258
00:15:34.480 --> 00:15:36.760
<v 2>These ancient systems.</v>

259
00:15:37.030 --> 00:15:38.410
<v 3>That were put into place when there wasn't,</v>

260
00:15:38.470 --> 00:15:41.320
it wasn't a better option with these better options. It's,

261
00:15:41.650 --> 00:15:45.310
it's much better to get your food from a supermarket than it is to chase down a

262
00:15:45.311 --> 00:15:49.420
gazelle for two days until it dies, you know, heat stroke. You know, I mean,

263
00:15:49.421 --> 00:15:54.040
these, these systems improve over time. You know, this, um,

264
00:15:54.280 --> 00:15:56.440
this animal that we are now is very different than,

265
00:15:56.441 --> 00:16:00.340
than the animal we used to be. Why do we want to stay in this imperfect state?

266
00:16:00.341 --> 00:16:03.490
That seems even more ridiculous that we'd like to stay humans forever,

267
00:16:04.000 --> 00:16:05.680
but humans are so flawed. We been,

268
00:16:05.681 --> 00:16:10.120
there's a reason why we have all this nonsense in the world and our society is

269
00:16:10.121 --> 00:16:12.640
sick and we are twisted and confused. But.

270
00:16:13.150 --> 00:16:15.460
<v 2>A part of it at least has to be.</v>

271
00:16:15.700 --> 00:16:20.050
<v 3>That the human animal itself is very flawed because there's no perfect culture.</v>

272
00:16:20.051 --> 00:16:23.950
You can't just chalk it up to culture because if you did chalk it up to culture,

273
00:16:23.951 --> 00:16:27.340
you would say, well, this culture sucks. But if you go to this culture,

274
00:16:27.341 --> 00:16:31.330
it's amazing. There's no crime. Everyone loves everyone. It's completely open.

275
00:16:31.570 --> 00:16:35.350
There's no need to worry about money because everybody's generous and everything

276
00:16:35.530 --> 00:16:35.891
gets, and they're,

277
00:16:35.891 --> 00:16:39.550
they're really brilliant and they get along and they create new architecture.

278
00:16:39.551 --> 00:16:42.850
And everything's fantastic. It's the perfect society that doesn't exist.

279
00:16:43.360 --> 00:16:47.860
<v 2>But isn't that the culture that we can create with the help of psychedelic</v>

280
00:16:49.120 --> 00:16:49.490
now is.

281
00:16:49.490 --> 00:16:52.450
<v 0>That what we're shooting for now, we're talking human.</v>

282
00:16:52.450 --> 00:16:57.130
<v 2>A truly humanistic culture where people where love is</v>

283
00:16:57.131 --> 00:16:58.150
what's happening,

284
00:16:58.180 --> 00:17:02.410
where it's driven by love and not by hatred and rivalry and

285
00:17:02.411 --> 00:17:07.120
scarcity and fear, you know, and that's the whole thing.

286
00:17:07.121 --> 00:17:11.860
The psychedelics can be the catalyst that teaches us how to love

287
00:17:11.861 --> 00:17:14.980
ourselves, how to love each other, how to love the earth. I mean,

288
00:17:14.981 --> 00:17:18.370
I know that sounds cliche and trivial,

289
00:17:18.371 --> 00:17:20.110
but that is a sincere act.

290
00:17:20.140 --> 00:17:24.580
That is in fact what the promise that they hold for us.

291
00:17:24.581 --> 00:17:28.090
That's why they're teachers, they're teaching, learning tools.

292
00:17:28.091 --> 00:17:32.950
They can teach us to be the human beings that

293
00:17:32.951 --> 00:17:37.870
we would like to be, you know, Ray and I, and that's the thing,

294
00:17:37.871 --> 00:17:42.280
that's, that's the alternative to this hyper technologic coal

295
00:17:43.030 --> 00:17:47.830
future, you know, and, uh, I mean, I'm, I'm all for technology.

296
00:17:47.831 --> 00:17:52.710
I'm not against technology again. I think we have to, you know,

297
00:17:52.711 --> 00:17:53.700
we have to,

298
00:17:56.070 --> 00:18:00.660
we have to bring wisdom to it. We have to make a situation where, you know,

299
00:18:01.530 --> 00:18:04.590
it is not controlling us. We are controlling it.

300
00:18:04.680 --> 00:18:08.160
And we're thinking clearly about, we have, you know,

301
00:18:08.170 --> 00:18:12.150
this enormous panoply of technologies that can do so many things.

302
00:18:12.151 --> 00:18:16.080
We have to think about how do we deploy those in such a way to

303
00:18:16.081 --> 00:18:20.880
maximize human potential or, you know, our humanity.

304
00:18:21.330 --> 00:18:22.590
So that's really,

305
00:18:22.591 --> 00:18:27.330
I think what the promise that psychedelics holdout and that's,

306
00:18:27.331 --> 00:18:30.210
you know, that's what we're hoping to create. Uh, you know,

307
00:18:30.211 --> 00:18:32.460
as a Colonel and we're not the only ones,

308
00:18:32.461 --> 00:18:36.000
obviously a lot of people have this idea and it's happening,

309
00:18:36.030 --> 00:18:40.980
but that's the idea is to create a place where

310
00:18:41.220 --> 00:18:44.790
people couldn't learn this and you know, and, uh,

311
00:18:46.380 --> 00:18:51.260
that's my hope for the future of humanity. Well, it's, it's great hope for the.

312
00:18:51.260 --> 00:18:54.200
<v 3>Future that people that are alive today. And I think that's the most important.</v>

313
00:18:54.201 --> 00:18:56.870
We don't know what's coming, but we do know what's here. Right.

314
00:18:56.871 --> 00:18:58.490
And I think that's a great hope for what's right.

