WEBVTT

1
00:00:01.020 --> 00:00:03.840
<v 0>The Joe Rogan experience. All right, Nick, this is a,</v>

2
00:00:03.841 --> 00:00:08.760
one of the things that scares people more than anything is

3
00:00:08.761 --> 00:00:12.210
the idea that we're creating something or someone's going to create something

4
00:00:12.300 --> 00:00:15.150
that's going to be smarter than us. It's going to replace us.

5
00:00:16.140 --> 00:00:18.780
Is that something we should really be concerned about?

6
00:00:19.970 --> 00:00:21.440
<v 1>I presume you're referring to babies,</v>

7
00:00:23.660 --> 00:00:28.250
I'm referring to artificial intelligence. Yes. Well,

8
00:00:28.251 --> 00:00:33.110
it's the big, fair and the big hope, I think both at the same time. Yeah.

9
00:00:33.700 --> 00:00:35.630
How's it. The big hope? Well,

10
00:00:36.500 --> 00:00:39.230
there are a lot of things wrong with the world as it is now called this up to

11
00:00:39.231 --> 00:00:40.880
your face. Um,

12
00:00:42.770 --> 00:00:45.740
all the problems we have, uh,

13
00:00:46.220 --> 00:00:51.140
most of them could be solved if we were smarter or if we

14
00:00:51.141 --> 00:00:55.100
had somebody on our side who are a lot smarter with better technology and so

15
00:00:55.101 --> 00:00:56.720
forth. Um,

16
00:00:57.500 --> 00:01:01.970
also I think if we want to imagine with some really grand future

17
00:01:02.090 --> 00:01:06.950
where humanity or our descendants one day go out and

18
00:01:06.951 --> 00:01:08.330
colonize the universe,

19
00:01:08.510 --> 00:01:11.480
I think that's likely to happen if it's going to happen at all.

20
00:01:12.350 --> 00:01:16.700
After we have superintelligence that then develops the technology to make that

21
00:01:16.701 --> 00:01:17.534
possible.

22
00:01:17.630 --> 00:01:20.360
<v 0>The real question is whether or not we would be able to harness this</v>

23
00:01:20.361 --> 00:01:22.310
intelligence or whether it would dominate.

24
00:01:23.540 --> 00:01:27.680
<v 1>Yeah, that certainly is one question I'm not the only,</v>

25
00:01:28.460 --> 00:01:30.020
you could imagine that we harness it,

26
00:01:30.470 --> 00:01:34.670
but then use it for bad purposes as we have a lot of other technologies through

27
00:01:34.671 --> 00:01:39.020
history. So I think there are really two challenges we need to meet. One,

28
00:01:39.050 --> 00:01:43.730
one is to make sure we can align it with human values and then make sure that

29
00:01:43.760 --> 00:01:48.290
we together do something better with it than fighting wars or oppressing one

30
00:01:48.291 --> 00:01:49.124
another.

31
00:01:49.400 --> 00:01:49.820
<v 0>Well,</v>

32
00:01:49.820 --> 00:01:52.220
when I'm worried about more than anything is that human beings are going to

33
00:01:52.221 --> 00:01:55.070
become obsolete. That we're going to invent something.

34
00:01:55.071 --> 00:01:58.610
That's the next stage of evolution. I'm really concerned with that.

35
00:01:58.670 --> 00:02:02.780
I'm really concerned with, if we look back on ancient hominids, Australia,

36
00:02:03.080 --> 00:02:06.380
pithy guests, just think of some primitive ancestor of man.

37
00:02:06.770 --> 00:02:10.730
We don't want to go back to that. Like that that's a terrible way to live.

38
00:02:11.570 --> 00:02:14.810
I'm worried that what we're creating is the next thing.

39
00:02:16.760 --> 00:02:17.593
<v 1>I think</v>

40
00:02:19.370 --> 00:02:22.070
we don't necessarily want,

41
00:02:22.460 --> 00:02:25.760
or at least I wouldn't be totally thrilled with,

42
00:02:25.780 --> 00:02:30.710
with a future where humanity as it is now, what's the last and final word,

43
00:02:31.460 --> 00:02:33.980
the PA like ultimate version beyond, right?

44
00:02:34.080 --> 00:02:36.080
I think there's a lot of room for improvement. Sure.

45
00:02:36.110 --> 00:02:39.440
But not anything that is different. It's an improvement. Right? So,

46
00:02:39.500 --> 00:02:40.580
so the key would be,

47
00:02:40.730 --> 00:02:45.680
I think to find some path forward where the best in

48
00:02:45.681 --> 00:02:49.730
us, uh, can continue to exist and develop, uh,

49
00:02:49.760 --> 00:02:52.400
to even greater levels. And maybe at the end of that path,

50
00:02:53.120 --> 00:02:56.810
it looks nothing like we do not. Maybe it's not two legged,

51
00:02:56.811 --> 00:03:00.850
two armed creatures running around with three of thinking matter, right?

52
00:03:00.970 --> 00:03:04.490
It might be something quite different, but as long as it, what we value is,

53
00:03:04.491 --> 00:03:07.990
is present there and ideally in a much higher degree than in the current world,

54
00:03:08.020 --> 00:03:10.120
then that could count as a success.

55
00:03:10.800 --> 00:03:15.660
<v 0>Yeah. The idea that we're in a state of evolution that we are just like,</v>

56
00:03:15.661 --> 00:03:19.530
we look at ancient hominids that we are eventually going to become something

57
00:03:19.560 --> 00:03:22.590
more advanced or at least more complicated than we are now.

58
00:03:23.100 --> 00:03:27.120
But what I'm worried is that biological life itself has so many limitations.

59
00:03:27.300 --> 00:03:32.040
When we look at the evolution of technology, if you look at Moore's law,

60
00:03:32.070 --> 00:03:33.720
or if you'd just look at new cell phones,

61
00:03:33.721 --> 00:03:35.640
like they just released a new iPhone yesterday,

62
00:03:35.641 --> 00:03:38.910
and they talked about all these incremental increases in the ability to take

63
00:03:38.911 --> 00:03:43.200
photographs and wide angle lenses and night mode and a new chip that works even

64
00:03:43.201 --> 00:03:47.580
faster. These things, there's not the word evolutions incorrect,

65
00:03:47.610 --> 00:03:51.900
but the innovation of technology is so much more rapid than anything we could

66
00:03:51.901 --> 00:03:56.820
ever even imagine. Biologically, like if we had a thing that we'd created,

67
00:03:56.821 --> 00:04:00.030
if we had created, um, instead of artificial intelligence,

68
00:04:00.031 --> 00:04:02.850
in terms of like some something in a chipper computer,

69
00:04:03.210 --> 00:04:06.570
if we created a life form, a biological life form,

70
00:04:06.810 --> 00:04:10.740
but this biological life form was improving radically every year.

71
00:04:10.741 --> 00:04:14.250
Like it didn't even exist. Like the iPhone existed in 2007,

72
00:04:14.251 --> 00:04:17.880
that's when it was invented. If we had something that was 12 years old,

73
00:04:18.300 --> 00:04:23.100
but all of a sudden was infinitely faster and better and smarter and wiser than

74
00:04:23.101 --> 00:04:27.750
it was 12 years ago. The newest version of it, version X one, we would,

75
00:04:27.751 --> 00:04:31.050
we would start going, whoa, we'll pay, hit the brakes on this thing, man. How,

76
00:04:31.230 --> 00:04:34.740
how many more generations before this thing's way smarter than us.

77
00:04:34.980 --> 00:04:37.770
How many more generations before this thing thinks that human beings are

78
00:04:37.771 --> 00:04:38.604
obsolete?

79
00:04:39.480 --> 00:04:43.770
<v 1>Yeah. It's coffee coming at us faster, but some people think, oh,</v>

80
00:04:43.771 --> 00:04:48.450
it's a slowing down now. Um, who thinks it's slowing down?

81
00:04:48.810 --> 00:04:53.250
Well, don't I have like Tyler Cowen and you, you got an, a, um,

82
00:04:53.280 --> 00:04:58.230
Peter teal sometimes goes on about the pace of innovation and not

83
00:04:58.320 --> 00:05:01.680
really being what it needs to be. Um, I mean,

84
00:05:02.250 --> 00:05:07.200
maybe it was faster in like 1890s or, but,

85
00:05:07.201 --> 00:05:10.740
but still compared to almost all of human history,

86
00:05:10.741 --> 00:05:15.630
it seems like a period of unprecedented rapid progress right now,

87
00:05:15.930 --> 00:05:18.000
unprecedented, I would say so. Yeah. I mean,

88
00:05:18.001 --> 00:05:21.780
except for maybe a couple of decades a hundred years ago when there was a lot of

89
00:05:22.500 --> 00:05:24.510
electricity, the whole thing. Yeah.

90
00:05:25.140 --> 00:05:27.750
<v 0>No, I agree. Um, I just I'm,</v>

91
00:05:29.340 --> 00:05:33.270
I don't think it's a concern because it's more of a curiosity to me. I mean,

92
00:05:33.300 --> 00:05:36.990
I am concerned, but the more I look at it and go, well,

93
00:05:36.991 --> 00:05:41.850
this is C it seems inevitable that we're going to run into artificial

94
00:05:41.851 --> 00:05:45.000
intelligence, but the questions are so open-ended,

95
00:05:45.001 --> 00:05:47.880
we really don't know when we don't really don't know what form it's going to

96
00:05:47.881 --> 00:05:52.020
take and we really don't know what it's going to do to us.

97
00:05:52.560 --> 00:05:56.550
<v 1>Yeah. So I, I see it as not something that, um,</v>

98
00:05:56.900 --> 00:05:58.850
should be avoided, uh,

99
00:05:59.300 --> 00:06:01.640
neither something that we should just be completely gung ho about,

100
00:06:01.641 --> 00:06:05.900
but more like a kind of gate through which we will have to pass at some point

101
00:06:05.930 --> 00:06:09.710
all paths that are both possible and lead to really great futures.

102
00:06:09.740 --> 00:06:14.510
I think at some point involve the development of greater than human intelligence

103
00:06:14.540 --> 00:06:15.373
machine intelligence.

104
00:06:15.740 --> 00:06:19.910
And so that our focus should be on getting our act together as much as we can in

105
00:06:19.970 --> 00:06:24.800
whatever period of time we have before that cars prepare

106
00:06:24.801 --> 00:06:27.590
ourselves. Well, I mean, that might involve doing some,

107
00:06:27.620 --> 00:06:29.720
some research into various technical questions.

108
00:06:29.870 --> 00:06:32.420
That's how you build these systems so that we actually understand what they are

109
00:06:32.421 --> 00:06:36.920
doing and they have some, you know, intended impact on the world.

110
00:06:36.921 --> 00:06:41.130
It might also, if, if we are able to get our act together a little bit on,

111
00:06:41.131 --> 00:06:44.540
on the kind of global political scene, um,

112
00:06:44.600 --> 00:06:46.610
a little bit more peace and love in the world would be.

113
00:06:46.610 --> 00:06:49.100
<v 2>Good. I think it'd be nice. Um.</v>

114
00:06:49.460 --> 00:06:50.920
<v 1>So, um, and,</v>

115
00:06:50.921 --> 00:06:54.380
and then like reframing from destroying ourselves through some other means

116
00:06:54.381 --> 00:06:58.520
before we even get a chance to try to, uh, needle our way through this, uh,

117
00:06:58.970 --> 00:06:59.800
this game.

118
00:06:59.800 --> 00:07:01.090
<v 0>Well, that's certainly possible.</v>

119
00:07:01.230 --> 00:07:04.990
And we were certainly capable of screwing it all up is the current state of

120
00:07:04.991 --> 00:07:08.500
technology now in regards to artificial intelligence.

121
00:07:08.501 --> 00:07:12.340
And how far away do you think we are from AGI?

122
00:07:13.420 --> 00:07:17.050
<v 1>Well, different people have different views on that. I think the, uh,</v>

123
00:07:17.890 --> 00:07:20.950
the truth of the matter is that it's very hard to,

124
00:07:21.190 --> 00:07:24.040
to have accurate views about the timelines for these things that that's,

125
00:07:24.340 --> 00:07:28.990
you know, are still involved, kind of big new breakthroughs that have to happen.

126
00:07:30.130 --> 00:07:33.630
Um, uh, certainly, I mean, over the last eight or 10 years,

127
00:07:33.760 --> 00:07:37.510
there has been a lot of excitement with the deep learning revolution, um,

128
00:07:38.490 --> 00:07:42.310
things that w mean it used to be that people thought of AI as this kind of

129
00:07:42.910 --> 00:07:44.050
autistic savant,

130
00:07:44.051 --> 00:07:47.980
really good at logic and counting and memorizing facts,

131
00:07:48.010 --> 00:07:51.190
but with, with no, no intuition,

132
00:07:51.490 --> 00:07:53.650
and there's this deep learning revolution.

133
00:07:53.651 --> 00:07:55.570
And when you began to do these deep neural networks,

134
00:07:55.640 --> 00:08:00.630
you kind of solved perception in some sense that you

135
00:08:00.820 --> 00:08:04.240
have to have computers that can see that that can hear and that have visual

136
00:08:04.241 --> 00:08:07.660
intuition. Um, so, so, so that,

137
00:08:07.661 --> 00:08:11.650
that doesn't enable the whole wide suite of applications, uh,

138
00:08:11.710 --> 00:08:13.210
which makes it commercially valuable,

139
00:08:13.211 --> 00:08:17.170
which then drives a lot of investment in it, which, you know, there's a,

140
00:08:17.470 --> 00:08:21.470
so there's now quite a lot of momentum, um, uh, in,

141
00:08:21.471 --> 00:08:24.940
in machine learning and, and trying to kind of stay ahead of that. It's.

142
00:08:24.940 --> 00:08:28.900
<v 0>Interesting that when we think about artificial intelligence and whatever</v>

143
00:08:29.020 --> 00:08:33.490
potential form that is going to take, if you look at films like 2001,

144
00:08:33.550 --> 00:08:36.790
like how like open the door, how, you know,

145
00:08:37.030 --> 00:08:40.360
like we think of something that's communicating to us, like,

146
00:08:40.390 --> 00:08:44.560
like a person would, and maybe is a little bit colder and doesn't,

147
00:08:44.620 --> 00:08:49.480
doesn't share our values and has a more pragmatic view of life

148
00:08:49.481 --> 00:08:53.770
and death and things. But when we think of intelligence though,

149
00:08:53.771 --> 00:08:55.150
I think intelligence,

150
00:08:55.230 --> 00:08:59.040
our mind is almost an extra be connected to all the things that make us human,

151
00:08:59.041 --> 00:09:03.540
like emotions and, and, and ambition and all of these things.

152
00:09:03.541 --> 00:09:06.090
Like the reason why we innovate. Like we,

153
00:09:06.150 --> 00:09:10.110
it's not really clear like why we innovate because we enjoy innovation.

154
00:09:10.111 --> 00:09:11.790
And because we want to make the world a better place.

155
00:09:11.791 --> 00:09:15.000
And because we want to fix some problems that we've created and we want to solve

156
00:09:15.001 --> 00:09:19.350
some limitations of the human body and the environment that we live in,

157
00:09:19.950 --> 00:09:24.420
but we sort of assume that intelligence that we create will also have some

158
00:09:24.421 --> 00:09:25.380
motivations.

159
00:09:26.840 --> 00:09:30.530
<v 1>Well, there is a fairly large class of possible</v>

160
00:09:31.580 --> 00:09:32.930
structures you could do.

161
00:09:32.931 --> 00:09:35.870
If you want to do anything that has any kind of cognitive or intellectual

162
00:09:35.871 --> 00:09:39.500
capacity at all, a large class of those would be what we might call agents.

163
00:09:40.160 --> 00:09:44.600
So these would be yeah. Systems that interact with the world, um,

164
00:09:45.080 --> 00:09:47.840
in pursuit of some goal. Um,

165
00:09:48.230 --> 00:09:52.820
and if there are a sophisticated class of agents that they can plan ahead to

166
00:09:52.821 --> 00:09:56.480
sequence of actions, like more primitive agents might just have reflexes.

167
00:09:58.100 --> 00:10:00.770
Um, but, but the sophisticated eight on my tab,

168
00:10:00.800 --> 00:10:04.190
a model of the world where it can kind of think ahead before it starts doing

169
00:10:04.191 --> 00:10:05.570
stuff, it can kind of think what,

170
00:10:05.870 --> 00:10:09.800
what would I need to do in order to reach this desired state and then recent

171
00:10:09.801 --> 00:10:12.440
backwards from that. So I think it's a fairly natural,

172
00:10:12.800 --> 00:10:15.200
it's not the only possible cognitive system you could build,

173
00:10:15.500 --> 00:10:19.160
but it's also not this weird, bizarre, special case that, you know,

174
00:10:19.510 --> 00:10:23.480
it's a fairly natural thing to, to infer. If you are able to specify the goal,

175
00:10:23.510 --> 00:10:26.270
something you want to achieve, but you don't know how to achieve it.

176
00:10:26.360 --> 00:10:29.480
The natural way of trying to go about that is by building the system that has

177
00:10:29.481 --> 00:10:33.200
this goal and is an agent, and then moves around and tries different things.

178
00:10:33.201 --> 00:10:38.180
And eventually perhaps learn to, to solve that task. Do you.

179
00:10:38.570 --> 00:10:42.950
<v 0>Anticipate different types of artificial intelligence,</v>

180
00:10:43.100 --> 00:10:47.870
like artificial intelligence that mimics that the human emotions, like,

181
00:10:47.871 --> 00:10:48.230
do you think,

182
00:10:48.230 --> 00:10:52.670
do you think that people will construct something that's very similar to us in a

183
00:10:52.671 --> 00:10:55.730
way that we can interact with it in common terms?

184
00:10:55.731 --> 00:11:00.530
Or do you think it will be almost like communicating with an alien?

185
00:11:02.930 --> 00:11:04.850
<v 1>So there are different scenarios here. I mean,</v>

186
00:11:04.851 --> 00:11:09.800
I guess my guess is that the first thing that actually achieved

187
00:11:09.801 --> 00:11:13.520
superintendents would not be very human. Like, um,

188
00:11:14.630 --> 00:11:18.710
there are different possible ways you could try to get to this level of

189
00:11:18.711 --> 00:11:21.950
technology. One would be by trying to reverse engineer the human brain,

190
00:11:22.070 --> 00:11:23.630
but we haven't existed in, in,

191
00:11:23.810 --> 00:11:27.830
in the limit limiting case you might met if you just made an exact duplicate in,

192
00:11:27.831 --> 00:11:31.310
in Silicon of the human brain, like every neuron had some counterpart.

193
00:11:32.870 --> 00:11:36.320
Um, so that, that, that seems technologically very difficult to do,

194
00:11:36.321 --> 00:11:39.680
but it wouldn't require an, a big theoretical breakthrough to do it.

195
00:11:39.681 --> 00:11:40.401
You could just through,

196
00:11:40.401 --> 00:11:44.540
if you had sufficiently good microscopy and large enough computers and in us

197
00:11:45.260 --> 00:11:46.640
elbow grease, you could kind of,

198
00:11:46.670 --> 00:11:51.230
but it seems to me plausible that what will work before we are able to do it

199
00:11:51.231 --> 00:11:54.820
that way will be some more synthetic where, um,

200
00:11:55.090 --> 00:12:00.060
that would only be a very rough resemblance, uh, maybe with the neocortex. Yeah.

201
00:12:00.061 --> 00:12:00.390
That's.

202
00:12:00.390 --> 00:12:01.860
<v 0>One of the big questions, right?</v>

203
00:12:01.861 --> 00:12:06.210
Whether or not we can replicate all the functions of the human brain in the way

204
00:12:06.240 --> 00:12:09.150
it functions in, in like mimic it exactly.

205
00:12:09.360 --> 00:12:12.570
Or whether we could have some sort of superior method that achieves the same

206
00:12:12.571 --> 00:12:15.930
results that the human brain does in terms of its ability to calculate and

207
00:12:15.931 --> 00:12:19.080
reason and, and do multiple tasks at the same time.

208
00:12:19.170 --> 00:12:19.770
<v 1>Yeah.</v>

209
00:12:19.770 --> 00:12:24.270
And I also think that maybe once you have a sufficiently high level of this

210
00:12:24.540 --> 00:12:26.420
general form of intellidose,

211
00:12:26.790 --> 00:12:31.620
then you could use that maybe to emulate or mimic things that we

212
00:12:31.621 --> 00:12:36.300
do differently. So maybe we, our cortex is quite limited.

213
00:12:36.301 --> 00:12:40.950
So we rely a lot on earlier neurological structures that we have, we have to,

214
00:12:41.010 --> 00:12:44.370
we have to be guided by emotion because we can't just calculate everything out.

215
00:12:45.570 --> 00:12:48.620
Um, and, and instinct and [inaudible]. And if we lost all of that, we would

216
00:12:48.620 --> 00:12:48.620
 be helpless, but maybe some system that had a sufficiently high level of this more abstract reasoning capable that they could maybe use that to substitute for things that weren't built in, in the same way that we do. Have you ever talked to Sam Harris about this? Yeah. 

217
00:12:48.620 --> 00:12:48.620
A little bit. Have you ever had a podcast with him? I

218
00:12:48.620 --> 00:12:49.453
, yeah, actually he had him on his podcast a half year ago. I'll have.

219
00:13:10.890 --> 00:13:11.160
<v 0>To go,</v>

220
00:13:11.160 --> 00:13:16.060
I'll have to listen to it because he has the worst view of the

221
00:13:16.710 --> 00:13:20.040
future in terms of artificial intelligence. He's terrified of it.

222
00:13:20.340 --> 00:13:24.540
And when I talked to him, he terrifies me and Elon Musk is right up there.

223
00:13:24.541 --> 00:13:28.710
He also has a terrifying view of what our artificial intelligence could

224
00:13:28.711 --> 00:13:31.500
potentially be. What do you say to those guys?

225
00:13:32.850 --> 00:13:33.683
<v 1>Well, I mean,</v>

226
00:13:33.690 --> 00:13:38.100
I do think that that are these significant risks that will be

227
00:13:38.101 --> 00:13:42.900
associated with this transition to the machine intelligence era, um,

228
00:13:43.200 --> 00:13:44.640
including existential risks,

229
00:13:44.641 --> 00:13:48.390
threats to the very survival of humanity or what we care about,

230
00:13:48.420 --> 00:13:51.780
why are we doing this? Well, there are a lot of things we are,

231
00:13:52.140 --> 00:13:55.900
we're doing that may be globally. It would be better if we didn't do. I mean,

232
00:13:56.100 --> 00:13:58.800
why, why do we build thousands of nuclear weapons? Right.

233
00:13:59.080 --> 00:14:03.750
Why do we overfish the ocean? Yeah. So, um, now if I'd actually ask,

234
00:14:03.751 --> 00:14:07.680
why do different individuals work on AI research or why to different

235
00:14:08.970 --> 00:14:11.220
companies and governments funded? I mean, there are a lot of explanation.

236
00:14:11.221 --> 00:14:14.190
It's like a great scientific, uh, endeavor.

237
00:14:14.370 --> 00:14:17.040
If you can make the Google search engine 1% better,

238
00:14:17.041 --> 00:14:20.790
that's gotta be worth like a billion dollars right off the bat. Um,

239
00:14:20.880 --> 00:14:23.040
it's become a kind of prestige thing now where,

240
00:14:23.041 --> 00:14:26.760
where nations want to have some sort of strategy because it's seen as this new

241
00:14:26.970 --> 00:14:31.950
frontier, um, just like when you had, um, you know,

242
00:14:32.220 --> 00:14:35.880
steam engines and industrialization a few hundred years ago,

243
00:14:35.881 --> 00:14:36.870
and then like Tricity,

244
00:14:36.871 --> 00:14:40.560
like it's going to just open up a lot of economic opportunities you want to be

245
00:14:40.561 --> 00:14:43.260
in there where it's you, do you want to be this kind of,

246
00:14:43.710 --> 00:14:46.920
we were going to do subsistence agriculture while the rest of the world is

247
00:14:46.921 --> 00:14:50.220
moving on. Um, so, so there's, there's a lot of,

248
00:14:50.250 --> 00:14:54.020
it's kind of like you could remove some of these reasons and that would still be

249
00:14:54.021 --> 00:14:56.780
not reasons for why people would be pushing forward with this.

250
00:14:57.880 --> 00:15:01.240
<v 0>One of the things that scares me the most is the idea that if we do create</v>

251
00:15:01.241 --> 00:15:02.260
artificial intelligence,

252
00:15:02.261 --> 00:15:06.880
then it will improve upon our design and create far more sophisticated versions

253
00:15:06.881 --> 00:15:07.714
of itself.

254
00:15:07.980 --> 00:15:12.430
And then it'll continue to do that until it's unrecognizable until it reaches

255
00:15:12.490 --> 00:15:16.630
literally a God-like potential that sup I mean,

256
00:15:16.631 --> 00:15:18.670
I forget what the real numbers were. Maybe you could tell us,

257
00:15:19.030 --> 00:15:23.620
but someone had calculated some reputable source had calculated the

258
00:15:23.621 --> 00:15:28.540
amount of improvement that sentient artificial intelligence would be able to

259
00:15:28.541 --> 00:15:30.760
create inside of a small window of time.

260
00:15:31.120 --> 00:15:34.900
Like if it was allowed to innovate and then make better versions of itself.

261
00:15:34.901 --> 00:15:37.570
And those better versions of itself were allowed to innovate and make better

262
00:15:37.571 --> 00:15:38.500
versions of itself.

263
00:15:38.830 --> 00:15:41.260
You're talking about not an exponential increase of intelligence,

264
00:15:41.261 --> 00:15:42.100
but an explosion.

265
00:15:43.330 --> 00:15:45.370
<v 1>W w w we don't know. So</v>

266
00:15:47.060 --> 00:15:51.700
it's hard enough to forecast the pace at which we will make

267
00:15:51.730 --> 00:15:52.630
advances in AI,

268
00:15:53.050 --> 00:15:55.960
because we just don't know how hard the problems are that we haven't yet solved.

269
00:15:55.990 --> 00:16:00.940
Right. And, you know, once you get to human level or a little bit above, I mean,

270
00:16:00.941 --> 00:16:04.870
who knows it could be that there is some level of where to get further.

271
00:16:05.060 --> 00:16:09.670
You need to put in a lot of thinking time to kind of get there. Now,

272
00:16:09.790 --> 00:16:14.320
what is easier to estimate is if you just look at the speed,

273
00:16:14.590 --> 00:16:17.350
because that's just a function of the hardware that you're running it on. Right?

274
00:16:17.890 --> 00:16:21.880
So, so there, we know that there is a lot of room in principle.

275
00:16:21.881 --> 00:16:25.570
If you look at the physics of computation and you look at what would an

276
00:16:26.050 --> 00:16:29.410
optimally arranged physical system be, and that was optimized for computation,

277
00:16:29.560 --> 00:16:34.150
that it would be like way many, many orders above what we can do now. Um,

278
00:16:34.180 --> 00:16:38.830
and that then you could have arbitrarily large systems like that. So, um, from,

279
00:16:38.831 --> 00:16:39.640
from that point of view,

280
00:16:39.640 --> 00:16:42.760
we know that that could be things that wouldn't be like a million times faster

281
00:16:43.390 --> 00:16:46.930
than the human brain and, and with a lot more memory and stuff like that. D.

282
00:16:46.950 --> 00:16:48.070
<v 0>And then something,</v>

283
00:16:48.130 --> 00:16:51.280
if it did have a million times more power than the human brain,

284
00:16:51.310 --> 00:16:54.610
it could create something with a million times more compute,

285
00:16:55.300 --> 00:17:00.130
computational power than itself. It could make better versions.

286
00:17:00.160 --> 00:17:04.540
It could continue to innovate. Like if we create something that we,

287
00:17:04.620 --> 00:17:08.650
and we say you are mean it is sentient,

288
00:17:08.710 --> 00:17:12.580
it is artificial intelligence. Now please go innovate,

289
00:17:12.940 --> 00:17:16.540
please go follow the same directive and improve upon your design.

290
00:17:16.900 --> 00:17:18.670
<v 1>Yeah. Well, w we don't know how,</v>

291
00:17:19.120 --> 00:17:21.820
how long that would take then to get to something. I mean,

292
00:17:21.821 --> 00:17:26.440
we already have sort of millions of times more thinking capacity

293
00:17:26.441 --> 00:17:30.250
than a human has. I mean, we have millions of humans, right. Um,

294
00:17:30.460 --> 00:17:32.980
so if you kind of break it down,

295
00:17:32.990 --> 00:17:35.830
you think there's like one milestone when you have maybe an AI that could do

296
00:17:35.831 --> 00:17:36.700
what one human can do,

297
00:17:36.701 --> 00:17:40.870
but then that might still be quite a lot of orders of magnitude, uh, you know,

298
00:17:40.871 --> 00:17:45.130
until it would be equivalent of the whole human species. Um,

299
00:17:45.180 --> 00:17:49.890
and maybe during that time, other things happen, maybe we upgrade, you know,

300
00:17:50.320 --> 00:17:52.120
our own abilities in some way. So there,

301
00:17:52.121 --> 00:17:54.420
there are some scenarios where it's so hard to get,

302
00:17:54.421 --> 00:17:58.740
even to one human baseline that we kind of use this

303
00:17:59.040 --> 00:18:03.810
massive amount of resources just to barely create kind of, you know,

304
00:18:04.050 --> 00:18:07.740
villages, uh, using billions of dollars of compute. Right.

305
00:18:07.950 --> 00:18:10.230
So if that's the way we get there, then, I mean,

306
00:18:10.231 --> 00:18:14.250
it might take quite a while because you can't easily scale something that you've

307
00:18:14.251 --> 00:18:16.080
already spent billions of dollars building.

308
00:18:16.310 --> 00:18:16.491
<v 0>Yeah.</v>

309
00:18:16.491 --> 00:18:19.670
Some people think the whole thing is blown out of proportion that we're so far

310
00:18:19.671 --> 00:18:23.450
away from creating artificial general intelligence that resembles human beings,

311
00:18:23.720 --> 00:18:27.350
that it's all just vaporware. What do you say to those people?

312
00:18:28.160 --> 00:18:29.720
<v 1>Uh, well, I mean, one,</v>

313
00:18:29.721 --> 00:18:33.920
one would be that I would want to be more precise about just how far away does

314
00:18:33.921 --> 00:18:38.660
it have to be in order for us to be rational, to ignore it.

315
00:18:38.930 --> 00:18:43.700
It might be that if something is sufficiently important and high stakes than

316
00:18:43.730 --> 00:18:46.820
even if it's not going to happen in the next 5, 10, 20, 30 years,

317
00:18:46.821 --> 00:18:49.760
it might still be wise for, you know,

318
00:18:49.761 --> 00:18:54.560
our pool of 7 billion plus people to have some people actually thinking

319
00:18:54.561 --> 00:18:58.700
about this ahead of time. So some of these disagreements,

320
00:18:58.730 --> 00:19:01.520
I guess this is my point are, are more apparent than real.

321
00:19:01.550 --> 00:19:04.490
Like there's some people say it's going to happen soon on some other people say,

322
00:19:04.550 --> 00:19:06.590
no, it's not going to happen for a long time. And then,

323
00:19:07.730 --> 00:19:10.340
you know what one person means by soon five years.

324
00:19:10.341 --> 00:19:14.480
And another person means by a long time, five years, and, you know,

325
00:19:14.510 --> 00:19:17.710
it's more of different attitudes rather than different specific beliefs. So,

326
00:19:17.711 --> 00:19:22.160
so I would first want to make sure that there actually is a disagreement. Um,

327
00:19:22.790 --> 00:19:23.810
now if there is,

328
00:19:24.140 --> 00:19:26.990
if somebody is very confident that it's not going to happen in hundreds and

329
00:19:26.991 --> 00:19:31.390
hundreds of years, that I guess I would want to know the reasons for,

330
00:19:31.391 --> 00:19:32.990
for that level of confidence, what what's,

331
00:19:33.110 --> 00:19:35.540
what's the evidence they're looking at, uh, you know,

332
00:19:35.720 --> 00:19:38.990
do they have some ground for, for being very sure about this,

333
00:19:39.380 --> 00:19:43.910
that certainly the history of technology prediction is not that great. Um,

334
00:19:43.970 --> 00:19:48.530
you can find a lot of other examples where even very eminent technologists and

335
00:19:48.531 --> 00:19:49.940
scientists were culture,

336
00:19:49.941 --> 00:19:53.990
it's not going to happen in our lifetime or nothing like it. In some cases,

337
00:19:54.110 --> 00:19:56.930
it actually already just happened in some other parts of the world,

338
00:19:57.440 --> 00:20:01.670
or it happened a year or two later. So I think some,

339
00:20:01.700 --> 00:20:06.050
some epistemic humility with these things. So it would be a Y.

