WEBVTT

1
00:00:00.990 --> 00:00:02.730
<v 0>The Joe Rogan experience.</v>

2
00:00:02.760 --> 00:00:06.920
Fundamentally talk about what you can talk about to what Neurolink is.

3
00:00:07.130 --> 00:00:09.530
Cause the last time you were here, I really couldn't discuss it.

4
00:00:09.860 --> 00:00:11.780
And then there was a, I guess,

5
00:00:11.781 --> 00:00:15.220
a press release or something that sort of outlined that, that,

6
00:00:15.280 --> 00:00:17.510
that happened quite a bit after the last time you were here.

7
00:00:17.720 --> 00:00:21.170
So what exactly is it, how do you do, what,

8
00:00:21.470 --> 00:00:26.210
what happens if someone ultimately does get a neuro link installed?

9
00:00:26.211 --> 00:00:30.650
What will take place? Well, for version one of the device,

10
00:00:30.651 --> 00:00:32.120
it would be, um,

11
00:00:33.350 --> 00:00:37.370
it basically implanted in your skull. So, uh,

12
00:00:38.510 --> 00:00:42.170
but it would be flush with your skull. So you basically,

13
00:00:44.030 --> 00:00:48.860
uh, take out a chunk of skull, replace, put the neurotic device in there. Um,

14
00:00:49.730 --> 00:00:51.590
your, you put the electrode,

15
00:00:51.650 --> 00:00:56.630
you'd insert the electrode threads very carefully into the brain. Um, and,

16
00:00:57.800 --> 00:01:02.210
uh, and then you, you know, stitch it up and, um,

17
00:01:02.390 --> 00:01:06.560
and you wouldn't even know that somebody has. Um, and then, and, and so then it,

18
00:01:06.920 --> 00:01:11.180
it can interface basically anywhere in, anywhere in your brain. Um,

19
00:01:11.210 --> 00:01:15.110
so it could be something that, uh, you know, helps cure say, uh, eyesight,

20
00:01:15.140 --> 00:01:17.180
like give you returns your eyesight,

21
00:01:17.181 --> 00:01:21.340
even if you're like lush or optic nerve type of thing. Uh, really, yeah,

22
00:01:21.380 --> 00:01:25.400
absolutely hearing them say, um, I mean,

23
00:01:25.430 --> 00:01:28.070
pretty much anything that w where that,

24
00:01:28.071 --> 00:01:32.510
that it could in principle fix almost anything that is wrong with the brain. Um,

25
00:01:32.540 --> 00:01:37.460
and it could, it could, um, restore, uh, limb functionality.

26
00:01:37.461 --> 00:01:38.720
So if you've got, uh,

27
00:01:39.280 --> 00:01:44.000
interface into the motor cortex and then an implant, that's say,

28
00:01:44.720 --> 00:01:49.500
uh, that's like a microcontroller, uh, in near muscle groups, uh, you,

29
00:01:49.501 --> 00:01:50.780
you could then create a,

30
00:01:51.410 --> 00:01:56.150
sort of a neural shunt that restores somebody who is a quadriplegic

31
00:01:56.151 --> 00:02:00.290
to full functionality. Like they can walk around, be normal.

32
00:02:02.030 --> 00:02:07.010
Well, yeah. So maybe slightly better, slightly better over time. Yes.

33
00:02:07.400 --> 00:02:10.490
You mean with future iterations? Like, you know, $6 million man,

34
00:02:10.520 --> 00:02:13.660
although these days that doesn't sound like much. Yeah, yeah.

35
00:02:14.510 --> 00:02:19.250
<v 1>$6 Billion man. Yeah. So the, the whole would be small.</v>

36
00:02:19.270 --> 00:02:22.640
How big would the whole bean that you have to drill and then replace with this

37
00:02:22.641 --> 00:02:24.680
piece? There's only one hole.

38
00:02:25.370 --> 00:02:27.710
<v 0>Well, um, yeah,</v>

39
00:02:27.711 --> 00:02:30.830
the device we're working on right now is about,

40
00:02:31.910 --> 00:02:34.550
it's about an inch in diameter, um, and your,

41
00:02:34.551 --> 00:02:37.820
your skull is pretty thick by the way. So our skulls are for sure.

42
00:02:37.880 --> 00:02:42.110
It might actually literally, um, I mean, if you're a big, if you're a big guy,

43
00:02:42.111 --> 00:02:45.950
your skull is actually fairly thick. Um, skulls,

44
00:02:45.980 --> 00:02:49.400
like it's like seven to 14 millimeters. Um,

45
00:02:49.430 --> 00:02:54.410
so a couple of inches or half inch, half inch thick skull. Uh, so,

46
00:02:55.490 --> 00:02:58.310
um, yeah, so that, that's a fair bit of like our,

47
00:02:58.670 --> 00:03:00.940
our we've got quite a coconut going on there. It's not,

48
00:03:01.000 --> 00:03:05.740
it's not like some eggshell. Oh yeah. I believe you. Um, so the, yeah,

49
00:03:05.890 --> 00:03:08.110
you basically implant the device.

50
00:03:09.090 --> 00:03:13.350
<v 1>Would be like a one inch square, one inch in diameter.</v>

51
00:03:14.070 --> 00:03:15.570
So an inch circle, like a circle. Yeah.

52
00:03:16.140 --> 00:03:20.010
<v 0>Yeah. I think like a, like a smartwatch or something like that earth. Okay.</v>

53
00:03:20.400 --> 00:03:21.233
Yeah.

54
00:03:21.570 --> 00:03:25.800
<v 1>Okay. So you take this one is dominant, like ice fishing, right?</v>

55
00:03:25.890 --> 00:03:29.730
You ever go ice fishing? No, but I'd like to, it's great. Yeah. It's really fun.

56
00:03:29.940 --> 00:03:34.920
So you basically take an auger and you drill through

57
00:03:34.921 --> 00:03:39.150
the surface of the ice and you create a small hole and you can dunk your line in

58
00:03:39.151 --> 00:03:42.000
there. So this is like that you're ice fishing on the top of your skull,

59
00:03:42.210 --> 00:03:43.043
and then you cork it.

60
00:03:44.220 --> 00:03:46.770
<v 0>Yeah. And you replaced that, um,</v>

61
00:03:46.800 --> 00:03:51.600
say one inch diameter piece of skull with the neural link device.

62
00:03:52.380 --> 00:03:56.880
Um, and that has a battery and a, and a Bluetooth and a inductive charger.

63
00:03:57.720 --> 00:04:02.640
Um, and then you, and then you've also got to insert the electrodes. Uh,

64
00:04:02.641 --> 00:04:07.050
so the electrodes are very Kathleen sorted, uh, with our, uh, uh,

65
00:04:07.090 --> 00:04:08.910
a robot that we developed, uh,

66
00:04:09.120 --> 00:04:12.780
that it's know very carefully putting in the electrodes and avoiding, you know,

67
00:04:12.960 --> 00:04:17.280
and any veins or arteries. Uh, so it's, you know, it doesn't create trauma.

68
00:04:17.340 --> 00:04:22.320
<v 1>So this one inch diameter device electrodes be</v>

69
00:04:22.321 --> 00:04:26.160
inserted and they will find their way, like tiny wires, tiny wires anywhere,

70
00:04:26.280 --> 00:04:29.490
and they'll find their way to specific areas of the brain to stimulate.

71
00:04:29.940 --> 00:04:34.200
<v 0>No, you literally put them where it, where they're supposed to go. Oh, okay.</v>

72
00:04:34.380 --> 00:04:38.070
Yeah. You said inside the normal, these wires be, uh, I mean,

73
00:04:38.071 --> 00:04:42.180
they usually go on like, you know, depending on where it is, like

74
00:04:43.800 --> 00:04:45.300
yeah. Two or three millimeters.

75
00:04:46.170 --> 00:04:49.380
<v 1>So they just find the spots. Yeah. Wow.</v>

76
00:04:50.850 --> 00:04:54.900
<v 0>Um, and then, um, yeah, and then you put the device in,</v>

77
00:04:54.901 --> 00:04:58.230
and that, that gets, uh, that, that replaces the,

78
00:04:58.300 --> 00:05:02.110
the little piece of skull I was taken out. Uh, and then you,

79
00:05:02.680 --> 00:05:06.030
you stitch up the hole and, um, they,

80
00:05:06.170 --> 00:05:08.990
you said like a little scar and, well.

81
00:05:08.990 --> 00:05:12.770
<v 1>This would be replaceable or reversible. Like if someone can't take it anymore,</v>

82
00:05:13.040 --> 00:05:15.470
I'm too smart. I can't take it. Yeah. You can totally take it on.

83
00:05:15.890 --> 00:05:20.810
And what is the besides re re restoring limb function and eyesight and

84
00:05:20.811 --> 00:05:22.380
hearing, which are all amazing. Is there,

85
00:05:22.381 --> 00:05:25.580
there any cognitive benefits that you anticipate from something like this?

86
00:05:26.780 --> 00:05:30.590
<v 0>Uh, yeah. I mean, you could, for sure. Um, uh,</v>

87
00:05:31.640 --> 00:05:35.960
I mean, basically it's a generalized,

88
00:05:36.680 --> 00:05:40.820
um, sort of, uh,

89
00:05:42.080 --> 00:05:45.170
thing for, uh, for fixing any kind of brain injury in, in principle. I,

90
00:05:45.330 --> 00:05:48.260
if view or if you've got like, like severe epilepsy or something like that,

91
00:05:48.261 --> 00:05:49.094
it could, it could just,

92
00:05:49.130 --> 00:05:51.890
it gets just sort of stopped the episode epilepsy from occurring.

93
00:05:51.891 --> 00:05:55.140
Like it could detect it in real time and then fire, uh, uh,

94
00:05:55.160 --> 00:05:59.750
counter pulse and stop the epilepsy. Um, if, um,

95
00:06:00.920 --> 00:06:03.710
I mean, there's, there's a whole range of brain injuries. Like people,

96
00:06:03.711 --> 00:06:08.470
somebody gets a stroke, they could lose the ability to speak. Uh, th that, that,

97
00:06:08.920 --> 00:06:13.400
that could also be, so she'd get like stroke damage or you lose say,

98
00:06:14.150 --> 00:06:16.640
you know, muscle control over part of your face or something like that.

99
00:06:17.150 --> 00:06:20.930
I think E and then when, when you, when you get older, you tend to, um,

100
00:06:21.200 --> 00:06:24.080
if you've got like, you know, uh Alzheimer's or something like that,

101
00:06:24.470 --> 00:06:28.250
then you lose memory and this could help you with, you know,

102
00:06:28.251 --> 00:06:29.780
restoring your memory, that kind of thing.

103
00:06:30.550 --> 00:06:34.000
<v 1>We're storing memory and what, what is happening. It's allowing it to do that,</v>

104
00:06:34.001 --> 00:06:38.380
like the wires, these small wires stimulating these areas of the brain.

105
00:06:38.410 --> 00:06:42.910
And then is it that the areas of the brain are there they're losing some sort of

106
00:06:42.911 --> 00:06:46.420
electrical force, like what is, what is happening? Yeah. Yeah. So it's, it's,

107
00:06:46.430 --> 00:06:46.870
it's like.

108
00:06:46.870 --> 00:06:49.150
<v 0>It's like the thing of just like a bunch of circuits,</v>

109
00:06:49.151 --> 00:06:53.500
and there's some like circuits that are broken and we can like,

110
00:06:55.090 --> 00:06:58.600
uh, fix those circuits. It's a substitute for those circuits. Okay.

111
00:06:59.080 --> 00:07:02.320
<v 1>And so a specific frequency we'll go through this. Yeah.</v>

112
00:07:02.321 --> 00:07:06.760
It's just so specific in that we is the process

113
00:07:07.000 --> 00:07:09.940
figuring out how much or how little has to be,

114
00:07:10.420 --> 00:07:13.360
how much these areas of the brain have to be juiced up.

115
00:07:14.350 --> 00:07:18.580
<v 0>Yeah. I mean, there's still a lot of work to do. So when I say, you know,</v>

116
00:07:18.790 --> 00:07:23.260
we've got a shot at probably putting in an, in, in a person and, you know,

117
00:07:23.800 --> 00:07:26.530
within a year, I think that that's that's well, that's exactly what I mean.

118
00:07:26.531 --> 00:07:29.770
I think we've, we have a chance of putting in, put in someone and having them,

119
00:07:30.400 --> 00:07:35.350
having them be healthy and, and restoring some functionality that they've lost.

120
00:07:35.800 --> 00:07:39.730
<v 1>The fear is that eventually you're going to have to cut the whole top of</v>

121
00:07:39.731 --> 00:07:44.290
someone's head off and put a new top with a whole bunch of wires.

122
00:07:44.291 --> 00:07:48.940
If you want to get, you know, the real turbo charged version,

123
00:07:49.960 --> 00:07:54.160
the P 100 D of brain stimulation.

124
00:07:55.300 --> 00:07:57.070
<v 0>Yeah. I mean, ultimately, if you,</v>

125
00:07:57.130 --> 00:08:00.490
if you want to go with full AI symbiosis,

126
00:08:01.090 --> 00:08:03.850
you'll probably want to do something like that.

127
00:08:03.851 --> 00:08:08.560
Symbiosis is a scary word when it comes to AI. It's optional.

128
00:08:11.950 --> 00:08:14.680
<v 1>I would hope so. Yeah. It's just, I mean,</v>

129
00:08:15.160 --> 00:08:19.540
once you enjoy the Dr. Manhattan lifestyle, once you, once you become a,

130
00:08:19.541 --> 00:08:23.410
God seems very, very unlikely.

131
00:08:23.470 --> 00:08:26.110
You're going to want to go back to being stupid again. I mean,

132
00:08:26.140 --> 00:08:30.040
you literally could fundamentally change the way human beings interface with

133
00:08:30.041 --> 00:08:33.940
each other. Yes, yes. You wouldn't need to talk.

134
00:08:37.630 --> 00:08:41.230
I'm so scared of that, but so excited about it at the same time. Is that weird?

135
00:08:42.700 --> 00:08:47.650
<v 0>Yeah. I mean, the, I think this is one of the paths to, um,</v>

136
00:08:50.140 --> 00:08:52.780
you know, like I think like what, what are the four,

137
00:08:53.290 --> 00:08:55.650
like AI is getting better and better. Um,

138
00:08:55.680 --> 00:08:59.430
so now let's assume it's sort of like a, a benign AI scenario.

139
00:08:59.910 --> 00:09:03.750
Uh you're in a benign scenario we're kind of left behind, you know, we're,

140
00:09:03.751 --> 00:09:07.680
we're, we're not, we're not along for the ride. Um, we're just too dumb. Right.

141
00:09:07.920 --> 00:09:12.780
So, so, so how do you go along for the ride? Um, and it's like,

142
00:09:12.781 --> 00:09:17.190
you can't beat them, join them. So, um, and we're,

143
00:09:17.191 --> 00:09:20.550
we're ready. We're already a sidewalk to some degree, right.

144
00:09:20.700 --> 00:09:24.240
Cause you've got your phone, you've got your laptop. Yeah. Good drawer,

145
00:09:24.810 --> 00:09:29.680
you know, electronic devices. Yeah. Um, and, um, I mean,

146
00:09:29.880 --> 00:09:32.820
today, if you, your phone, if you, if you don't bring your phone long,

147
00:09:32.821 --> 00:09:35.040
it's like you have missing limb syndrome. It's like,

148
00:09:35.790 --> 00:09:39.900
it feels like something's really, really missing. So we're ready.

149
00:09:40.440 --> 00:09:45.270
Partly, um, part, you know, partly a cyborg,

150
00:09:45.990 --> 00:09:49.020
um, or an AI symbiote essentially. Um,

151
00:09:50.670 --> 00:09:53.790
it's just that the data rate to the electronics is slow.

152
00:09:54.810 --> 00:09:59.160
So especially output, like you're just going with your thumbs.

153
00:09:59.820 --> 00:10:02.070
I mean, like what, what's your data, right.

154
00:10:02.400 --> 00:10:06.390
Maybe optimistically a hundred bits per second. That's being generous.

155
00:10:07.470 --> 00:10:12.270
Um, and now the computer can communicate it, like, you know, a hundred,

156
00:10:12.300 --> 00:10:17.000
a hundred terabytes. Yeah. Yeah. So, so it certainly,

157
00:10:17.160 --> 00:10:20.040
you know, get gigabits or a trivial at this point.

158
00:10:20.610 --> 00:10:25.530
So this is like, yeah. Basically your computer could do,

159
00:10:25.920 --> 00:10:28.770
uh, do things a million times faster,

160
00:10:29.040 --> 00:10:33.000
or you're at a certain point it's like token,

161
00:10:33.010 --> 00:10:36.540
they asked like token or a tree. Okay. Just boring.

162
00:10:37.500 --> 00:10:42.480
You talk to a tree it's price. It's not very entertaining. Um, so,

163
00:10:44.730 --> 00:10:47.590
um, so if you can, if you can solve the,

164
00:10:47.591 --> 00:10:51.630
the data rate issue and especially output, but input too,

165
00:10:52.080 --> 00:10:56.820
then you can improve the symbiosis that is already occurring

166
00:10:56.880 --> 00:10:58.590
between man and machine.

167
00:10:59.790 --> 00:11:02.370
<v 1>So you, you could improve it in what,</v>

168
00:11:02.520 --> 00:11:05.340
when you said you won't have to talk to each other anymore.

169
00:11:05.700 --> 00:11:07.200
We used to joke around about that.

170
00:11:07.201 --> 00:11:09.660
I I've joked around about that a million times in this podcast,

171
00:11:09.661 --> 00:11:10.920
that one day in the future,

172
00:11:10.921 --> 00:11:14.520
there's going to come a time where you can read each other's minds and well,

173
00:11:14.550 --> 00:11:17.700
you'll be able to interface with each other in some sort of a non-verbal

174
00:11:19.200 --> 00:11:22.620
non-physical way where you will transfer data back and forth to each other

175
00:11:22.860 --> 00:11:24.720
without having to actually use your mouth.

176
00:11:27.230 --> 00:11:30.950
<v 0>Exactly. So when you, like, what happens when you, when, like,</v>

177
00:11:31.160 --> 00:11:33.590
let's say you've got some complex idea that you're trying to convey to somebody

178
00:11:33.591 --> 00:11:38.300
else, and how do you do that? Well, your brain spends a lot of effort,

179
00:11:38.660 --> 00:11:43.400
compressing a complex concept into words.

180
00:11:44.330 --> 00:11:45.950
And there's a, there's a lot of,

181
00:11:46.040 --> 00:11:50.690
a lot of loss information loss that occurs when compressing a complex

182
00:11:50.840 --> 00:11:54.550
concept into words. And then you say those, those words are then interpreted.

183
00:11:54.580 --> 00:11:58.470
Then they're decompressed by the person who is listening. Um, and they,

184
00:11:58.471 --> 00:12:03.100
they will at best get a very incomplete understanding of what you're trying to

185
00:12:03.101 --> 00:12:07.270
convey. It's very difficult to convey a complex concept with precision, um,

186
00:12:07.300 --> 00:12:11.380
because you've got compression decompression,

187
00:12:12.070 --> 00:12:14.920
you may not even have heard all the words correctly.

188
00:12:16.090 --> 00:12:18.820
And so communication is difficult. Yeah.

189
00:12:18.830 --> 00:12:23.460
What we have here is a failure to communicate cool. And Luke. Yes.

190
00:12:23.610 --> 00:12:27.570
<v 1>And there's great movie. Yeah. There's an interpretation factor too.</v>

191
00:12:27.571 --> 00:12:31.830
Like you can choose to interpret certain

192
00:12:32.190 --> 00:12:36.780
series of words in different ways and they're dependent upon tone

193
00:12:37.470 --> 00:12:42.030
dependent upon social cues, even facial expressions, sarcasm.

194
00:12:42.060 --> 00:12:45.660
There's a lot of variables. Sarcasm is difficult. Yes. Yeah.

195
00:12:45.900 --> 00:12:47.790
And so when I'm,

196
00:12:47.791 --> 00:12:52.500
one of the things that I've said is like that there could be potentially a

197
00:12:52.501 --> 00:12:57.120
universal language that's created through computers that particularly young

198
00:12:57.150 --> 00:13:01.740
kids would pick up very quickly. Like my kids do Tik TOK and all this jazz,

199
00:13:01.741 --> 00:13:03.870
and I don't know what they're doing. They just know how to do it.

200
00:13:04.050 --> 00:13:05.430
And they know how to do it really quickly.

201
00:13:05.610 --> 00:13:08.610
Like they learn really quickly and they show me how to edit things. And yeah,

202
00:13:08.640 --> 00:13:09.090
it's,

203
00:13:09.090 --> 00:13:14.070
if you taught a child from first grade on how to use some new universal

204
00:13:14.071 --> 00:13:15.120
language, I mean,

205
00:13:15.121 --> 00:13:19.710
essentially like a Rosetta stone and something that's done

206
00:13:20.150 --> 00:13:22.740
with that interprets your thoughts,

207
00:13:22.950 --> 00:13:27.090
and you can convey your thoughts with no room for interpretation,

208
00:13:27.270 --> 00:13:31.830
with clear, very clear where, you know what a person's saying,

209
00:13:32.040 --> 00:13:36.660
and you can tell them what you're saying, and there's no need for noises,

210
00:13:36.720 --> 00:13:38.010
no need for mouth noises,

211
00:13:38.130 --> 00:13:42.660
no need for these sort of accepted ways that we've, uh,

212
00:13:42.690 --> 00:13:47.580
sort of evolved to make sounds that we all agree,

213
00:13:47.790 --> 00:13:51.150
we through our cultural dictionary and we agree,

214
00:13:51.151 --> 00:13:52.830
or we can bypass all that.

215
00:13:53.160 --> 00:13:56.100
<v 0>I mean, you could still do it for sentimental reasons, right?</v>

216
00:13:57.240 --> 00:14:01.890
Like campfires. Yeah, exactly. You don't need campfires. You don't need a roast.

217
00:14:01.891 --> 00:14:05.670
Marshmallows were kinda fun. Um, so yeah. Um,

218
00:14:06.840 --> 00:14:11.460
yeah, I think you would, in principle, you would be able to communicate, uh,

219
00:14:12.990 --> 00:14:17.640
very quickly and with far more precision,

220
00:14:18.480 --> 00:14:22.470
uh, ideas, uh, and, and language would,

221
00:14:23.880 --> 00:14:25.050
I'm not sure what would have in July,

222
00:14:25.051 --> 00:14:27.930
which you could probably in a situation like this,

223
00:14:27.931 --> 00:14:30.060
that you would be able to just, it'd be like, kinda like the matrix. You,

224
00:14:30.061 --> 00:14:32.340
you want to speak a different language. No problem. Right.

225
00:14:32.580 --> 00:14:34.230
That's why I downloaded the program.

226
00:14:36.360 --> 00:14:39.810
<v 1>Right. So at least for the first iterations,</v>

227
00:14:39.840 --> 00:14:44.370
first few iterations would just be able to use, like, I know that Google has,

228
00:14:44.640 --> 00:14:45.600
uh, their, uh,

229
00:14:46.140 --> 00:14:50.970
some of their pixel buds have the ability to interpret languages in real time.

230
00:14:51.950 --> 00:14:53.450
Sure. Yeah. You can hear it. And they'll,

231
00:14:53.550 --> 00:14:56.690
it'll play things back to you in whatever language you choose.

232
00:14:56.870 --> 00:15:00.550
So it'd be something along those lines. Yeah.

233
00:15:00.580 --> 00:15:02.110
For the first few iterations.

234
00:15:03.220 --> 00:15:05.860
<v 0>Well, the first few iterations are what I'm talking about.</v>

235
00:15:05.980 --> 00:15:10.000
It's like in the limit over time, you know, w with a lot of development, um,

236
00:15:10.030 --> 00:15:13.720
the first few iterations, really in the first few versions,

237
00:15:13.840 --> 00:15:17.380
all we're gonna be trying to do is solve it, brain injuries. Um, so,

238
00:15:17.590 --> 00:15:18.760
so it's like, don't, don't,

239
00:15:18.790 --> 00:15:22.360
don't worry that it's not going to sneak up on you because this this'll take a

240
00:15:22.840 --> 00:15:26.920
while. How many years before you don't have to talk,

241
00:15:30.530 --> 00:15:32.000
if the, if the, if the development

242
00:15:34.010 --> 00:15:36.290
continues to accelerate, then

243
00:15:38.210 --> 00:15:42.860
maybe like five years, five to 10 years quick.

244
00:15:43.490 --> 00:15:47.180
<v 1>That's really quick. That's the best case scenario, no talking anymore in.</v>

245
00:15:47.180 --> 00:15:51.080
<v 0>Five years, best case scenario, but, um, tens 10 years, more like it,</v>

246
00:15:52.250 --> 00:15:52.460
I've.

247
00:15:52.460 --> 00:15:56.660
<v 1>Always speculated that aliens could potentially be us in the future.</v>

248
00:15:56.840 --> 00:16:00.140
Because if you look at like the size of their heads and the fact that they have

249
00:16:00.141 --> 00:16:03.080
very little muscle in there, then they don't use their mouth anymore.

250
00:16:03.170 --> 00:16:05.220
They was tiny little, I mean, the archetypal,

251
00:16:05.450 --> 00:16:08.030
an alien that you see in like close encounters of the third kind,

252
00:16:08.360 --> 00:16:12.830
they they're like if you went from like, uh, Australia,

253
00:16:12.831 --> 00:16:17.660
Pythagoras or ancient hominid to us, what's the difference, less hair,

254
00:16:17.810 --> 00:16:19.580
less muscle, bigger head.

255
00:16:20.270 --> 00:16:24.950
And then just keep going thousand million, whatever year or five years,

256
00:16:25.190 --> 00:16:29.360
whatever, whatever happens when neural link goes on online.

257
00:16:29.450 --> 00:16:34.190
And then we slowly start to adapt to this new way

258
00:16:34.460 --> 00:16:39.410
of being where we don't use our muscles anymore. We have this gigantic head.

259
00:16:39.620 --> 00:16:41.180
We can talk without words.

260
00:16:42.330 --> 00:16:45.050
<v 0>You could also save state,</v>

261
00:16:47.330 --> 00:16:50.300
save state, save state, like save your brain state, like,

262
00:16:50.360 --> 00:16:52.640
like a saved game in a video game. Whoa.

263
00:16:53.210 --> 00:16:58.040
<v 1>Like, like if you want to swap from windows 95. Well, yeah.</v>

264
00:16:58.700 --> 00:17:00.800
<v 0>Hopefully a little bit of fun in that, but yeah.</v>

265
00:17:01.160 --> 00:17:04.460
<v 1>We are windows 95 right now. Mommy.</v>

266
00:17:04.760 --> 00:17:09.170
<v 0>Fisher perspective, probably. Um, but yeah, I mean, you, you could, uh,</v>

267
00:17:09.350 --> 00:17:14.210
save state, um, and restore that state into a biological being,

268
00:17:14.211 --> 00:17:16.910
if you, if you wanted to in the future in principle, it's like nothing,

269
00:17:16.970 --> 00:17:19.210
like from a physics standpoint that prevents us at you,

270
00:17:19.211 --> 00:17:20.150
you'd be a little different,

271
00:17:20.151 --> 00:17:22.550
but then you're also a little different when you wake up in the morning from

272
00:17:22.551 --> 00:17:24.650
yesterday and you were a little different. In fact,

273
00:17:24.680 --> 00:17:28.760
if you say like you five years ago versus you today is quite a big difference.

274
00:17:29.720 --> 00:17:32.840
Um, so you'll be substantially you, I mean, you'd be,

275
00:17:32.870 --> 00:17:34.520
you'd suddenly think you're you, but.

276
00:17:34.520 --> 00:17:39.110
<v 1>The idea of saving yourself and then transforming</v>

277
00:17:39.140 --> 00:17:41.420
that into some sort of a biological state,

278
00:17:41.720 --> 00:17:43.730
like you could hang out with 30 year old, you.

279
00:17:45.950 --> 00:17:48.620
<v 0>I mean, the possibilities are endless. Um,</v>

280
00:17:50.220 --> 00:17:53.400
it's so weird. I mean, just think like how your phone can,

281
00:17:53.430 --> 00:17:55.740
you can record videos on your phone. Like,

282
00:17:55.741 --> 00:18:00.060
there's no way you could remember a video as accurately as your phone or a

283
00:18:00.061 --> 00:18:04.870
camera, you know, could, so, uh, now if you, if you've got like,

284
00:18:05.130 --> 00:18:09.040
you know, some, some version 10, your link, whatever,

285
00:18:09.120 --> 00:18:11.160
and far in the future, you,

286
00:18:11.161 --> 00:18:15.570
you could re you could re you could recall everything, but just like,

287
00:18:15.571 --> 00:18:20.460
it's a movie clip. It concluding all the entire sensory experience, emotions,

288
00:18:20.520 --> 00:18:22.530
everything, everything, everything,

289
00:18:23.400 --> 00:18:27.650
and play it back and edit it, edit it.

290
00:18:27.800 --> 00:18:29.330
<v 1>Yeah. So you can change your past.</v>

291
00:18:30.500 --> 00:18:32.930
<v 0>You change. What do you think was your past? Yeah. Well,</v>

292
00:18:33.080 --> 00:18:37.760
so if you had like this whole thing right now could be a replayed memory.

293
00:18:38.240 --> 00:18:38.960
It could be.

294
00:18:38.960 --> 00:18:43.220
<v 1>Yeah. It may be what's the odds of this being a replayed memory.</v>

295
00:18:43.730 --> 00:18:46.250
If you had to guess it's more than 50%.

296
00:18:48.290 --> 00:18:51.980
<v 0>There's no way to assign a probability with accuracy here. Right. But roughly.</v>

297
00:18:55.120 --> 00:18:57.550
<v 1>If you had just had a, just gut instinct.</v>

298
00:19:00.000 --> 00:19:03.420
<v 0>Well, I don't have a neuro link in my brain. So I'd say right now, 0%.</v>

299
00:19:06.340 --> 00:19:10.180
But at the point at which you do have in your link, then it rises above 0%.

300
00:19:13.440 --> 00:19:18.090
<v 1>The idea that we're experiencing some sort of a preserved memory is,</v>

301
00:19:18.520 --> 00:19:22.110
uh, even though it's still the same, it's not comforting,

302
00:19:23.250 --> 00:19:27.390
right? For some reason, when, when people talk about simulation theory,

303
00:19:27.720 --> 00:19:31.140
they talk about the potential for this currently being a simulation. It,

304
00:19:31.170 --> 00:19:35.250
even though your life might be wonderful, you might be in love.

305
00:19:35.400 --> 00:19:38.340
You might love your career. You might have great friends,

306
00:19:39.120 --> 00:19:43.110
but it's not comforting to know that this experience somehow or another doesn't

307
00:19:43.111 --> 00:19:48.030
exist in a material form that you can knock on. It feels real. It feels real,

308
00:19:48.031 --> 00:19:49.020
but, but if it's,

309
00:19:49.320 --> 00:19:53.730
but the idea that it's not as safe for some strange reason,

310
00:19:53.910 --> 00:19:55.210
disconcerting, well.

311
00:19:55.540 --> 00:20:00.080
<v 0>Yeah, I'm sure it should be disconcerting that cause then if this is not real,</v>

312
00:20:00.081 --> 00:20:04.640
what is, um, but, but the, you know, there's that old sort of,

313
00:20:05.210 --> 00:20:09.440
um, thought experiment of like, how do you know you're not a brain and event?

314
00:20:10.400 --> 00:20:13.790
You know? I mean, now he has the thing, you are a brain in event,

315
00:20:13.850 --> 00:20:17.720
then that faddish of skull. Yes. And everything you see, feel,

316
00:20:17.721 --> 00:20:22.520
hear everything, all your senses are, are electrical signals, everything,

317
00:20:23.060 --> 00:20:23.930
everything

318
00:20:26.990 --> 00:20:28.280
is an electrical signal to a,

319
00:20:28.400 --> 00:20:31.050
to a rain and Yvette where the package was called and.

320
00:20:31.080 --> 00:20:35.400
<v 1>All your hormones, all your neuro-transmitters, all these things are drugs.</v>

321
00:20:35.760 --> 00:20:40.050
Adrenaline's a drug. Dopamine is a drug, you're a drug factory.

322
00:20:40.890 --> 00:20:45.840
You're constantly changing your state with love and oxytocin and beauty

323
00:20:46.380 --> 00:20:49.810
sure. Changes your state. Great music changes your state. Absolutely.

324
00:20:52.080 --> 00:20:56.820
<v 0>And yet here's another sort of interesting idea, which is, um,</v>

325
00:20:57.240 --> 00:21:00.330
cause you say like what, where did consciousness arise? Well,

326
00:21:00.331 --> 00:21:04.170
assuming you believe the belief in physics,

327
00:21:04.171 --> 00:21:07.710
which appears to be true. Um, then you know, we,

328
00:21:08.040 --> 00:21:12.600
the universe started off as basically quarks and leptons and quickly became

329
00:21:12.601 --> 00:21:16.050
hydrogen and helium, lithium,

330
00:21:16.080 --> 00:21:18.300
like basically elements of the periodic table. Uh,

331
00:21:18.440 --> 00:21:22.530
but like mostly hydrogen basically. Um, and then,

332
00:21:23.520 --> 00:21:27.510
and then over a long period of time, uh, you know,

333
00:21:27.840 --> 00:21:31.770
13.8 billion years later, that hydrogen became Santi ant,

334
00:21:33.770 --> 00:21:38.660
w w where along the way did conduit,

335
00:21:39.110 --> 00:21:40.040
where does consciousness?

336
00:21:40.041 --> 00:21:43.670
What's the line of consciousness and not consciousness between hydrogen and

337
00:21:43.671 --> 00:21:44.680
here. Right?

338
00:21:44.770 --> 00:21:47.530
<v 1>When do we call it? When do we call it consciousness?</v>

339
00:21:47.830 --> 00:21:51.160
I was watching a video today that we played on a podcast earlier of a monkey

340
00:21:51.460 --> 00:21:53.230
riding a motorcycle down the street,

341
00:21:53.440 --> 00:21:55.300
jumps off the motorcycle and tries to steal.

342
00:21:55.300 --> 00:21:57.280
<v 0>A baby. Yeah. So that one Pharaoh.</v>

343
00:21:57.730 --> 00:22:02.470
<v 1>What is that monkey conscious? It seems like it is, it seems like it had a plan.</v>

344
00:22:02.471 --> 00:22:04.150
It was riding a motorcycle.

345
00:22:04.360 --> 00:22:08.620
And the jumped off the motorcycle to try to steal a baby. Seems pretty. Yeah.

346
00:22:08.860 --> 00:22:11.800
<v 0>The one that just dragged baby down the street pretty far. Yeah. Yeah.</v>

347
00:22:12.420 --> 00:22:14.980
It seems pretty conscious, right?

348
00:22:16.120 --> 00:22:18.550
There's definitely some degree of consciousness there.

349
00:22:19.120 --> 00:22:23.170
<v 1>Yeah. It's not like it's not a worm. It seems to be on another level.</v>

350
00:22:23.800 --> 00:22:25.150
Yeah. And it's going to keep going.

351
00:22:25.151 --> 00:22:28.810
And that's the real concern.

352
00:22:29.770 --> 00:22:34.360
When, when people think about the potential future versions of human beings,

353
00:22:34.361 --> 00:22:37.900
especially when you consider a symbiotic relationship to artificial

354
00:22:37.901 --> 00:22:38.500
intelligence,

355
00:22:38.500 --> 00:22:43.330
it will be unrecognizable that one day will be so far removed from what this is.

356
00:22:43.331 --> 00:22:47.620
We'll look back on this, the way we look back now on, you know,

357
00:22:48.070 --> 00:22:51.130
simple, simple organisms that we evolved from.

358
00:22:52.300 --> 00:22:55.690
And then it won't be that far in the future that we do have this,

359
00:22:56.010 --> 00:22:56.890
this view back.

360
00:22:58.120 --> 00:23:00.640
<v 0>Well, I hope consciousness propagates into the future and it gets more,</v>

361
00:23:00.940 --> 00:23:02.590
more sophisticated and complex. And,

362
00:23:03.430 --> 00:23:07.810
and that it understands the questions to ask about the universe.

363
00:23:08.710 --> 00:23:12.130
<v 1>Do you think that's the case as he, as a human being, as yourself,</v>

364
00:23:12.160 --> 00:23:16.900
you're clearly trying to make conscious decisions to be a better

365
00:23:16.901 --> 00:23:18.190
version of you, right?

366
00:23:18.191 --> 00:23:20.980
This is the idea of like getting rid of your possessions and realizing that

367
00:23:21.460 --> 00:23:25.450
you're trying to, like, I don't like this. I will try to improve this.

368
00:23:25.810 --> 00:23:30.580
I would try to do a better version of the way I interface with reality that this

369
00:23:30.581 --> 00:23:34.740
is always the way things are. If you're, if you're moving in, uh,

370
00:23:35.140 --> 00:23:37.780
some sort of a direction where you're trying to improve things,

371
00:23:37.990 --> 00:23:42.040
you're always going to move into this new place where you look back in the old

372
00:23:42.041 --> 00:23:43.780
place and go, I was doing it wrong back then.

373
00:23:45.740 --> 00:23:47.750
So this is an accelerated version of that.

374
00:23:48.230 --> 00:23:49.760
It's a super accelerated version of that.

375
00:23:51.580 --> 00:23:53.500
<v 0>I mean, you don't always improve,</v>

376
00:23:53.530 --> 00:23:57.640
but you can aspire to improve or you can aspire to be less wrong. Yeah.

377
00:23:58.270 --> 00:24:02.260
This is like, I think a good, the tools of physics are very powerful.

378
00:24:02.261 --> 00:24:05.080
Like just assume you're wrong. And you're asking your goals to be less wrong.

379
00:24:06.370 --> 00:24:09.280
I don't think you're going to succeed everyday in being less wrong,

380
00:24:09.281 --> 00:24:12.430
but you know, if you're going to succeed in being less wrong,

381
00:24:12.970 --> 00:24:14.410
most of the time you're doing great.

382
00:24:15.220 --> 00:24:19.720
<v 1>It's a great way of putting it aspire to be less wrong. But then when you know,</v>

383
00:24:19.721 --> 00:24:23.080
people look back and nostalgia about simpler times, there's that too.

384
00:24:23.081 --> 00:24:27.790
It's very romantic and exciting to look back on campfires.

385
00:24:28.750 --> 00:24:30.490
<v 0>But you can still have a campfire. Yes. Yeah.</v>

386
00:24:31.150 --> 00:24:32.620
<v 1>We appreciate it when you're a super nerd,</v>

387
00:24:32.800 --> 00:24:34.990
when you connect it to the grid and you have some,

388
00:24:34.991 --> 00:24:38.530
a skullcap in place of the top of your head,

389
00:24:38.740 --> 00:24:43.270
it's interfacing with the inter international language at the rest of the

390
00:24:43.271 --> 00:24:46.480
universe now enjoys communication with people.

391
00:24:48.010 --> 00:24:51.730
<v 0>Yeah, sure. I, I think so. Yeah. I like that part.</v>

392
00:25:01.210 --> 00:25:01.210
<v 2>[Inaudible].</v>

