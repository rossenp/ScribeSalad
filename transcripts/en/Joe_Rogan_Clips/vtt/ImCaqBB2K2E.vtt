WEBVTT

1
00:00:01.020 --> 00:00:05.220
<v 0>The Joe Rogan experience. I think there's so much of this, uh,</v>

2
00:00:05.221 --> 00:00:08.610
the YouTube political world, the YouTube commentary world,

3
00:00:08.611 --> 00:00:13.440
where people are so toxic, you know, there's, there's so much negativity,

4
00:00:13.470 --> 00:00:15.960
there's so much what they call dunking on people.

5
00:00:16.070 --> 00:00:19.080
And there's so much dunking and you do a little Dunkin. Some of it's warranted.

6
00:00:19.110 --> 00:00:21.900
It is warranted. Yes. But I don't know if it's beneficial.

7
00:00:22.820 --> 00:00:25.640
<v 1>Uh, to the, to the people doing the dunking. Yes. Or.</v>

8
00:00:25.640 --> 00:00:29.690
<v 0>Even to the cause I think it is temporarily. Well,</v>

9
00:00:29.720 --> 00:00:31.400
sometimes it's good because it, it,

10
00:00:31.410 --> 00:00:34.790
it shows it mocks people's positions and it makes people realize, Hey,

11
00:00:34.791 --> 00:00:37.710
that is a ridiculous position. So if you're on the fence of,

12
00:00:37.730 --> 00:00:40.640
you're not really quite sure how you feel about things and you see someone get

13
00:00:40.641 --> 00:00:44.240
mocked for ridiculous position that maybe you've even shared for a little bit,

14
00:00:44.450 --> 00:00:46.160
right. Maybe, maybe haven't explored it deeply.

15
00:00:46.340 --> 00:00:50.870
And you see someone who has explored it deeply sort of expose all the flaws in

16
00:00:50.871 --> 00:00:54.770
this line of thinking. It's good. But my thing, what I'm,

17
00:00:55.610 --> 00:00:55.991
I,

18
00:00:55.991 --> 00:00:59.360
I interview a lot of people on the right and a lot of people on the left and I

19
00:00:59.361 --> 00:01:03.620
just hate all this conflict that I I'd say.

20
00:01:03.680 --> 00:01:08.120
The unnecessary conflict, I think is when you,

21
00:01:08.121 --> 00:01:12.980
when you watch television today and you see Antifa fighting with

22
00:01:13.190 --> 00:01:16.190
the, you know, Trump supporters and all this,

23
00:01:16.820 --> 00:01:20.360
all this weird conflict, I don't,

24
00:01:20.810 --> 00:01:25.460
I don't necessarily think that most of it is, is necessary.

25
00:01:25.970 --> 00:01:30.140
<v 1>Necessary. Well, I think the devil's in the details. So like, as an example,</v>

26
00:01:30.200 --> 00:01:33.830
if you want to bring together, I don't know people who are on opposite,

27
00:01:34.190 --> 00:01:37.790
opposite, opposite sides of the climate debate, for example. Good luck.

28
00:01:37.791 --> 00:01:39.080
I'm sure. Right. Well,

29
00:01:39.081 --> 00:01:43.790
why is part of that you could argue is if one side just does not

30
00:01:43.791 --> 00:01:46.910
accept science, right? How can you really bring those people together?

31
00:01:46.911 --> 00:01:49.550
It doesn't mean you need physical conflict to resolve it. In fact,

32
00:01:49.551 --> 00:01:53.240
I completely agree with you. The physical conflict is totally counterproductive,

33
00:01:53.540 --> 00:01:56.330
but at a certain point on some issues,

34
00:01:56.420 --> 00:02:01.400
I understand why there's like an intractability to the debate where it

35
00:02:01.401 --> 00:02:05.120
seems completely impossible to move forward, because whichever side you're on,

36
00:02:05.180 --> 00:02:08.660
I would argue that I'm on the right side of these issues and others would

37
00:02:08.661 --> 00:02:13.640
disagree when you're far apart in a way that you can't even agree

38
00:02:13.641 --> 00:02:16.730
as to like what the starting point facts are about the conversation.

39
00:02:16.760 --> 00:02:20.990
How do you even, how do you start? I have some ideas as to how I try to do it,

40
00:02:21.200 --> 00:02:22.610
but it's very tough.

41
00:02:22.940 --> 00:02:23.511
<v 0>It was very tough.</v>

42
00:02:23.511 --> 00:02:28.100
I just don't think dunking on people always like constantly on

43
00:02:28.101 --> 00:02:29.600
people is necessarily the way.

44
00:02:30.140 --> 00:02:30.440
<v 1>Yeah.</v>

45
00:02:30.440 --> 00:02:34.490
And I think it's important to distinguish between just straight up ad hominem,

46
00:02:34.790 --> 00:02:37.670
where someone is wrong and bad, uh,

47
00:02:38.120 --> 00:02:40.550
because I think they're a bad person or they're an idiot or whatever,

48
00:02:41.270 --> 00:02:46.070
to recognizing when somebody is a participant in bad faith in a conversation

49
00:02:46.520 --> 00:02:51.080
to when someone has maybe fallen prey to audience capture

50
00:02:51.260 --> 00:02:56.240
or whatever else might be kind of influencing what, what and how they're doing.

51
00:02:56.300 --> 00:02:58.430
I think that those criticisms are legitimate,

52
00:02:58.431 --> 00:03:02.400
but you gotta stay away from just ad hominem. Yes. Yes.

53
00:03:02.430 --> 00:03:06.080
<v 0>I agree. And I think that it's just so common today. It's, it's, it's,</v>

54
00:03:06.081 --> 00:03:10.290
it's also extremely attractive. The, the YouTube algorithm, uh, you know,

55
00:03:10.350 --> 00:03:15.030
as far as, um, comments go, I mean, it actually kind of encourages it.

56
00:03:15.240 --> 00:03:17.850
And so does Facebook's. So does, you know,

57
00:03:18.720 --> 00:03:22.140
anytime there's a social media platform that is ad dependent,

58
00:03:22.680 --> 00:03:27.600
one of the best ways to get people to engage is to have something

59
00:03:27.601 --> 00:03:30.300
they disagree with so they can get angry. Yeah. Yes.

60
00:03:30.360 --> 00:03:32.370
<v 1>Until it becomes no longer brand safe,</v>

61
00:03:32.430 --> 00:03:35.190
according to whoever's running the platform. Right. I mean,

62
00:03:35.430 --> 00:03:38.010
if you go back to April, 2017,

63
00:03:38.370 --> 00:03:43.350
where I woke up and saw that my YouTube channel made 19 cents the

64
00:03:43.351 --> 00:03:47.220
previous day, and I text Kyle Kalinsky and I say,

65
00:03:47.250 --> 00:03:50.730
I think there's like a glitch. Is it like, it says, I made 19 cents.

66
00:03:50.731 --> 00:03:54.120
And he says, it says I made 35 cents or something like that. Uh,

67
00:03:54.150 --> 00:03:57.690
something's going on. And it was the beginning of like ad pocalypse 1.0,

68
00:03:58.170 --> 00:04:02.760
and that was a rough three week period. And, uh, so, so it's, you know,

69
00:04:03.530 --> 00:04:05.730
encouraged the debate and the battle of ideas,

70
00:04:05.731 --> 00:04:10.260
so to speak and all of this stuff until advertisers get worried and they say,

71
00:04:10.320 --> 00:04:12.000
oh, you know, our ads are showing up on stuff.

72
00:04:12.001 --> 00:04:13.530
That's a little bit touch and go for.

73
00:04:13.530 --> 00:04:17.130
<v 0>Us. It's a weird one to me because, um, I,</v>

74
00:04:17.280 --> 00:04:20.340
YouTube has always been a secondary thought for me,

75
00:04:20.460 --> 00:04:23.850
the first thought was the audio version of the podcast.

76
00:04:24.930 --> 00:04:27.810
And in fact, when we were uploading it to YouTube at first, I was like,

77
00:04:27.811 --> 00:04:29.610
why are we even doing this? I guess, why not?

78
00:04:29.790 --> 00:04:33.270
Some people probably want to watch it. And then somewhere along the line,

79
00:04:33.690 --> 00:04:38.670
it became at least close to as big as the audio version of it.

80
00:04:38.730 --> 00:04:43.680
And then maybe even more significant because one of the things

81
00:04:43.681 --> 00:04:45.960
that the YouTube version has is the comment section,

82
00:04:45.961 --> 00:04:49.340
which is often a dumpster fire, but it,

83
00:04:49.530 --> 00:04:53.850
at least there is some sort of like a community engagement aspect of it that

84
00:04:53.851 --> 00:04:57.600
doesn't really exist in iTunes. Like in iTunes, it's sort of it's in a vacuum.

85
00:04:57.601 --> 00:05:02.550
Right? Sure. Um, but when the ad apocalypse thing happened,

86
00:05:02.580 --> 00:05:04.950
I was like, Hmm, what was going on here? Like, it wasn't,

87
00:05:04.951 --> 00:05:08.370
it wasn't my primary focus. So it wasn't terrifying,

88
00:05:08.550 --> 00:05:12.930
but people that only did YouTube and people that relied on that for their

89
00:05:12.931 --> 00:05:14.520
living. I mean, it was, it's a huge blow.

90
00:05:14.550 --> 00:05:17.370
<v 1>It was huge. And at the time I'm trying to think back,</v>

91
00:05:17.880 --> 00:05:22.740
I think maybe like around 30% of my entire shows revenue was coming from

92
00:05:22.741 --> 00:05:27.420
YouTube at the time. So it was not everything, but it was still significant,

93
00:05:27.421 --> 00:05:30.090
right? I mean, I have staff and overhead and all of that stuff.

94
00:05:30.091 --> 00:05:33.090
So just overnight 30% going away is huge.

95
00:05:33.091 --> 00:05:36.360
And that's why I've tried to move to the model of telling my audience,

96
00:05:36.720 --> 00:05:40.500
you can skip all of this stuff, you know, even, uh, some of these other,

97
00:05:40.590 --> 00:05:43.170
you know, super chats and all of this other stuff.

98
00:05:43.200 --> 00:05:45.330
Like we run a membership program on my website.

99
00:05:45.331 --> 00:05:46.710
I control a hundred percent of it.

100
00:05:46.770 --> 00:05:51.240
So it's not a Patrion deal or we're on Patrion, but it's not big for us.

101
00:05:51.750 --> 00:05:56.130
The way I think about it is as long as, I mean, listen, yeah, there's, you know,

102
00:05:56.490 --> 00:05:59.270
marijuana companies that are having trouble, even processing payments,

103
00:05:59.570 --> 00:06:01.700
but assuming like Stripe and PayPal,

104
00:06:01.701 --> 00:06:05.030
don't say you can't even accept payments anymore. David PackMan,

105
00:06:05.430 --> 00:06:08.930
I control the entire process on my website. So when people pay their six bucks,

106
00:06:09.140 --> 00:06:13.100
all, but 2.9% gets to me. And when at apocalypse happened,

107
00:06:13.280 --> 00:06:15.830
I saw it as a maybe blessing in disguise.

108
00:06:15.831 --> 00:06:17.600
And that I could now explain to the audience,

109
00:06:18.230 --> 00:06:22.670
here's the problem with these algorithms. Here's the problem. When it goes from,

110
00:06:23.210 --> 00:06:27.890
I am fighting white supremacist content to an algorithm can't distinguish

111
00:06:27.950 --> 00:06:32.570
between that and white supremacist conduct. Right. That's bad for me. Right.

112
00:06:32.571 --> 00:06:34.220
When I interviewed Richard Spencer,

113
00:06:35.090 --> 00:06:37.100
I obviously don't agree with Richard Spencer,

114
00:06:38.000 --> 00:06:42.770
but can an algorithm figure out that there's a difference between an interview I

115
00:06:42.771 --> 00:06:46.070
do with Richard Spencer and white nationalist propaganda. I don't know,

116
00:06:46.071 --> 00:06:50.300
but we can kind of get around all of that if you just go directly to me.

117
00:06:50.301 --> 00:06:53.230
And that's why my focus has been growing that those directors, did you.

118
00:06:53.230 --> 00:06:57.700
<v 0>Interview Richard Spencer? Yeah. Did you get for that? Yes. Yeah.</v>

119
00:06:57.970 --> 00:07:01.360
That's a weird one. Right. And you know, um, I'm sure you're aware of that. Uh,

120
00:07:01.870 --> 00:07:06.070
that, what is it called? Data and society that the can,

121
00:07:06.130 --> 00:07:09.370
whether there was a woman who made a bunch of connections,

122
00:07:09.610 --> 00:07:13.990
like Joe Rogan knows David Pac-Man and Joe Rogan also knows Alex Jones,

123
00:07:14.550 --> 00:07:16.480
Alex Johnson must be friends with David Pac-Man. Yeah.

124
00:07:16.750 --> 00:07:20.680
It's like one of those minds, you know, like, and it was really weird.

125
00:07:21.100 --> 00:07:23.110
It's it's, it's like guilt by association.

126
00:07:23.380 --> 00:07:25.420
<v 1>I saw a couple of them, there was like an initial one,</v>

127
00:07:25.421 --> 00:07:28.840
which made me thinking of, then there was a map of like the YouTube sphere,

128
00:07:28.841 --> 00:07:31.480
specifically the left middle and right. Yeah.

129
00:07:31.480 --> 00:07:32.080
<v 0>Yeah.</v>

130
00:07:32.080 --> 00:07:35.680
And this idea that everyone's like a part of a grand conspiracy to help each

131
00:07:35.681 --> 00:07:39.940
other out and push right. Ideology, even though, you know,

132
00:07:39.941 --> 00:07:42.760
a lot of people that were labeled as right. Aren't right.

133
00:07:43.000 --> 00:07:44.890
<v 2>Like who like me? Oh.</v>

134
00:07:45.370 --> 00:07:47.200
<v 0>I'm not right at all. My.</v>

135
00:07:47.200 --> 00:07:51.100
<v 1>Sense is your politics are pretty left on most stuff.</v>

136
00:07:52.240 --> 00:07:53.500
Um, although I don't, I mean, I don't know,

137
00:07:53.501 --> 00:07:56.860
you personally beyond just seeing your shows, um,

138
00:07:57.070 --> 00:07:58.840
but maybe the critique is based on,

139
00:07:58.990 --> 00:08:02.560
cause I think that those maps were based on what is the YouTube algorithm

140
00:08:03.190 --> 00:08:07.540
suggesting. And so that may not be in line with your personal, right.

141
00:08:07.540 --> 00:08:09.400
<v 0>It's just maybe what we're talking about.</v>

142
00:08:09.580 --> 00:08:13.390
Like if you're interested in conflict and if you're trying to get engagement,

143
00:08:13.590 --> 00:08:14.980
that that's the way to do it. Like in a few,

144
00:08:15.160 --> 00:08:19.240
two out YouTube algorithm is constantly suggesting people like Ben Shapiro or

145
00:08:19.241 --> 00:08:22.660
Gavin McInnes or whatever, and those videos come up over and over. Sure.

146
00:08:22.660 --> 00:08:27.280
<v 1>And I mean, so a lot of those people's candles do really well on YouTube.</v>

147
00:08:27.880 --> 00:08:31.930
So if you interview someone who has a channel themselves,

148
00:08:31.931 --> 00:08:33.820
there's a very good chance that the algorithm,

149
00:08:33.850 --> 00:08:36.610
if they're watching your interview with that person will say, well,

150
00:08:36.611 --> 00:08:39.850
here's a lot of their stuff. And then once you click there,

151
00:08:39.880 --> 00:08:42.820
the algorithm very quickly starts to build a picture of,

152
00:08:42.830 --> 00:08:46.630
of every individual user. If you watch your interview with Ben Shapiro,

153
00:08:46.690 --> 00:08:48.580
and then it takes you to a daily wire video,

154
00:08:48.940 --> 00:08:52.180
then it takes you to like the daily wire, second stringer guy. Right.

155
00:08:52.510 --> 00:08:54.100
And then you're off, who knows where.

156
00:08:54.440 --> 00:08:57.510
<v 0>Right. It's machine learning. Right. I mean, that's all the most part. Yeah.</v>

157
00:08:57.900 --> 00:09:00.240
That's, uh, it is,

158
00:09:01.080 --> 00:09:05.490
it's a troubling aspect of that thing that they do where they suggest the next

159
00:09:05.491 --> 00:09:09.630
videos, which didn't use to be a thing it used to be, you would go to YouTube,

160
00:09:09.990 --> 00:09:13.410
you would watch a video. Right. And then you would go find another video. Right.

161
00:09:13.440 --> 00:09:16.860
They didn't suggest anything. And then somewhere along the line,

162
00:09:17.070 --> 00:09:18.480
I don't remember what year it was,

163
00:09:18.481 --> 00:09:21.780
but this started happening and then they start auto-playing the next.

164
00:09:21.780 --> 00:09:22.700
<v 1>Video. Yeah.</v>

165
00:09:22.730 --> 00:09:27.680
I think there was some kind of recommendation thing very

166
00:09:27.681 --> 00:09:28.341
early on,

167
00:09:28.341 --> 00:09:31.820
but initially it might've been restricted to just other videos from the same

168
00:09:31.821 --> 00:09:35.180
channel you're watching probably. And at a certain point,

169
00:09:35.210 --> 00:09:37.340
it started to recommend other things.

170
00:09:37.700 --> 00:09:41.150
And I don't know if you look at your analytics and see what percentage of your

171
00:09:41.151 --> 00:09:45.410
views are coming from that recommendations, feed from other stuff.

172
00:09:46.370 --> 00:09:50.030
Um, but it's significant for a lot of YouTube channels, the,

173
00:09:50.031 --> 00:09:54.920
the tagging your videos and getting the right metadata on them in order to

174
00:09:54.921 --> 00:09:56.780
bring an audience is an important thing.

175
00:09:56.781 --> 00:09:59.660
So it's a double-edged sword in some sense. It sounds, it sounds like,

176
00:10:00.050 --> 00:10:03.650
but to get back to what you were saying about, so rich labeled you. Oh,

177
00:10:03.800 --> 00:10:06.260
as Richard's okay. Yeah. I was going to say they labeled you as right.

178
00:10:06.261 --> 00:10:07.094
But you're not right. Yeah.

179
00:10:07.740 --> 00:10:11.060
<v 0>It's disingenuous. I mean, I've said it over and over and over again.</v>

180
00:10:11.061 --> 00:10:13.130
I've never voted for a Republican in my life.

181
00:10:13.160 --> 00:10:16.790
I've voted independent for Gary Johnson just because he did my podcast and I

182
00:10:16.791 --> 00:10:19.760
wasn't happy with Clinton and I wasn't happy with Trump. She was like,

183
00:10:19.761 --> 00:10:23.060
this is just gross. I'm just going to vote for Gary Johnson. I mean,

184
00:10:23.150 --> 00:10:25.090
I didn't think he was going to win, you know,

185
00:10:25.900 --> 00:10:29.210
he had almost no chance when he didn't know where at what Aleppo was. Right.

186
00:10:29.430 --> 00:10:32.480
I was like, that was his, uh, he was his scream, you know,

187
00:10:32.481 --> 00:10:36.770
like what's his face from New Hampshire and Howard Dean Howard Dean. Yeah. Well.

188
00:10:36.770 --> 00:10:41.330
<v 1>Voting in California also. I, I assume it wasn't going to. Yeah.</v>

189
00:10:42.410 --> 00:10:45.410
<v 0>Um, but people conveniently will just as,</v>

190
00:10:45.440 --> 00:10:48.650
or they'll say that like you're a Trojan horse. They like, uh,

191
00:10:48.710 --> 00:10:52.700
you're a pretend left wing person. Who's really just pushing right.

192
00:10:52.850 --> 00:10:56.600
Wing ideologies. I'm like, well, which one? Which, which right. Wing ideology,

193
00:10:56.601 --> 00:10:58.550
is it gay marriage? Is it, what is it?

194
00:10:58.580 --> 00:11:02.690
I put I'm on the left on everything except maybe the second amendment.

195
00:11:02.930 --> 00:11:06.470
<v 1>Right. I think the criticism that, um,</v>

196
00:11:06.800 --> 00:11:10.670
could be levied if one wanted to make it into a criticism would be,

197
00:11:11.120 --> 00:11:16.040
if you engage with right-wing ideas that you don't

198
00:11:16.041 --> 00:11:18.080
agree with, right? Like I take you at your face, you know,

199
00:11:18.140 --> 00:11:19.280
at face value that you don't agree with.

200
00:11:19.281 --> 00:11:21.500
A lot of the stuff that you're right-wing guests say,

201
00:11:22.040 --> 00:11:26.240
one could make the argument that by not challenging those ideas,

202
00:11:26.270 --> 00:11:31.160
it's implicitly lending them more credibility than maybe you think they should

203
00:11:31.161 --> 00:11:31.970
have.

204
00:11:31.970 --> 00:11:35.960
<v 0>It's interesting because what I try to do with people, um,</v>

205
00:11:36.020 --> 00:11:38.120
unless something saying, someone's saying something, agregious,

206
00:11:38.150 --> 00:11:41.480
I try to let them talk. I want to know how they feel.

207
00:11:41.630 --> 00:11:43.460
I want to know what their thought process is.

208
00:11:43.760 --> 00:11:46.430
And so instead of just challenging them on it, on everything,

209
00:11:46.431 --> 00:11:50.930
I want them to elaborate. Right. And I feel like by doing that,

210
00:11:50.990 --> 00:11:54.820
I get a sense of how they've come that conclusion and whether it's logical,

211
00:11:54.850 --> 00:11:58.870
right. Whether it's, uh, when they're there,

212
00:11:58.930 --> 00:12:02.860
they've actually used their thoughts and they've really calculated and thought,

213
00:12:02.890 --> 00:12:07.120
this is the position I take. And this is why. And I, a lot of people don't know,

214
00:12:07.121 --> 00:12:07.954
there's a lot of,

215
00:12:08.160 --> 00:12:09.910
a lot of the times where you challenged people in their positions,

216
00:12:09.911 --> 00:12:13.360
you find out like, they don't really know what the they're talking about.

217
00:12:13.420 --> 00:12:17.560
And that the best way to find that out is to let them talk like Candace Owens on

218
00:12:17.561 --> 00:12:18.210
climate.

219
00:12:18.210 --> 00:12:21.810
<v 1>Right. That was, I mean, there's the Socratic method of questioning,</v>

220
00:12:22.230 --> 00:12:26.280
which is why do you think that? And how do you know that that's true, et cetera,

221
00:12:26.281 --> 00:12:29.550
et cetera. And sort of some other questions that, that come from it, uh,

222
00:12:29.551 --> 00:12:32.330
which I do as well. Um, I mean, I think, I dunno to,

223
00:12:32.331 --> 00:12:34.800
to tie it to the Richard Spencer interview that I did,

224
00:12:35.280 --> 00:12:39.510
some of the criticism I received after was from people on the left. I mean,

225
00:12:39.511 --> 00:12:42.750
the people I want to say it was most of the people on the low for doing the

226
00:12:42.751 --> 00:12:46.680
interview or for what I said in the interview for what I'm doing for doing the

227
00:12:46.681 --> 00:12:47.281
interview at all,

228
00:12:47.281 --> 00:12:50.460
the criticism was more from the left for what I said in the interview,

229
00:12:50.760 --> 00:12:52.440
the criticism was more from the right,

230
00:12:52.441 --> 00:12:54.060
from people who just agreed with Richard Spencer.

231
00:12:54.270 --> 00:12:59.130
Like what things did they agree with that it is inevitable

232
00:12:59.160 --> 00:13:04.050
that people with different ethnic or religious backgrounds simply

233
00:13:04.051 --> 00:13:06.270
will not be able to co-exist together peacefully.

234
00:13:06.271 --> 00:13:11.070
And we're better off trying to figure out how can we separate people based on

235
00:13:11.520 --> 00:13:16.350
their membership in ethnic or religious groups, separatism. I mean,

236
00:13:16.351 --> 00:13:17.184
literally separate. Yeah.

237
00:13:17.760 --> 00:13:18.593
<v 0>That's sad.</v>

238
00:13:19.320 --> 00:13:23.490
That's a sad thought that you just can't get along with people that do other

239
00:13:23.491 --> 00:13:27.450
things that are interest interested in other things that come from other places

240
00:13:27.451 --> 00:13:31.710
that have different religions that have different points of view. Like why? Uh.

241
00:13:31.950 --> 00:13:34.920
<v 1>Yeah. Well, they, they have a series of, uh, you know,</v>

242
00:13:34.921 --> 00:13:38.370
decades of what they call scholarship, supporting their view.

243
00:13:38.400 --> 00:13:40.530
But for the context of my interview,

244
00:13:40.770 --> 00:13:43.650
I made it abundantly clear that I didn't agree with that stuff. Right.

245
00:13:44.070 --> 00:13:45.690
And my view is,

246
00:13:45.691 --> 00:13:48.210
and everybody can have a different view about how they do interviews.

247
00:13:48.211 --> 00:13:51.450
My view is if I just allow the,

248
00:13:51.570 --> 00:13:55.710
what I consider to be disgusting views to be spread out, right. You know,

249
00:13:55.740 --> 00:13:59.370
like a spray bottle, just spray them everywhere. Not do anything else. Uh,

250
00:13:59.940 --> 00:14:02.640
I can't say that I'm doing something that I think is valuable.

251
00:14:02.700 --> 00:14:06.270
I don't feel like it's valuable. So my, my approach is,

252
00:14:06.870 --> 00:14:11.190
are the ideas known enough to be worth refuting?

253
00:14:11.191 --> 00:14:11.910
That's number one,

254
00:14:11.910 --> 00:14:16.470
if it's some weird conspiracy theory that has

255
00:14:16.650 --> 00:14:18.660
not even any following whatsoever,

256
00:14:18.690 --> 00:14:22.140
I'm probably not going to choose to even entertain it because it's irrelevant

257
00:14:22.420 --> 00:14:24.450
and sort of always, so my first question is,

258
00:14:25.210 --> 00:14:28.170
was Richard Spencer relevant at the time alt-right was rising?

259
00:14:28.260 --> 00:14:31.860
This guy was considered by many of the sort of creator of the alt-right.

260
00:14:32.100 --> 00:14:35.220
He was growing a following in the context of the Trump, uh,

261
00:14:35.250 --> 00:14:37.320
candidacy at the time, or maybe administration.

262
00:14:37.321 --> 00:14:41.340
I don't remember when exactly it was, it was, I think 2016,

263
00:14:41.341 --> 00:14:42.630
or I'm trying to remember when it was,

264
00:14:42.930 --> 00:14:45.180
I don't remember when I first heard his name. Yeah.

265
00:14:46.260 --> 00:14:48.420
<v 0>But how did he become, how did he come to prominence?</v>

266
00:14:49.290 --> 00:14:52.340
<v 1>I don't know the sequence, but I think he had a web,</v>

267
00:14:52.370 --> 00:14:56.270
an alt-right website that had articles of some kind.

268
00:14:56.720 --> 00:14:58.760
And then, um, he,

269
00:14:59.570 --> 00:15:02.340
that website became more known and that terms.

270
00:15:02.650 --> 00:15:07.510
<v 0>So talk th th all right, you know, all left the centrist,</v>

271
00:15:07.570 --> 00:15:09.460
I got all these, all these different labels are.

272
00:15:10.230 --> 00:15:14.140
<v 1>Talking about issues. It's so clunky. So, you know, first thing was,</v>

273
00:15:14.141 --> 00:15:15.400
I did want to interview him,

274
00:15:15.430 --> 00:15:19.720
but if I had felt that I wouldn't be prepared to make it abundantly clear that I

275
00:15:19.721 --> 00:15:22.000
don't agree with the guy. And I think his ideas are terrible.

276
00:15:22.001 --> 00:15:23.140
I wouldn't have done the interview. Right.

277
00:15:23.410 --> 00:15:27.130
So the problem I had with the critiques, from the left of me doing that,

278
00:15:27.370 --> 00:15:31.510
some who said, the last thing we need to be doing is giving this guy a voice.

279
00:15:31.540 --> 00:15:33.610
That's often how they say it or a platform.

280
00:15:34.270 --> 00:15:38.320
My response was this guy is getting interviewed in lots of other places that

281
00:15:38.321 --> 00:15:39.880
aren't even challenging him. Right?

282
00:15:39.910 --> 00:15:44.500
I'm at least making an attempt here to get something in the record that there

283
00:15:44.501 --> 00:15:47.140
are arguments against these ideas. These are bad ideas.

284
00:15:47.200 --> 00:15:50.860
And I don't want to be part of the diffusion of just the ideas themselves.

285
00:15:51.160 --> 00:15:51.730
I want to be pushing.

286
00:15:51.730 --> 00:15:54.700
<v 0>Back. I'm going to have to, now, now, when you did do that, like,</v>

287
00:15:54.701 --> 00:15:56.920
what was his response was during the interview?

288
00:15:56.960 --> 00:15:59.260
What was his response to your pushback? Uh.

289
00:15:59.320 --> 00:16:02.410
<v 1>I mean, he had answers, you know, he's he's, he was well-prepared.</v>

290
00:16:02.530 --> 00:16:07.450
I don't know if they're, they were unique or new arguments that I was making.

291
00:16:08.290 --> 00:16:08.320
Um,

292
00:16:08.320 --> 00:16:12.130
but there was no argument to be made that I was letting him just parrot white

293
00:16:12.131 --> 00:16:14.470
nationalists talking points on a post, which I wouldn't.

294
00:16:14.471 --> 00:16:17.320
I just wouldn't feel good about that. That's not how I do interviews. Yeah.

295
00:16:17.390 --> 00:16:20.650
<v 0>And then the, the left was upset that you were giving him air quotes,</v>

296
00:16:20.680 --> 00:16:21.790
a platform, a very small.

297
00:16:21.790 --> 00:16:24.670
<v 1>Portion of the left. I want to be super clear the left. I mean,</v>

298
00:16:24.671 --> 00:16:28.630
my audience is very left. Almost everybody understood what I was doing.

299
00:16:29.080 --> 00:16:31.810
10 years ago, I was interviewing the Westboro Baptist church.

300
00:16:32.110 --> 00:16:35.380
Most people understood what I was doing. They were more prominent at the time,

301
00:16:35.381 --> 00:16:39.040
but there was this sliver of the left that just didn't want the conversation to

302
00:16:39.041 --> 00:16:43.210
take place. And I always struggle with this because as you can see,

303
00:16:43.270 --> 00:16:46.240
I have no problem criticizing that sliver of the left.

304
00:16:46.570 --> 00:16:51.520
My concern is getting like overly wrapped up to criticisms of the left

305
00:16:51.910 --> 00:16:55.930
that are only held by these like niche slices. Yes.

306
00:16:56.080 --> 00:16:57.940
And that's why, um,

307
00:16:58.000 --> 00:17:02.350
I try to avoid going further than necessary into those criticism.

308
00:17:03.250 --> 00:17:03.250
Okay.

