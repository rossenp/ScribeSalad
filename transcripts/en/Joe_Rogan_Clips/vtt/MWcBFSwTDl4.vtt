WEBVTT

1
00:00:01.020 --> 00:00:02.360
<v 0>The Joe Rogan experience.</v>

2
00:00:03.040 --> 00:00:06.200
I was watching a talk that you were giving and you were talking about the,

3
00:00:06.300 --> 00:00:11.160
the growth of innovation and technology and GDP over the last 100 years.

4
00:00:11.300 --> 00:00:16.040
And you were talking about the entire history of life on earth and

5
00:00:16.041 --> 00:00:19.720
what a short period of time humans have been here. And then what a,

6
00:00:19.740 --> 00:00:21.390
and during what short period of time,

7
00:00:21.420 --> 00:00:26.310
what a stunning amount of innovation and how much change we've enacted on

8
00:00:26.311 --> 00:00:30.510
the earth and just a, a blink of an eye. And you had the scale of GDP,

9
00:00:30.930 --> 00:00:32.630
mm-hmm, &lt;affirmative&gt; over, you know, the,

10
00:00:32.631 --> 00:00:36.030
the course of the last a hundred years. It's, it's crazy to,

11
00:00:36.031 --> 00:00:39.750
because it's so difficult for us with our current perspective,

12
00:00:40.100 --> 00:00:44.580
just being a person and living going about the day to day life,

13
00:00:44.581 --> 00:00:49.580
that seems so normal to put it in perspective time wise and see what an

14
00:00:49.780 --> 00:00:54.340
enormous amount of change has taken place in relatively an incredibly short

15
00:00:54.341 --> 00:00:55.500
amount of time. Yeah.

16
00:00:55.660 --> 00:00:57.220
<v 1>I mean, we,</v>

17
00:00:57.221 --> 00:01:00.140
we think of this as sort of the normal way for things to be that the,

18
00:01:00.141 --> 00:01:02.290
the idea that the alarm way see up in the morning,

19
00:01:02.590 --> 00:01:05.490
and then you commute in and sit in front of a computer all day and you try not

20
00:01:05.491 --> 00:01:10.130
to eat too much. And, uh, and that if, if you sort of imagine that, oh, no,

21
00:01:10.131 --> 00:01:12.810
maybe in 50 years or a hundred years, or at some point in the future,

22
00:01:13.090 --> 00:01:15.410
it's gotta be very different. That's like some radical hypothesis,

23
00:01:16.110 --> 00:01:20.450
but of course this quote unquote normal condition is a huge

24
00:01:20.920 --> 00:01:23.160
anomaly and it, and it way you look at it. I mean,

25
00:01:23.161 --> 00:01:27.960
if you look at it on geological timescale, the human species is very young. Um,

26
00:01:28.740 --> 00:01:33.440
if you look at it historically, you know, for, you know, more than 90%,

27
00:01:34.020 --> 00:01:37.720
we were just hunter gatherers running around and the agriculturalists for, and,

28
00:01:37.721 --> 00:01:42.400
and the what, what, what the last couple of hundred years,

29
00:01:42.550 --> 00:01:47.190
when some parts of the world have escaped the maltusian condition, where,

30
00:01:47.191 --> 00:01:49.110
where you basically, um,

31
00:01:49.220 --> 00:01:53.350
only have as much income as you need to be able to produce to children. Uh,

32
00:01:53.370 --> 00:01:56.390
and we have the population exploit, like all of this is very, very, very recent,

33
00:01:57.390 --> 00:02:02.110
um, and in, in space as well, of course, almost everything is ultra high vacuum.

34
00:02:02.650 --> 00:02:07.540
And, and we live on the surface of this little special crumb. Mm.

35
00:02:07.900 --> 00:02:10.620
Um, and, and yet we think this is normal and everything else is weird,

36
00:02:10.760 --> 00:02:15.020
but I think that's a complete inversion. And so when you do plot,

37
00:02:15.080 --> 00:02:18.660
if you do plot, for example, um, world GDP,

38
00:02:19.070 --> 00:02:23.460
which is a kind of rough measure for the total amount of productive cable

39
00:02:23.570 --> 00:02:27.250
ability that we have, right. Um,

40
00:02:27.790 --> 00:02:29.570
if you plotted over 10,000 years, what,

41
00:02:29.571 --> 00:02:33.730
what you see is just a flat line and then a, a vertical line,

42
00:02:34.310 --> 00:02:36.730
and you can't really see any other structure. Like it's,

43
00:02:36.731 --> 00:02:41.450
it's so extreme that the degree to which, um, humanities productive CAPA. So,

44
00:02:41.670 --> 00:02:46.200
so if looks at this picture and we, no, now we imagine this is now the normal,

45
00:02:46.201 --> 00:02:49.960
this is the way it's gonna be now indefinitely. It just can means a

46
00:02:51.560 --> 00:02:55.040
PRI FESA impossibleible like, it, it sort of doesn't look like,

47
00:02:55.041 --> 00:02:57.480
like we are in a static period right now. It,

48
00:02:57.481 --> 00:03:00.360
it looks like this is we're in the middle of some kind of explosion.

49
00:03:00.910 --> 00:03:05.160
<v 0>Explosion. And, and oddly enough, everyone involved in the explosion,</v>

50
00:03:06.440 --> 00:03:09.600
everyone that's innovating everyone. That's creating all this new technology.

51
00:03:09.790 --> 00:03:14.600
They're all a part of this momentum that was created before they were even born.

52
00:03:15.140 --> 00:03:16.520
So it does feel normal.

53
00:03:16.590 --> 00:03:21.470
They're just a part of this whole spinning machine. And they jump in,

54
00:03:21.471 --> 00:03:23.310
they're born, they go to college. Next thing you know,

55
00:03:23.311 --> 00:03:25.510
they have a job and they're contributing, they're making new technology.

56
00:03:25.690 --> 00:03:28.430
And then more people jump in and add onto it.

57
00:03:28.970 --> 00:03:33.510
And there's very little perspective in terms of like the historical significance

58
00:03:33.511 --> 00:03:36.550
of this incredible explosion. Technologically,

59
00:03:36.740 --> 00:03:41.420
when you look at what you're talking about, that C spike, no one feels it,

60
00:03:41.421 --> 00:03:43.260
which is one of the weirdest things about it.

61
00:03:43.930 --> 00:03:45.820
<v 1>That, I mean, you kind of expect every year,</v>

62
00:03:46.060 --> 00:03:48.300
there will be a better iPhone or whatever, right. If not,

63
00:03:48.301 --> 00:03:51.660
would be upset for almost all of human history, people lived and died.

64
00:03:52.080 --> 00:03:55.700
And so absolutely no technological change. And in fact, you could have many,

65
00:03:55.701 --> 00:03:57.420
many generations. Um,

66
00:03:58.240 --> 00:04:03.170
the very idea that that was some trajectory in

67
00:04:03.390 --> 00:04:08.290
the material conditions is, is a relatively new idea. I mean,

68
00:04:08.291 --> 00:04:11.090
people thought of history either as, you know,

69
00:04:11.091 --> 00:04:15.650
some kind of descent from a golden age or some people had a cyclical view,

70
00:04:16.350 --> 00:04:20.440
but it was all in terms of political organization that would be great kingdom.

71
00:04:20.700 --> 00:04:23.200
And then a wiseis ruler would rule for a while.

72
00:04:23.340 --> 00:04:25.600
And then like a few hundred years later, you know,

73
00:04:25.601 --> 00:04:30.120
their grand greatgrandchildren would be too greedy and it would come into

74
00:04:30.121 --> 00:04:31.560
anarchy. And then, you know,

75
00:04:31.561 --> 00:04:34.240
a few hundred years later it would come back together again. Yes. So,

76
00:04:34.460 --> 00:04:36.840
so it would be all these kind of the pieces moving around. Right.

77
00:04:36.860 --> 00:04:38.960
But no new pieces really entering, or if they did,

78
00:04:39.020 --> 00:04:42.590
it was at such a slower rate that you didn't notice. Um,

79
00:04:42.770 --> 00:04:47.150
but over the eons, you know, the, the wheel slowly turns and,

80
00:04:47.490 --> 00:04:51.670
you know, somebody makes a slightly better wheel. Somebody figures out how to,

81
00:04:52.570 --> 00:04:56.430
you know, irrigate slightly better day, they breed better crops. And, and, uh,

82
00:04:56.630 --> 00:05:01.420
and eventually there is enough that you could have enough of a population, um,

83
00:05:01.840 --> 00:05:06.620
enough brains that then create more ideas at the quick enough rate that you,

84
00:05:06.621 --> 00:05:10.620
you get this industrial revolution. Um, and, uh,

85
00:05:10.760 --> 00:05:12.140
and that's where we are now. I think.

86
00:05:12.890 --> 00:05:17.020
<v 0>Elon Musk had the most terrifying description of humanity said that we are the</v>

87
00:05:17.021 --> 00:05:20.380
biological boot loader for artificial intelligence.

88
00:05:21.990 --> 00:05:22.823
<v 1>Mm-hmm, &lt;affirmative&gt;.</v>

89
00:05:23.480 --> 00:05:24.690
<v 0>That, that's what we're here.</v>

90
00:05:24.690 --> 00:05:26.250
<v 1>For. Well, bootlers are important.</v>

91
00:05:26.810 --> 00:05:30.570
<v 0>&lt;Laugh&gt; they are important. But I think, oh,</v>

92
00:05:30.571 --> 00:05:34.210
there there's like objectively, and there's personally like objectively,

93
00:05:34.350 --> 00:05:37.730
if you were outside of the human race and you were looking at all these various

94
00:05:37.800 --> 00:05:41.250
life forms competing, uh, on this planet for,

95
00:05:41.470 --> 00:05:45.920
for resource and for survival, you would look at humanity and you'd go, well,

96
00:05:45.980 --> 00:05:47.800
you know, clearly that's not, it's not finished,

97
00:05:48.020 --> 00:05:50.200
so there's gonna be another version of it. It's like,

98
00:05:50.201 --> 00:05:51.680
when is this version gonna take place?

99
00:05:51.681 --> 00:05:54.480
Is it gonna take place over millions and millions of years?

100
00:05:54.670 --> 00:05:59.440
Like it has historically when it comes to biological organisms or is it going

101
00:05:59.500 --> 00:06:03.480
to invent something that takes over from there?

102
00:06:03.500 --> 00:06:06.720
And then that's the new thing. Some something that's not based on tissue,

103
00:06:06.721 --> 00:06:08.440
something that's not based on cells.

104
00:06:08.920 --> 00:06:10.760
And doesn't have the biological limitations that we have,

105
00:06:11.020 --> 00:06:16.000
nor nor does it have all the emotional attachments attachments to things like

106
00:06:16.010 --> 00:06:18.270
breed, social dominance, hierarchies,

107
00:06:18.290 --> 00:06:22.430
all those things are no consequence to it doesn't mean anything because it's not

108
00:06:22.431 --> 00:06:23.264
biological.

109
00:06:23.900 --> 00:06:28.550
<v 1>Yeah. Yeah. I mean, I, I, I don't think millions of years, I mean,</v>

110
00:06:29.330 --> 00:06:30.870
number of decades or whatever, but,

111
00:06:30.970 --> 00:06:32.990
but it's interesting that even if we set that aside,

112
00:06:33.130 --> 00:06:36.870
we say machine intelligence is possible. For some reason,

113
00:06:37.030 --> 00:06:40.980
let's just play with that. Then I still think that would be a very rapid change,

114
00:06:40.981 --> 00:06:45.100
including biological change. And, um, um, I mean,

115
00:06:45.101 --> 00:06:49.260
we are doing great advances, making great advances in biotech as well,

116
00:06:49.520 --> 00:06:54.140
and will increasingly be able to control what our own

117
00:06:54.420 --> 00:06:58.250
organisms are doing, um, through different means, um,

118
00:06:58.750 --> 00:07:02.610
and enhance human capacities through biotechnology.

119
00:07:03.390 --> 00:07:07.130
And so, so even there a lot is, is not gonna happen overnight,

120
00:07:07.750 --> 00:07:12.090
but over an historically very short period of time, I,

121
00:07:12.170 --> 00:07:16.650
I think you would still see quite profound change just from applying bioscience

122
00:07:16.750 --> 00:07:19.440
to, to change human capacities. Yeah.

123
00:07:19.460 --> 00:07:21.440
<v 0>One of the technologies,</v>

124
00:07:21.500 --> 00:07:24.920
or one of the things that's been discussed to sort of mitigate the dangers of

125
00:07:24.921 --> 00:07:28.600
artificial intelligence is a potential merge, um,

126
00:07:28.950 --> 00:07:33.600
some sort of symbiotic relationship with technology that you, you see,

127
00:07:33.601 --> 00:07:35.520
you hear discussed. Like, um,

128
00:07:36.040 --> 00:07:39.630
I don't know exactly how Elon's rule link works,

129
00:07:40.290 --> 00:07:42.750
but it seems like a step in that direction.

130
00:07:42.751 --> 00:07:47.670
There's some sort of a brain implant that in that interacts with

131
00:07:47.671 --> 00:07:49.670
an external device and this,

132
00:07:49.970 --> 00:07:53.830
all of this increases the bandwidth for available intelligence and knowledge.

133
00:07:54.780 --> 00:07:55.090
Yeah.

134
00:07:55.090 --> 00:07:57.910
<v 1>I'm, I'm sort of skeptical that that will work. I mean,</v>

135
00:07:58.620 --> 00:08:02.300
good &lt;affirmative&gt; that somebody tries it, you know, uh, but, um,

136
00:08:02.460 --> 00:08:06.180
I think it's quite technically hard to improve

137
00:08:08.260 --> 00:08:10.540
a normal, healthy human beings, uh,

138
00:08:10.960 --> 00:08:15.260
say cognitive capacity or the capacities by implanting things in them, um,

139
00:08:15.880 --> 00:08:19.890
and get benefits that you couldn't equally well get by having the GAT outside of

140
00:08:19.891 --> 00:08:23.010
the body. Mm-hmm &lt;affirmative&gt; so, um, I,

141
00:08:23.090 --> 00:08:26.210
I don't need to have an implant to be able to use Google. Right. I right.

142
00:08:27.230 --> 00:08:30.330
And there are a lot of advantages to, to having it external, you,

143
00:08:30.331 --> 00:08:33.850
you can kind of upgrade it very easily. You can shut it off because, well,

144
00:08:33.851 --> 00:08:36.530
hopefully you could do that even with implant &lt;laugh&gt;. Um,

145
00:08:37.350 --> 00:08:40.680
and once you start to look into the details, so there, so that of demos,

146
00:08:40.681 --> 00:08:43.080
but then if you, if you actually look at the papers,

147
00:08:43.090 --> 00:08:46.360
often you find wild and then there were these side effects and the person had

148
00:08:46.520 --> 00:08:49.720
headaches, or they had some deficit and the speech, you know, like yes,

149
00:08:49.721 --> 00:08:54.040
infection like this just biology is messy. Yes. Um, so,

150
00:08:55.480 --> 00:08:59.960
um, you know, maybe it will work better than, than I expect that, that,

151
00:08:59.990 --> 00:09:04.920
that could be good, but other, otherwise I think, um, that the,

152
00:09:04.940 --> 00:09:09.760
the place where it will first become possible to enhance you of hu human

153
00:09:09.761 --> 00:09:14.080
biological capacities, uh, would be through, um,

154
00:09:14.510 --> 00:09:18.830
ethics selection, um, which is technologically, uh,

155
00:09:19.100 --> 00:09:20.710
something very near do you.

156
00:09:20.710 --> 00:09:21.790
<v 0>Mean like CRISPR type.</v>

157
00:09:22.110 --> 00:09:23.630
<v 1>Things? Well, so that would be editing right.</v>

158
00:09:23.631 --> 00:09:26.110
When you actually go in and change things that, that also is moving.

159
00:09:26.111 --> 00:09:27.430
What do you, you mean by selection? Well,

160
00:09:27.431 --> 00:09:30.630
so this would just be in the context of say in vitro fertilizization you have,

161
00:09:31.630 --> 00:09:32.100
um,

162
00:09:32.100 --> 00:09:36.980
usually some half dozen or dozen else created during this fertility

163
00:09:36.981 --> 00:09:38.420
procedure, which is standardly used.

164
00:09:39.000 --> 00:09:41.940
So rather than just the doctor kind of looking at these embryos and saying,

165
00:09:41.941 --> 00:09:45.020
well, that one looks healthy, or I'm gonna implant that, uh, you,

166
00:09:45.021 --> 00:09:49.980
you could run some genetic test and then use that as a predictor and select

167
00:09:51.040 --> 00:09:54.380
the one you think has the, the Mo the most desirable attributes.

168
00:09:54.840 --> 00:09:59.530
<v 0>And so this could be a trend in terms of how human beings reproduce that we,</v>

169
00:10:00.040 --> 00:10:03.810
instead of just randomly having sex woman gets pregnant,

170
00:10:03.940 --> 00:10:06.930
gives birth to a child. We don't know what it's gonna be,

171
00:10:07.000 --> 00:10:08.130
what it's what's gonna happen.

172
00:10:08.790 --> 00:10:11.930
We just hope that it's a good kid instead of that,

173
00:10:12.270 --> 00:10:16.720
you start looking at the, all the various components that we can measure.

174
00:10:17.790 --> 00:10:21.280
<v 1>Yeah. Uh, and so, I mean, to some extent, we already do this.</v>

175
00:10:21.281 --> 00:10:25.760
There are a lot of, um, testing done, um, um,

176
00:10:26.700 --> 00:10:30.880
for various chmosomal abnormalities that you can already check for, but,

177
00:10:30.881 --> 00:10:32.600
but our ability to, uh,

178
00:10:32.780 --> 00:10:37.670
to look beyond clear stark diseases that Ongen is wrong. Like,

179
00:10:37.940 --> 00:10:42.230
like to look at more complex trait is, is, is increasing rapidly. Um,

180
00:10:42.570 --> 00:10:45.990
so obviously there are a lot of ethical issues and different view that come into

181
00:10:45.991 --> 00:10:49.550
that. But if I, if you're just talking, what is technologicalically feasible? I,

182
00:10:49.590 --> 00:10:50.990
I think that, that, I mean, already,

183
00:10:50.991 --> 00:10:54.590
you could do a very limited amount of that today, and maybe you would get,

184
00:10:56.090 --> 00:10:56.923
you know,

185
00:10:57.240 --> 00:11:01.140
two or three ICU points in expectation more if you selected using current

186
00:11:01.141 --> 00:11:05.340
technology based on 10 embryos, let's say so very small, but,

187
00:11:05.560 --> 00:11:07.060
but as genomics, uh,

188
00:11:07.570 --> 00:11:12.260
gets better at deciphering the genetic architecture of complex

189
00:11:12.261 --> 00:11:16.260
traits, whether it's intelligence or, or personality attributes than, than you,

190
00:11:16.261 --> 00:11:19.010
you would have more select power and you could do more. And,

191
00:11:19.011 --> 00:11:21.730
and then there is a number of other technologies we don't yet have,

192
00:11:21.731 --> 00:11:24.290
but which if you did, would then kind of stack with that and,

193
00:11:24.291 --> 00:11:28.690
and enable much more powerful forms of, of enhancement. Um,

194
00:11:29.110 --> 00:11:33.570
so, so, so there, uh, yeah, I don't think there are, and,

195
00:11:33.770 --> 00:11:36.450
and a major technological hurdles really in, in the way,

196
00:11:36.680 --> 00:11:39.520
just some small amount of incremental father improvement.

197
00:11:40.340 --> 00:11:41.840
<v 0>That's when you talk about</v>

198
00:11:43.490 --> 00:11:48.160
doing something with genetics and human beings and

199
00:11:48.470 --> 00:11:51.680
selecting, selecting for the superior versions.

200
00:11:52.220 --> 00:11:55.640
And then if everybody starts doing that, the ethical concerns, when,

201
00:11:55.880 --> 00:11:57.760
when you start discussing that people get very nervous.

202
00:11:58.320 --> 00:12:01.120
Cause they start to look at their own genetic defects and they go, oh my God,

203
00:12:01.121 --> 00:12:03.240
what if I didn't make the cut? Yeah. Like I wouldn't be here.

204
00:12:03.241 --> 00:12:06.240
And you start thinking about all the imperfect people that have actually

205
00:12:06.241 --> 00:12:10.080
contributed in some pretty spectacular ways yeah. To what our culture is.

206
00:12:10.700 --> 00:12:13.310
And like, well, what if everybody has perfect genes,

207
00:12:13.311 --> 00:12:16.750
would all these things even take place? Like, what are we doing really,

208
00:12:16.930 --> 00:12:21.670
if we're bypassing nature and we're choosing to select for the traits

209
00:12:21.770 --> 00:12:25.750
and the attributes that we find to be the most positive and attractive,

210
00:12:26.340 --> 00:12:29.110
like what are like that? And.

211
00:12:30.030 --> 00:12:31.030
<v 1>You take what, what would've happened.</v>

212
00:12:31.090 --> 00:12:35.540
If say some earlier age had had this ability to kind of

213
00:12:36.290 --> 00:12:40.580
lock in their yes. You know, their, their, their prejudice or,

214
00:12:41.700 --> 00:12:45.060
um, if Victorians had had this sure. Maybe it would all be, uh,

215
00:12:45.340 --> 00:12:50.260
whatever Pius and patriotic now or something. Yeah. We know like another. Yeah.

216
00:12:50.640 --> 00:12:54.650
So, um, so, so in, in general,

217
00:12:54.680 --> 00:12:58.650
with all of these powerful technologies, we, we are developing there.

218
00:12:58.651 --> 00:13:00.170
There is like,

219
00:13:00.370 --> 00:13:04.210
I think the ideal course would be that we would first gain a bit more wisdom and

220
00:13:04.211 --> 00:13:08.250
then we would get all of these powerful tools. Um,

221
00:13:08.350 --> 00:13:12.970
but it looks like we're getting the powerful tools before we have really, uh,

222
00:13:12.971 --> 00:13:15.400
achieved a very high level of waste them. Yeah. And so.

223
00:13:15.980 --> 00:13:20.680
<v 0>But we haven't earned them. The people that are using them are sort of we're,</v>

224
00:13:20.940 --> 00:13:25.680
we haven't like thing about the, the technology that all of us use. How many,

225
00:13:26.700 --> 00:13:30.200
how many pieces of technology do you use in a day and how much do you actually

226
00:13:30.201 --> 00:13:31.640
understand any of those?

227
00:13:31.950 --> 00:13:34.480
Most people have very little understanding of any of the thing,

228
00:13:34.550 --> 00:13:39.270
things they use work. They put no effort at all into creating those things,

229
00:13:39.770 --> 00:13:42.790
but yet they've inherited the responsibility of the power that those things

230
00:13:42.791 --> 00:13:43.624
possess.

231
00:13:43.860 --> 00:13:46.150
<v 1>Yeah. I mean that, that's the only way we, we can do it. Uh,</v>

232
00:13:46.290 --> 00:13:49.070
it is just way too complex for any person. If,

233
00:13:49.071 --> 00:13:52.230
if you had to sort of learn how to build everything, every tool you use,

234
00:13:52.510 --> 00:13:53.750
like you wouldn't get very far.

235
00:13:54.000 --> 00:13:54.990
<v 0>Isn't that fascinating though,</v>

236
00:13:55.090 --> 00:13:57.780
when you think about human beings and all the different things we do,

237
00:13:58.760 --> 00:14:03.740
we have very little understanding of the mechanisms behind most of what

238
00:14:03.840 --> 00:14:08.500
we need for day to day life. Yet we just use them because there's so many of us.

239
00:14:08.600 --> 00:14:12.220
And so many people are understanding various parts of all these different things

240
00:14:12.690 --> 00:14:17.050
that together collectively we can utilize intelligence of all these millions of

241
00:14:17.051 --> 00:14:20.490
people that have innovated. And we with no work whatsoever,

242
00:14:20.520 --> 00:14:23.250
just go into the Verizon store and pick up the new phone. Yeah.

243
00:14:23.530 --> 00:14:25.530
<v 1>I mean, and not just technology,</v>

244
00:14:25.590 --> 00:14:29.370
but worldviews and political ideas as well.

245
00:14:29.440 --> 00:14:33.010
It's not as if most people sit down, uh,

246
00:14:33.720 --> 00:14:36.840
with an empty table, try to think from the basic principles up,

247
00:14:36.841 --> 00:14:40.320
what would be the ideal configuration of the state or something like that. Yeah.

248
00:14:40.321 --> 00:14:44.720
You just kind of absorb it and go with it, float in the stream of culture. Yeah.

249
00:14:44.721 --> 00:14:47.120
And, uh, and, and it,

250
00:14:47.121 --> 00:14:51.000
it's amazing just how little of that actually at any point channels through your

251
00:14:51.040 --> 00:14:54.520
sort of conscious attention where you make some rational, otherwise,

252
00:14:54.521 --> 00:14:59.160
but likeliberate a decision most, you just get carried away with. Um,

253
00:14:59.460 --> 00:15:02.640
so, but, but that, that again, I mean, if, if we have, if,

254
00:15:02.641 --> 00:15:05.360
if this is what we have to work with, then I there's no other way.

255
00:15:05.470 --> 00:15:06.080
There's no other.

256
00:15:06.080 --> 00:15:08.960
<v 0>Way possib &lt;laugh&gt;, it's, there's no other way. And there's no way,</v>

257
00:15:09.510 --> 00:15:11.590
even like you now discussing this,

258
00:15:11.780 --> 00:15:14.870
like discussing the, the,

259
00:15:14.930 --> 00:15:18.590
the history of this incredible spike of evolution, uh, or,

260
00:15:18.610 --> 00:15:20.350
or innovation rather in technology,

261
00:15:21.550 --> 00:15:26.310
it Fe it just doesn't feel like anything. It feels normal.

262
00:15:26.970 --> 00:15:29.470
So even though we can intellectualize it,

263
00:15:29.500 --> 00:15:32.500
even though we can have this conversation and talk about what an incredible time

264
00:15:32.501 --> 00:15:36.060
we're in and how terrifying it is that things are moving at such an incredibly

265
00:15:36.061 --> 00:15:40.300
rapid rate. And no one, no one's putting the brakes on it. No,

266
00:15:40.301 --> 00:15:44.620
one's thinking about the, the potential pros and cons. We're just pushing ahead.

267
00:15:44.690 --> 00:15:45.480
Yeah. Well.

268
00:15:45.480 --> 00:15:49.500
<v 1>Not nobody not. I mean, there are a few people I've got my research group. Yes.</v>

269
00:15:49.501 --> 00:15:51.260
There's actually increase. I mean, when I,

270
00:15:52.130 --> 00:15:56.890
I got interested in these things in the nineties and it was very much a fringe,

271
00:15:58.170 --> 00:16:01.650
uh, activity. There was some internet mailing list on people exchanging ideas,

272
00:16:01.651 --> 00:16:04.450
but, but over since then, I mean, there's now a small

273
00:16:06.390 --> 00:16:10.010
set of academic research institutes and some other that are kind of actually

274
00:16:10.011 --> 00:16:14.280
trying to do more systematic thinking about some of these big picture topics.

