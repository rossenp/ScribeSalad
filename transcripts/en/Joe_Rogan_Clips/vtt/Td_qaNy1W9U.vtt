WEBVTT

1
00:00:01.050 --> 00:00:03.690
<v 0>The Joe Rogan experience. But I think if I.</v>

2
00:00:03.690 --> 00:00:04.680
<v 1>Was in the future,</v>

3
00:00:04.860 --> 00:00:09.090
some weird dystopian future where artificial intelligence runs everything and,

4
00:00:09.130 --> 00:00:13.260
and human beings are linked to some sort of

5
00:00:14.070 --> 00:00:18.090
neurological implant that connects us all together.

6
00:00:18.091 --> 00:00:21.390
And we long for the days of biological independence.

7
00:00:21.391 --> 00:00:22.860
And we would like to see what it,

8
00:00:22.861 --> 00:00:25.500
what was it like when they first started inventing phones?

9
00:00:25.680 --> 00:00:29.100
What was it like when the internet was first opened up for people?

10
00:00:29.280 --> 00:00:32.310
What was it like when people saw when, when,

11
00:00:32.340 --> 00:00:36.360
when someone had someone like you on a podcast and was talking about

12
00:00:36.570 --> 00:00:40.500
potential artificial intelligence of where it could lead us and what could the

13
00:00:40.501 --> 00:00:43.920
most interesting time, time? Yeah, that's,

14
00:00:43.921 --> 00:00:47.760
what's cool about it to me is that we seem to be in this,

15
00:00:48.420 --> 00:00:52.590
this really Goldilocks period of great change while we're still human,

16
00:00:52.860 --> 00:00:55.710
but we're worried about privacy. We're concerned.

17
00:00:55.711 --> 00:00:58.890
Our phones are listening to us. We're concerned about surveillance dates and,

18
00:00:59.160 --> 00:01:03.750
you know, people put little stickers over the laptop camera, we see it coming,

19
00:01:03.900 --> 00:01:06.090
but it hasn't quite hit us yet.

20
00:01:06.270 --> 00:01:10.050
We're just seeing the problems that are associated with this

21
00:01:10.290 --> 00:01:14.400
increased level of technology in our lives.

22
00:01:16.460 --> 00:01:21.320
<v 2>Which is yeah, that, that is a stretch thing. If we add up all these pieces,</v>

23
00:01:21.321 --> 00:01:26.120
it does put us in this very weirdly special position and you

24
00:01:26.121 --> 00:01:30.320
wonder, Hm it's a little bit too much of a coincidence.

25
00:01:32.210 --> 00:01:35.650
It might be the case, but yeah, it does put some strain on it.

26
00:01:35.690 --> 00:01:37.970
<v 1>When you say a little too much of a question. So.</v>

27
00:01:39.290 --> 00:01:41.750
<v 2>Um, I mean, I guess the intuitive way of thinking about it, like what way,</v>

28
00:01:41.751 --> 00:01:46.310
like what, what are the chances that just by chance you would happen to be, uh,

29
00:01:46.340 --> 00:01:51.320
living in the most interesting time in history being like a celebrity,

30
00:01:51.350 --> 00:01:56.210
like whatever, like that's pretty low prior probability. Like most people,

31
00:01:56.510 --> 00:02:00.650
well from you, I mean far far, but for all of us. Um,

32
00:02:02.030 --> 00:02:06.920
um, and so that, that, that could just be, I mean, I, I w if there's a lottery,

33
00:02:07.310 --> 00:02:11.150
someone has got to have the ticket. Right. But, um, um,

34
00:02:11.640 --> 00:02:15.830
or yeah, or, or, or we are wrong about this whole picture.

35
00:02:16.040 --> 00:02:19.640
And, uh, there is some very different structure in place that's,

36
00:02:19.641 --> 00:02:22.880
which would make our, uh, experiences more typical.

37
00:02:23.630 --> 00:02:26.750
<v 3>That's what I was getting to I, so I gathered, yeah.</v>

38
00:02:27.080 --> 00:02:31.490
<v 1>So how much have you considered the possibility of a</v>

39
00:02:31.491 --> 00:02:32.324
simulation?

40
00:02:32.780 --> 00:02:36.890
<v 2>Well, a lot. I mean, I, I developed a simulation argument. Yes. Um,</v>

41
00:02:36.950 --> 00:02:41.300
back in the early two thousands. And so, uh, yeah,

42
00:02:42.890 --> 00:02:43.100
but I.

43
00:02:43.100 --> 00:02:46.200
<v 1>Mean, I know that you developed this argument and I know that you've,</v>

44
00:02:46.201 --> 00:02:50.000
you've spent a great deal of time working on this, but personally,

45
00:02:50.390 --> 00:02:54.920
the way you view the world, how much,

46
00:02:55.250 --> 00:02:56.083
how much,

47
00:02:56.270 --> 00:03:00.760
how much does it play into your vision of what reality is?

48
00:03:02.230 --> 00:03:06.580
<v 3>Well, um, it's oh.</v>

49
00:03:06.630 --> 00:03:09.420
<v 2>Hard to say. I mean,</v>

50
00:03:09.450 --> 00:03:13.440
for the majority of my time,

51
00:03:13.441 --> 00:03:18.090
I'm not actively thinking about that. I'm just like, you know, living.

52
00:03:18.360 --> 00:03:20.300
And, uh,

53
00:03:20.340 --> 00:03:24.840
now I have this weird that my work is actually to think about big picture

54
00:03:24.841 --> 00:03:27.030
questions. So it kind of comes in

55
00:03:28.560 --> 00:03:32.460
through my work as well, when you're trying to make sense of our position,

56
00:03:32.520 --> 00:03:35.580
our possible future prospects, that leavers,

57
00:03:35.610 --> 00:03:39.500
which we might have available to affect the world. W what,

58
00:03:39.720 --> 00:03:43.590
what would be a good and bad way of pulling those leavers, then,

59
00:03:43.610 --> 00:03:46.800
then you have to try to put all of these constraints and considerations

60
00:03:46.801 --> 00:03:51.210
together. And in that context, I think it's important. Um,

61
00:03:51.540 --> 00:03:55.230
I think if you're just going about your daily existence,

62
00:03:55.260 --> 00:03:59.970
then it might not really be very useful or relevant

63
00:04:00.810 --> 00:04:04.110
to, uh, constantly like,

64
00:04:04.560 --> 00:04:09.510
try to bring in hypothesis about the nature of our reality and stuff like that.

65
00:04:09.930 --> 00:04:13.110
Because for most of the things you're doing on a day-to-day basis,

66
00:04:13.140 --> 00:04:17.750
like they work the same, whether it's inside a simulation or in basement level,

67
00:04:17.770 --> 00:04:20.250
physical relative, like you still need to get your car keys out,

68
00:04:20.251 --> 00:04:22.320
you still need to. Right. So in some sense,

69
00:04:22.321 --> 00:04:27.210
it kind of factors out and is irrelevant for many practical intents and

70
00:04:27.211 --> 00:04:28.140
purposes. What.

71
00:04:28.140 --> 00:04:30.630
<v 1>Do you remember when you started to contemplate the possibility of a summer?</v>

72
00:04:32.580 --> 00:04:35.780
<v 2>Um, Hmm, no, I, I mean, I remember when,</v>

73
00:04:35.880 --> 00:04:39.900
when the simulation argument occur to me, so, which is less, it's not just,

74
00:04:40.140 --> 00:04:42.270
I mean, as far as long as I can remember it, I can, yeah. I mean,

75
00:04:42.271 --> 00:04:45.330
maybe it's a possibility, like, oh, it could all be a dream.

76
00:04:45.331 --> 00:04:49.290
It could be a simulation, like, yeah. But, but, but that,

77
00:04:49.291 --> 00:04:51.000
that there is this specific argument that,

78
00:04:51.001 --> 00:04:55.170
that kind of narrows down the range of possibilities and where the simulation

79
00:04:55.171 --> 00:04:59.820
hypothesis is done. One of only three kind of options.

80
00:04:59.880 --> 00:05:01.380
What are the three options? Um, well,

81
00:05:01.381 --> 00:05:06.330
one is that there is all almost all civilizations at our current

82
00:05:06.331 --> 00:05:10.260
stage of technological development go extinct before reaching technological

83
00:05:10.261 --> 00:05:15.030
maturity. That's like option one kind of would be fine.

84
00:05:15.031 --> 00:05:17.310
Technological maturity, well say having developed,

85
00:05:17.640 --> 00:05:20.460
at least all those technologies that we already, uh,

86
00:05:20.520 --> 00:05:24.480
have good reason to think are physically possible. Okay.

87
00:05:24.540 --> 00:05:28.560
So that would include the technology to build extremely large and powerful

88
00:05:28.561 --> 00:05:30.420
computers, uh,

89
00:05:30.450 --> 00:05:35.100
on which you could run detailed computer simulations of conscious

90
00:05:35.670 --> 00:05:40.350
individuals. Um, so that that's kind of,

91
00:05:40.590 --> 00:05:43.620
kind of would be a pessimistic. Like if that's, if,

92
00:05:43.770 --> 00:05:48.540
if almost all civilizations is our stage failed to get there, that's bad news.

93
00:05:48.541 --> 00:05:51.840
Right. Because we've done, we'll fail as well. Almost certainly.

94
00:05:52.080 --> 00:05:56.220
That's one possibility. Yeah. So that's option one. Uh,

95
00:05:56.430 --> 00:06:01.040
option two is that there is a very strong convergence among all technologically

96
00:06:01.041 --> 00:06:03.200
mature civilizations, uh,

97
00:06:03.260 --> 00:06:07.820
in that they all lose interest in creating ancestor simulations or these kinds

98
00:06:07.821 --> 00:06:09.990
of detailed computer simulations.

99
00:06:09.991 --> 00:06:14.490
Self-conscious people like their historical predecessors or variations. So,

100
00:06:14.491 --> 00:06:16.850
so maybe they have all of these computers that could do it,

101
00:06:17.300 --> 00:06:20.420
but for whatever reason, they all decided not to do it.

102
00:06:20.580 --> 00:06:23.960
Maybe there's an ethical imperative not to do it or some other, I mean,

103
00:06:24.110 --> 00:06:27.470
we don't really know much about these post-human human creatures and what they

104
00:06:27.471 --> 00:06:31.430
want to do and don't want to do so post human creatures I'd imagine that by the

105
00:06:31.431 --> 00:06:33.500
time they have the technology to do this, yes,

106
00:06:34.040 --> 00:06:37.610
they would also have enhanced themselves in many different ways. Right.

107
00:06:37.640 --> 00:06:38.830
And so perhaps.

108
00:06:38.860 --> 00:06:43.120
<v 1>Enhancing their ability to recognize the consequences of creating some.</v>

109
00:06:43.120 --> 00:06:43.811
<v 2>Sort of a yeah. Yeah.</v>

110
00:06:43.811 --> 00:06:46.540
That would almost certainly have cognitive enhanced themselves, for example.

111
00:06:46.660 --> 00:06:51.550
<v 1>Well, is the concept of downloading consciousness into a computer.</v>

112
00:06:52.060 --> 00:06:56.110
It almost ensures that there's going to be some type of simulation. If you're,

113
00:06:56.111 --> 00:06:59.020
if you have the ability to download consciousness into a computer,

114
00:06:59.021 --> 00:07:02.140
once it's contained into this computer, what,

115
00:07:02.230 --> 00:07:05.440
what is what's to stop it from existing there,

116
00:07:06.070 --> 00:07:07.420
as long as there's power.

117
00:07:07.840 --> 00:07:12.670
And as long as these chips are firing and electricity is

118
00:07:12.671 --> 00:07:15.550
being transferred, data is being moved back and forth.

119
00:07:15.940 --> 00:07:19.000
You would essentially be in some sort of a simulation.

120
00:07:20.200 --> 00:07:23.350
<v 2>Well, I mean, if you have the capability to do that and also the motor yeah.</v>

121
00:07:23.770 --> 00:07:28.420
<v 1>It would have to simulate something that resembles some sort of a</v>

122
00:07:28.421 --> 00:07:31.540
biological interface. Otherwise it's not going to know what to do. Right. Yeah.

123
00:07:32.440 --> 00:07:33.850
<v 3>So, so.</v>

124
00:07:34.990 --> 00:07:35.720
<v 2>Um,</v>

125
00:07:35.720 --> 00:07:39.490
so we have these kind of virtual irrelative environments now that are imperfect,

126
00:07:39.520 --> 00:07:43.320
but improving. And you could kind of imagine that they get better and better,

127
00:07:43.540 --> 00:07:46.360
then you'll have a perfect virtual reality environment. Um,

128
00:07:46.540 --> 00:07:48.370
but imagine also that your brain,

129
00:07:48.470 --> 00:07:53.260
instead of sitting in a box with big headphones on some glasses on like that the

130
00:07:53.261 --> 00:07:57.670
brain itself also could be part of the simulation, the matrix. Uh, well,

131
00:07:57.671 --> 00:08:01.080
I think in the matrix, there are biological humans outside of plug-in. Right,

132
00:08:01.330 --> 00:08:01.870
right.

133
00:08:01.870 --> 00:08:06.670
But if you could include in the simulation just as you

134
00:08:06.671 --> 00:08:09.730
have maybe simulated coffee mugs and Carson, et cetera,

135
00:08:09.731 --> 00:08:13.800
you could have simulated brains that it, and,

136
00:08:13.801 --> 00:08:18.610
and so here is one assumption coming in from outside the

137
00:08:18.611 --> 00:08:21.010
simulation argument, and one can talk about it separately,

138
00:08:21.070 --> 00:08:22.990
but it's the idea that, um,

139
00:08:23.050 --> 00:08:28.030
I called it the substrate independence thesis that you could in principle have

140
00:08:28.540 --> 00:08:33.280
conscious experiences implemented, um, on different substrate.

141
00:08:33.281 --> 00:08:35.290
It doesn't have to be carbon atoms as it,

142
00:08:35.710 --> 00:08:37.060
as it's the case where the human brain,

143
00:08:37.061 --> 00:08:41.290
it could be Silicon atoms that that creates conscious experiences is some kind

144
00:08:41.291 --> 00:08:45.310
of structural feature of the computation that is being performed, uh,

145
00:08:45.340 --> 00:08:49.390
rather than the material that is used to underpin it.

146
00:08:49.810 --> 00:08:50.890
So in that case,

147
00:08:51.220 --> 00:08:54.790
you could have a simulation with detailed simulations of brains in it where

148
00:08:54.791 --> 00:08:57.270
maybe neuron and signups disseminated,

149
00:08:57.271 --> 00:09:00.270
and then those sprints would be conscious.

150
00:09:00.810 --> 00:09:02.760
<v 1>So accessibility number two. Well.</v>

151
00:09:02.940 --> 00:09:06.480
<v 2>No. So the possibility number two is that these tumors just are not at all</v>

152
00:09:06.481 --> 00:09:09.660
interested in doing it and not just that, some of them don't,

153
00:09:09.780 --> 00:09:13.200
but like all these civilizations that reach technological maturity,

154
00:09:13.440 --> 00:09:17.670
that's kind of pretty uniformly. Just don't do that. And what's number three,

155
00:09:18.150 --> 00:09:20.490
uh, that we are in a simulation, the simulation hypothesis.

156
00:09:21.300 --> 00:09:22.680
<v 1>And where do you lean?</v>

157
00:09:23.040 --> 00:09:23.611
<v 2>Well, I,</v>

158
00:09:23.611 --> 00:09:28.350
I didn't really tend to punt on the question of precise probabilities there.

159
00:09:28.351 --> 00:09:31.020
I mean, I think it would be a probability thing, right? Yes.

160
00:09:31.260 --> 00:09:34.770
It assigned some to each, but, um, yeah, I I've,

161
00:09:34.771 --> 00:09:39.150
I've refrained from giving a very precise number part,

162
00:09:39.151 --> 00:09:42.060
partly because, I mean, if I said some particular number,

163
00:09:42.061 --> 00:09:45.420
it would get cultivated and it would create this maybe a sense of false

164
00:09:45.421 --> 00:09:50.010
precision. Um, the argument doesn't allow it to derive this,

165
00:09:50.011 --> 00:09:51.570
the probabilities XYZ.

166
00:09:51.610 --> 00:09:55.620
It's just that at least one of these three has to obtain.

167
00:09:57.210 --> 00:10:00.390
Um, so yeah, so that, that narrows it down. Like, because you might think,

168
00:10:01.470 --> 00:10:03.060
you know, why do we know the future is big?

169
00:10:03.120 --> 00:10:06.280
You could just make up any story and we have no evidence for it.

170
00:10:06.440 --> 00:10:07.650
It seems that there actually,

171
00:10:07.651 --> 00:10:11.610
if you start to think everything through quite tight constraints, um,

172
00:10:11.670 --> 00:10:15.000
on what probabilistically coherent views you could have,

173
00:10:15.001 --> 00:10:19.350
and it's kind of hard even to find one overall hypothesis that fits

174
00:10:19.650 --> 00:10:23.670
this and various other considerations that, that we think we know.

175
00:10:24.410 --> 00:10:29.360
<v 1>The idea would be that if there is one day the ability to create a</v>

176
00:10:29.361 --> 00:10:33.920
simulation, that it would be indiscernible from reality itself.

177
00:10:34.170 --> 00:10:36.950
That if like, say if we are not in a simulation yet,

178
00:10:37.130 --> 00:10:38.660
if this is just biological life,

179
00:10:38.690 --> 00:10:41.510
we're just extremely fortunate to be in this Goldilocks period.

180
00:10:42.920 --> 00:10:47.900
But we're working on virtual reality in terms of like Oculus and all

181
00:10:47.901 --> 00:10:51.860
these companies are creating these consumer-based virtual reality things that

182
00:10:51.861 --> 00:10:52.970
are getting better and better,

183
00:10:52.971 --> 00:10:56.510
and really kind of interesting that you got to imagine that 20 years ago there

184
00:10:56.511 --> 00:11:01.070
was nothing like that 20 years from now, it might be indiscernible. You might,

185
00:11:01.130 --> 00:11:04.880
you might be able to create a virtual reality.

186
00:11:05.240 --> 00:11:07.130
That's impossible to,

187
00:11:07.980 --> 00:11:10.880
to discern from the reality that we're currently experiencing.

188
00:11:11.280 --> 00:11:13.520
<v 2>Or, or maybe 20,000 years. So 20 million years,</v>

189
00:11:13.521 --> 00:11:17.420
like the argument makes no assumption at all about how long it will take. Yeah.

190
00:11:17.990 --> 00:11:22.760
<v 1>But one day, if things continued to improve computational power,</v>

191
00:11:22.910 --> 00:11:27.800
the ability to replicate experiences and even feed back in terms of

192
00:11:27.801 --> 00:11:30.770
like biological feedback, touch and feel, and smell,

193
00:11:31.340 --> 00:11:33.080
if they figure out a way to do that one day,

194
00:11:33.081 --> 00:11:36.590
they will have an artificial reality.

195
00:11:36.591 --> 00:11:39.920
That's indiscernible from reality itself. And if that is the case,

196
00:11:40.130 --> 00:11:42.260
how do we know if we're in it, right?

197
00:11:42.830 --> 00:11:47.390
<v 2>That, that is roughly the, the, the gist of it. Now I,</v>

198
00:11:47.391 --> 00:11:51.290
as I said, I think, um, if you simulate the brain also,

199
00:11:53.440 --> 00:11:53.470
uh,

200
00:11:53.470 --> 00:11:58.360
you have a cheaper overall system than if you have a biological component in the

201
00:11:58.361 --> 00:12:01.150
center, it's surrounded by virtual reality gear.

202
00:12:03.190 --> 00:12:05.470
And so you could for a given cost,

203
00:12:06.340 --> 00:12:10.720
I think create many more ancestor simulations with simulated brains in them,

204
00:12:10.750 --> 00:12:15.700
rather than biological brains with VR gear solved most in these

205
00:12:15.701 --> 00:12:18.250
scenarios where that would be a lot of simulations, most of those scenarios,

206
00:12:18.310 --> 00:12:22.030
which would be, uh, the, the, the kind of where everything is digital,

207
00:12:22.600 --> 00:12:26.140
because it's just cheaper with mature technology to do it that way.

208
00:12:27.900 --> 00:12:32.880
<v 1>This is, uh, one of the biggest, for lack of a better terms,</v>

209
00:12:33.300 --> 00:12:36.870
mind when you really stop and think about reality itself,

210
00:12:36.871 --> 00:12:41.850
that if we are living in a simulation, like what, what is it and why,

211
00:12:42.750 --> 00:12:47.100
and where does it go and how do I respond?

212
00:12:47.430 --> 00:12:51.240
How do I move forward? If I really do believe this is a simulation? W what,

213
00:12:51.241 --> 00:12:55.440
w what am I doing here? Yeah. That was a big questions, huge questions.

214
00:12:56.850 --> 00:12:59.610
<v 2>Um, not some of them arise, even if we're not in a simulation.</v>

215
00:13:00.060 --> 00:13:04.710
<v 1>Yeah. And aren't there people that have done some strange,</v>

216
00:13:04.740 --> 00:13:08.940
impossible to understand calculations that are designed to determine whether or

217
00:13:08.941 --> 00:13:12.240
not there's a likelihood of us being involved in a simulation currently.

218
00:13:12.960 --> 00:13:16.290
<v 2>Yeah. I, I think it slightly misses the point. So the,</v>

219
00:13:18.180 --> 00:13:19.013
I think,

220
00:13:19.290 --> 00:13:23.850
so there are these attempts to try to figure out the computational

221
00:13:23.851 --> 00:13:25.620
requirements, uh,

222
00:13:25.621 --> 00:13:30.390
that would be required if you wanted to simulate some physical system

223
00:13:30.660 --> 00:13:31.860
with perfect precision.

224
00:13:33.780 --> 00:13:34.613
<v 3>Saw.</v>

225
00:13:35.220 --> 00:13:38.710
<v 2>Um, if we have some human or brain, a room that to say, and if we want to,</v>

226
00:13:39.240 --> 00:13:43.740
dissimulate every little part, every atom, every subatomic particle,

227
00:13:43.770 --> 00:13:47.580
the whole quantum wave function. Um,

228
00:13:49.390 --> 00:13:51.450
and what would be the computation, a load of that,

229
00:13:52.020 --> 00:13:55.410
and would it be possible to build a computer powerful enough that you could

230
00:13:55.411 --> 00:13:58.830
actually do this now?

231
00:13:58.860 --> 00:14:02.490
I think the way that this misses the point is that, um,

232
00:14:02.640 --> 00:14:07.470
it's not necessary to simulate all the details of this

233
00:14:07.471 --> 00:14:11.610
environment that you want to create in an ancestry simulation.

234
00:14:11.611 --> 00:14:16.390
You would only have to simulate it in so far as it is perceptible to the

235
00:14:16.590 --> 00:14:19.260
observer inside of the simulation. So if,

236
00:14:19.710 --> 00:14:23.820
if some post-human civilization wanted to create, uh, Joe Rogan, uh,

237
00:14:23.850 --> 00:14:28.740
doing a podcast simulation that needs to simulate Joe Rogan

238
00:14:28.741 --> 00:14:31.140
sprain, because that's where the experiences happened.

239
00:14:31.800 --> 00:14:35.370
And then whatever parts of the environment that you are able to perceive.

240
00:14:35.880 --> 00:14:39.870
So surface appearances, maybe off the table on walls Navy,

241
00:14:39.871 --> 00:14:41.490
they would need to simulate me as well,

242
00:14:41.520 --> 00:14:45.030
or at least a good enough simulacrum that I could sort of spit out words that

243
00:14:45.031 --> 00:14:48.570
would sound like they came from a real human, right.

244
00:14:48.600 --> 00:14:52.610
I don't know that now we're getting quite good with this GPT to like these kinds

245
00:14:52.611 --> 00:14:56.090
of AI that just spews out words with,

246
00:14:57.380 --> 00:15:00.320
I don't know whether anyway, so, so you'd have with like,

247
00:15:00.340 --> 00:15:03.560
what is happening inside this table right now is completely irrelevant.

248
00:15:03.600 --> 00:15:07.020
You have no idea of knowing whether there even are atoms terror. Now,

249
00:15:07.040 --> 00:15:12.030
now you could take a big electron microscope, um, and,

250
00:15:12.031 --> 00:15:13.970
and look at find finer structure. And then,

251
00:15:13.971 --> 00:15:16.130
then you could take an atomic force microscope,

252
00:15:16.131 --> 00:15:17.810
and you could see individual atoms even,

253
00:15:17.900 --> 00:15:19.460
and you could perform all kinds of measurements.

254
00:15:20.120 --> 00:15:22.760
And it might be important that if you did that,

255
00:15:22.790 --> 00:15:26.060
you wouldn't see anything weird because physicists do these experiments and they

256
00:15:26.070 --> 00:15:28.760
don't say anything weird, but then you could kind of fill in those details,

257
00:15:28.820 --> 00:15:32.450
like if, and when somebody were performing those experiments,

258
00:15:32.451 --> 00:15:35.720
that would be vastly cheaper than continuously running all of this.

259
00:15:36.260 --> 00:15:40.700
And so this is the way a lot of computer games are designed today that they have

260
00:15:40.701 --> 00:15:42.260
a certain rendering distance.

261
00:15:42.290 --> 00:15:45.860
Like you only like actually simulate the virtual world when,

262
00:15:45.861 --> 00:15:49.580
when the character goes close enough, that you could see it. Right. Um,

263
00:15:49.640 --> 00:15:52.730
and so you might have these kinds of super intelligent post humans doing this.

264
00:15:52.731 --> 00:15:57.110
Obviously they would have figured that out and a lot of other optimizations. So,

265
00:15:57.111 --> 00:16:00.440
so these, in other words, these calculations are experiments.

266
00:16:00.441 --> 00:16:03.380
I think don't really tell on the hypothesis.

267
00:16:05.500 --> 00:16:10.030
<v 1>Without assigning a probability to either one of those three scenarios.</v>

268
00:16:10.450 --> 00:16:13.930
What makes you think when you,

269
00:16:13.931 --> 00:16:17.180
if you do stop and think, I think we're in a simulation, what,

270
00:16:17.181 --> 00:16:20.920
what are the things that are convincing to you? Well.

271
00:16:20.920 --> 00:16:23.990
<v 2>It would mainly go through the, the simulation argument that if,</v>

272
00:16:23.991 --> 00:16:27.880
if to the extent that I think the alternative to hypothesis are improbable,

273
00:16:28.990 --> 00:16:32.980
then that would kind of shift the probability mass on the third remaining.

274
00:16:33.220 --> 00:16:33.760
Is it really only.

275
00:16:33.760 --> 00:16:37.240
<v 1>Three? So the, the, the ones are we,</v>

276
00:16:37.330 --> 00:16:39.010
that human beings go extinct.

277
00:16:39.260 --> 00:16:43.870
<v 2>And the other civilizations. So at our stage in the cosmos or whatever,</v>

278
00:16:44.110 --> 00:16:47.290
yes, this is strong. Yeah. Strong filter.

279
00:16:48.100 --> 00:16:48.250
<v 1>Okay.</v>

280
00:16:48.250 --> 00:16:51.820
That they either go extinct or they decide not to pursue all those same tests,

281
00:16:52.000 --> 00:16:54.820
or it becomes a simulation. Is that really the only thing? Well, I.

282
00:16:54.850 --> 00:16:59.350
<v 2>Think the only three live options now, so you can, I, I can get,</v>

283
00:16:59.400 --> 00:17:02.680
you can kind of unfold the argument a little bit more and look more granular.

284
00:17:02.681 --> 00:17:05.830
So I suppose that the first two options are false.

285
00:17:05.860 --> 00:17:10.450
So some non-trivial fraction of civilizations that are stage to get through.

286
00:17:11.230 --> 00:17:14.470
And some non-trivial fraction of those are still interested.

287
00:17:16.240 --> 00:17:17.073
<v 3>Then.</v>

288
00:17:17.830 --> 00:17:20.410
<v 2>I think you can convince English, show that, um,</v>

289
00:17:20.830 --> 00:17:24.970
by using just a small portion of their resources, they could create very,

290
00:17:24.971 --> 00:17:26.470
very many simulations.

291
00:17:27.280 --> 00:17:32.260
And you can show that or argue for that by comparing the computational power

292
00:17:32.920 --> 00:17:36.310
of systems that we know are physically possible to build.

293
00:17:38.230 --> 00:17:39.250
Um, we can't currently build them,

294
00:17:39.251 --> 00:17:41.260
but we could see that you could build them with nanotech.

295
00:17:41.261 --> 00:17:45.100
And if you have planetary sized in our resources on the one hand,

296
00:17:45.101 --> 00:17:46.120
and on the other hand,

297
00:17:46.420 --> 00:17:50.400
estimates of how much compute power it would take to a human brain,

298
00:17:52.350 --> 00:17:55.650
uh, and, and you find that a mature civilization would have many,

299
00:17:55.651 --> 00:17:59.910
many orders of magnitude more so that even if they just use 1% of their compute

300
00:17:59.911 --> 00:18:04.410
power of one planet for one minute, they could still run, you know,

301
00:18:05.010 --> 00:18:07.410
thousands and thousands and thousands of these simulations.

302
00:18:07.470 --> 00:18:10.980
And they might have billions of planets and they might last for billions of

303
00:18:10.981 --> 00:18:15.570
years. So, so the numbers are quite extreme. It seems. Um, so then,

304
00:18:15.600 --> 00:18:16.440
then what you get this,

305
00:18:16.441 --> 00:18:20.970
this implication that if the first two options are false,

306
00:18:21.000 --> 00:18:24.150
it would follow that that would be many,

307
00:18:24.151 --> 00:18:28.320
many more simulated experiences of our account.

308
00:18:28.350 --> 00:18:31.320
Then they would be original experiences of our account.

309
00:18:32.390 --> 00:18:35.810
<v 1>So the idea is that if we continue to innovate, if, if,</v>

310
00:18:35.830 --> 00:18:40.160
if human beings or intelligent life in the cosmos continues to innovate that

311
00:18:40.670 --> 00:18:44.990
creating a simulation is almost inevitable. No, no. I mean,

312
00:18:45.020 --> 00:18:46.430
the second might be a.

313
00:18:47.450 --> 00:18:50.150
<v 2>Yeah. And, and others with the same capability.</v>

314
00:18:50.630 --> 00:18:55.310
<v 1>They don't decide not to, if they don't decide to not choose, if we,</v>

315
00:18:55.490 --> 00:18:58.730
the first option, if human beings do, uh,

316
00:18:58.760 --> 00:19:01.700
figure out a way to not die and stay,

317
00:19:02.030 --> 00:19:03.860
stay innovative,

318
00:19:03.861 --> 00:19:08.630
and we don't have any sort of natural disasters or manmade created disasters,

319
00:19:09.080 --> 00:19:12.740
then step two, if we don't,

320
00:19:13.820 --> 00:19:15.950
we don't decide to not pursue this.

321
00:19:16.040 --> 00:19:20.750
If we continue to pursue all various forms of technological

322
00:19:20.751 --> 00:19:24.410
innovation, including simulations, that it becomes inevitable.

323
00:19:25.520 --> 00:19:28.730
If we get past those two first options,

324
00:19:29.000 --> 00:19:33.140
it becomes inevitable that we pursue it well.

325
00:19:33.140 --> 00:19:37.390
<v 2>So if they have the capacity, then they will do it. And the, the,</v>

326
00:19:37.391 --> 00:19:40.580
the motive or the desire to do so that then,

327
00:19:40.581 --> 00:19:43.790
then that would create hugely many of these.

328
00:19:44.540 --> 00:19:46.160
So not just one simulation, right.

329
00:19:46.190 --> 00:19:48.890
But because it's so cheap at the technological maturity,

330
00:19:48.891 --> 00:19:53.110
if you have a cosmic empire of resources, um, it, it,

331
00:19:53.260 --> 00:19:56.840
it don't have to have a very big decide to do this. They might just think, well,

332
00:19:56.900 --> 00:19:59.210
you know, um, well, that.

333
00:19:59.210 --> 00:20:02.600
<v 1>Was the big question that Elan said he would ask artificial intelligence said</v>

334
00:20:02.601 --> 00:20:06.770
what's beyond the simulation. Okay. That's the real question.

335
00:20:06.771 --> 00:20:09.620
If this is a simulation, if there's many,

336
00:20:09.621 --> 00:20:14.300
many simulations running currently what's beyond the simulation. Well.

337
00:20:14.340 --> 00:20:16.040
<v 2>Yeah. You might be curious about that. I mean,</v>

338
00:20:16.041 --> 00:20:19.340
I think the more important question would be like, what do we,

339
00:20:19.341 --> 00:20:24.230
all things considered, uh, have the most recent to do in our situation? Like,

340
00:20:24.231 --> 00:20:27.530
what would it be wise for us to do? Is that like some way that we can be,

341
00:20:29.420 --> 00:20:32.930
um, helpful or have the best life or whatever you.

342
00:20:33.140 --> 00:20:36.380
<v 1>Call that ridiculous to even consider, maybe it's beyond us.</v>

343
00:20:38.000 --> 00:20:39.710
What the question of what is outside?

344
00:20:39.740 --> 00:20:43.460
<v 3>Yes. Well, I mean.</v>

345
00:20:43.460 --> 00:20:47.120
<v 2>I don't think it's ridiculous to consider, I think, uh, it might be beyond,</v>

346
00:20:47.290 --> 00:20:51.550
but when maybe we would be able to form some abstract conception of what it is.

347
00:20:51.551 --> 00:20:53.260
I mean, in fact, if, if we, if, if there,

348
00:20:53.410 --> 00:20:57.460
if the path to believe the simulation hypothesis is the simulation argument.

349
00:20:57.610 --> 00:21:01.090
I mean, we have a bunch of structure there that gives us some idea I'd like,

350
00:21:01.091 --> 00:21:05.950
that would be some advanced civilization that they would have

351
00:21:06.460 --> 00:21:08.560
developed a lot of technology over time,

352
00:21:09.070 --> 00:21:13.990
including compute technology ability to do virtual reality for

353
00:21:13.991 --> 00:21:15.100
a while we demand.

354
00:21:15.101 --> 00:21:18.160
And probably they would have used our technology for a whole host of other

355
00:21:18.161 --> 00:21:21.970
purposes as well. You wouldn't just get that technology and, you know,

356
00:21:22.240 --> 00:21:25.690
not be able to create a train or something like that. As I got there,

357
00:21:25.760 --> 00:21:30.010
they'd probably be super intolerance and have the ability to colonize the

358
00:21:30.011 --> 00:21:33.670
universe and do a whole host of other things. Um,

359
00:21:33.700 --> 00:21:35.170
and then for one reason or another,

360
00:21:35.171 --> 00:21:37.470
they would have decided to use some of the resources to,

361
00:21:37.471 --> 00:21:41.560
to create simulations and inside one of those simulations, perhaps,

362
00:21:43.080 --> 00:21:47.830
uh, our experiences would be taking place. Um, so, and then you could,

363
00:21:47.831 --> 00:21:51.580
you could more speculative to fill in more detailed there, but,

364
00:21:51.610 --> 00:21:53.530
but I still think that fundamentally, uh,

365
00:21:54.040 --> 00:21:57.490
our ability to grok this whole thing would be very limited.

366
00:21:57.850 --> 00:22:02.500
And that might be other considerations that

367
00:22:02.770 --> 00:22:05.380
we are oblivious to. That would change. I mean,

368
00:22:05.381 --> 00:22:09.130
if you think about the simulation argument, is it quite recent? Right.

369
00:22:09.520 --> 00:22:14.410
So it's like, it's only less than 20 years old. So if you take that,

370
00:22:14.870 --> 00:22:18.280
so suppose it's correct, for the sake of argument, then up to this point,

371
00:22:18.430 --> 00:22:22.420
everybody was missing something like hugely important and fundamental. Yeah.

372
00:22:22.510 --> 00:22:26.830
Right. Very smart people, hundreds of years, like, yes,

373
00:22:26.960 --> 00:22:29.200
like this, this massive piece right. In the center,

374
00:22:29.650 --> 00:22:33.010
but what's the chances that we now have figured out the last big missing piece.

375
00:22:33.160 --> 00:22:36.340
Like, presumably that must be some father big,

376
00:22:36.400 --> 00:22:39.850
giant realization that that is like beyond us currently.

377
00:22:40.150 --> 00:22:44.230
So I think having some, yeah, I mean, that, that looks kind of possible,

378
00:22:44.231 --> 00:22:48.130
but maybe there are further bigger discoveries or revelations that that would

379
00:22:48.131 --> 00:22:50.710
kind of maybe not falsify the simulation,

380
00:22:50.711 --> 00:22:55.000
but maybe change the interpreter or like do something that is hard to know in

381
00:22:55.001 --> 00:22:56.170
advance. But that would be, it was.

382
00:22:56.170 --> 00:23:00.130
<v 1>The concept that if there is a simulation that all the stork will record is,</v>

383
00:23:00.380 --> 00:23:04.570
is simulated as well. Well, or when did it take in? Wow.

384
00:23:04.660 --> 00:23:06.070
<v 2>So there are different options there, right.</v>

385
00:23:06.071 --> 00:23:10.690
And there might be many different simulations that, uh, configure differently.

386
00:23:10.720 --> 00:23:14.230
That could be ones that run for a very long time.

387
00:23:14.231 --> 00:23:15.700
Once that's run for a short period of time,

388
00:23:15.701 --> 00:23:17.950
once that simulates everything and everybody,

389
00:23:18.340 --> 00:23:22.240
others that just focus on some particular scene or person,

390
00:23:22.810 --> 00:23:25.090
like it's just a vast space of possibilities there.

391
00:23:26.650 --> 00:23:28.510
And which ones of those would be most likely,

392
00:23:28.511 --> 00:23:31.090
is it really hard to say much about,

393
00:23:31.091 --> 00:23:33.640
because it would depend on the reasons for creating these simulations?

394
00:23:33.641 --> 00:23:37.710
Like what would the interests of these hypothetical post-human speed. Yeah.

