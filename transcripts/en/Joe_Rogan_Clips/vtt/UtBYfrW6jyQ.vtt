WEBVTT

1
00:00:01.020 --> 00:00:01.761
<v 0>The Joe Rogan experience.</v>

2
00:00:01.761 --> 00:00:06.280
There has been research done on making artificial insects that have like little

3
00:00:06.281 --> 00:00:11.080
cameras inside of them. Yep. That look like, like a dragonfly or some,

4
00:00:11.190 --> 00:00:14.120
some sort of bug and they fly around and they could film things.

5
00:00:14.500 --> 00:00:17.640
<v 1>And the thing that terrifies a lot of people is going more microscopic than</v>

6
00:00:17.641 --> 00:00:22.350
that. More like, uh, robots inside the body that's help you cure disease.

7
00:00:22.470 --> 00:00:25.830
And so on cope certain things, even at the nano scale.

8
00:00:27.970 --> 00:00:31.830
So basically creating viruses. Yeah. Creating new viruses, that little.

9
00:00:31.830 --> 00:00:32.580
<v 0>Tiny ones.</v>

10
00:00:32.580 --> 00:00:36.950
<v 1>Yeah. And they, if they learn, they can be pretty dumb, but on a mass scale</v>

11
00:00:38.540 --> 00:00:42.740
dumb, you don't have to be intelligent to destroy all of, of civilization. So,

12
00:00:43.020 --> 00:00:43.260
uh.

13
00:00:43.260 --> 00:00:46.980
<v 0>&lt;Laugh&gt; so the real question about this artificial intelligence stuff that</v>

14
00:00:46.981 --> 00:00:50.740
everybody seems to the ultimate end of the line, the,

15
00:00:50.741 --> 00:00:52.300
what Sam Harris is terrified of,

16
00:00:52.880 --> 00:00:57.820
is it becoming sentient and it making its own decisions and deciding that

17
00:00:57.821 --> 00:01:00.980
we don't need people. That's what everybody's really scared of. Right.

18
00:01:01.690 --> 00:01:02.523
&lt;affirmative&gt;.

19
00:01:03.250 --> 00:01:06.610
<v 1>I, I'm not sure if everybody's scared of it. Yeah. They might be.</v>

20
00:01:06.770 --> 00:01:11.410
I think that's a story that's the most compelling, the sexiest story that,

21
00:01:11.850 --> 00:01:16.410
uh, the philosopher side of a Sam Harris is very, is very attracted to. Yeah.

22
00:01:17.010 --> 00:01:21.170
Uh, I am also interested in that story, but I think achieving Saint,

23
00:01:22.240 --> 00:01:25.280
I think that requires also creating consciousness.

24
00:01:25.680 --> 00:01:30.280
I think that that requires creating the kind of intelligence and cognition and

25
00:01:30.281 --> 00:01:33.280
reasoning abilities. That's really, really difficult.

26
00:01:33.840 --> 00:01:38.080
I think we'll create dangerous software based systems before then there'll be a

27
00:01:38.081 --> 00:01:42.400
huge threat. I think we already have them, the YouTube, a,

28
00:01:42.690 --> 00:01:43.170
the Twitter,

29
00:01:43.170 --> 00:01:47.910
the recommender systems of Twitter and Facebook and YouTube from everything I

30
00:01:47.911 --> 00:01:51.470
know having talked to those folks, having worked on it, the,

31
00:01:51.690 --> 00:01:56.430
the challenging aspect there is they have the power to control

32
00:01:57.840 --> 00:02:02.790
minds, the mass sort of what the mass population thinks and

33
00:02:03.570 --> 00:02:08.380
YouTube itself and Twitter itself don't have direct ability to

34
00:02:08.381 --> 00:02:11.460
control the algorithm. Exactly. Like the one,

35
00:02:11.461 --> 00:02:14.300
they don't have a way to understand the algorithm and two,

36
00:02:14.301 --> 00:02:16.580
they don't have a way to control it because,

37
00:02:17.320 --> 00:02:22.020
but what I mean by control is control it in a way that leads to

38
00:02:23.300 --> 00:02:27.810
in a aggregate, a better civilization, meaning like sort of, uh,

39
00:02:27.811 --> 00:02:29.810
the Stephen pink or the better angio V nature.

40
00:02:29.950 --> 00:02:31.810
So encourag the better sides of ourselves.

41
00:02:32.280 --> 00:02:36.570
It's very difficult to control a single algorithm that

42
00:02:37.180 --> 00:02:39.010
recommends the,

43
00:02:39.750 --> 00:02:43.610
the journey of millions of people through the space of the internet.

44
00:02:43.720 --> 00:02:45.280
It's very difficult to control that.

45
00:02:45.540 --> 00:02:50.520
And I think that intelligence instilled in those algorithms will have a

46
00:02:50.521 --> 00:02:54.880
much more potentially either positive or detrimental effect than

47
00:02:55.400 --> 00:02:56.840
sentient killer robots.

48
00:02:57.320 --> 00:03:01.240
I hope we get to sentient killer robots &lt;laugh&gt; because that problem,

49
00:03:01.640 --> 00:03:04.240
I think we can work with. I I'm,

50
00:03:04.580 --> 00:03:09.480
I'm very optimistic about the positive aspects of approaching sentience of

51
00:03:09.680 --> 00:03:13.360
approaching general intelligence. There's going to be a huge amount of benefit.

52
00:03:14.220 --> 00:03:19.110
And I think there will be a there's a lot of mechanism can protect

53
00:03:19.111 --> 00:03:22.670
against that going wrong just from knowing the,

54
00:03:23.490 --> 00:03:28.270
we know how to control intelligent systems when

55
00:03:28.271 --> 00:03:31.270
they are sort of in a box when they're singular systems,

56
00:03:31.500 --> 00:03:33.790
when they're distributed across millions of people.

57
00:03:34.170 --> 00:03:37.390
And there's not a single control point that becomes really difficult. Mm.

58
00:03:37.810 --> 00:03:40.100
And that's, that's the way worry for me is, um,

59
00:03:41.040 --> 00:03:45.820
the distributed nature of dumb algorithms on every single phone sort of,

60
00:03:46.090 --> 00:03:49.220
sort of controlling the behavior, adjusting the behavior,

61
00:03:49.650 --> 00:03:54.100
adjusting the learning journey of different individuals. So like, to me,

62
00:03:54.120 --> 00:03:57.620
the biggest worry and the more exciting things is recommender systems.

63
00:03:57.621 --> 00:04:02.210
What they're called at Twitter at Facebook at YouTube, um,

64
00:04:02.211 --> 00:04:05.890
YouTube, especially that one, that one has just like,

65
00:04:05.930 --> 00:04:08.970
I think you mentioned there's something special about videos in terms of

66
00:04:09.000 --> 00:04:13.650
educating and sometimes indoctrinating people. Yeah. And YouTube

67
00:04:15.950 --> 00:04:19.290
has the hardest time. I mean,

68
00:04:19.600 --> 00:04:23.640
they such a difficult problem on their hands in terms of,

69
00:04:24.380 --> 00:04:28.480
in terms of that recommendation because they don't, uh, this is a,

70
00:04:28.910 --> 00:04:30.320
this is a machine learning problem,

71
00:04:30.860 --> 00:04:35.040
but knowing the contents of tweets is much easier than knowing the contents of

72
00:04:35.440 --> 00:04:36.480
videos. Mm. Like we,

73
00:04:36.620 --> 00:04:39.640
our algorithms are really dumb in terms of being able to watch a video

74
00:04:39.641 --> 00:04:41.430
understand what's being talked about. Right.

75
00:04:41.450 --> 00:04:45.390
So all it's all YouTube was looking at is the title and the description.

76
00:04:45.930 --> 00:04:50.390
And that's it mostly the title it's, it's like basically keyword searching.

77
00:04:50.540 --> 00:04:52.430
Yeah. And it's looking at the,

78
00:04:52.690 --> 00:04:56.470
at the clicking viewing behavior of the different people.

79
00:04:56.850 --> 00:04:59.830
So like it figures out that the flat earth, uh,

80
00:04:59.840 --> 00:05:02.420
supporters enjoy these kinds of videos,

81
00:05:02.421 --> 00:05:05.620
it forms a different kind of cluster and, you know,

82
00:05:05.621 --> 00:05:07.860
makes decisions based on that, by the way,

83
00:05:08.200 --> 00:05:11.460
it seems to make definitive decisions about, you know,

84
00:05:11.680 --> 00:05:14.140
it doesn't like flat earth YouTube, I think, well.

85
00:05:14.140 --> 00:05:16.100
<v 0>YouTube in particular are,</v>

86
00:05:16.101 --> 00:05:20.740
they're trying to do something about the influx of conspiracy theory,

87
00:05:20.840 --> 00:05:25.170
videos. Yeah. And the indoctrination aspect of them that,

88
00:05:25.750 --> 00:05:29.050
you know, one of the things about videos is like, say, if someone makes a video

89
00:05:30.830 --> 00:05:35.170
and, um, they make it on a very particular subject and they speak eloquently and

90
00:05:35.171 --> 00:05:37.890
articulately, but they're wrong about everything they're saying,

91
00:05:37.891 --> 00:05:38.970
they don't understand the science.

92
00:05:39.070 --> 00:05:40.930
Say if they're talking about artificial intelligence,

93
00:05:41.200 --> 00:05:45.680
they're saying something thing about things that you are an expert in. They,

94
00:05:45.710 --> 00:05:46.543
they could,

95
00:05:46.550 --> 00:05:50.560
without being checked without someone like you in the room that says that's not

96
00:05:50.760 --> 00:05:55.320
possible because of X, Y, and Z, without that they can just keep talking.

97
00:05:56.140 --> 00:05:57.480
So one of the things they do,

98
00:05:57.481 --> 00:06:01.200
whether it's about flat earth or whether it's about dinosaurs being fake or

99
00:06:01.201 --> 00:06:02.480
nuclear bombs being fake,

100
00:06:03.150 --> 00:06:06.760
they can just say these things and they do it with a,

101
00:06:06.761 --> 00:06:11.520
an excellent grasp of the English language. Right? So they say it they're,

102
00:06:11.521 --> 00:06:13.280
they're very compelling in the way they speak.

103
00:06:13.710 --> 00:06:18.550
They'll show you pictures and images and if are not very educated and you

104
00:06:18.551 --> 00:06:20.830
don't understand that this is nonsense. And if you're, especially,

105
00:06:20.831 --> 00:06:23.270
if you're not skeptical, you can get roped in. Yeah.

106
00:06:23.271 --> 00:06:25.550
You can get roped in real easy. And that's a problem.

107
00:06:26.010 --> 00:06:30.230
And it's a problem with some of the people that work in these

108
00:06:30.550 --> 00:06:35.030
platforms, their children get indoctrinated and they get angry,

109
00:06:35.360 --> 00:06:36.910
their children get indoctrine.

110
00:06:37.840 --> 00:06:42.580
Now what's interesting is they get indoctrinated also with

111
00:06:42.581 --> 00:06:43.900
right wing ideology.

112
00:06:44.040 --> 00:06:47.860
And then people get mad that they're indoctrinated by like Ben Shapiro videos.

113
00:06:48.320 --> 00:06:50.340
So they'll, they'll, they'll get off of that. Well,

114
00:06:51.360 --> 00:06:55.380
but you're okay with left wing, right. Why? Because you're left wing.

115
00:06:55.880 --> 00:07:00.490
So then it becomes like, okay, what is a problem? What's really a problem.

116
00:07:01.070 --> 00:07:05.330
And what is just something that's opposed to your personal ideology and how,

117
00:07:05.710 --> 00:07:07.170
who gets to make that distinction.

118
00:07:07.270 --> 00:07:11.890
And that is where the arguments for the first amendment come into play.

119
00:07:12.320 --> 00:07:17.130
Like should these social media companies that have massive

120
00:07:17.131 --> 00:07:18.600
amounts of power in and influence,

121
00:07:19.180 --> 00:07:23.560
should they be held to the same standards as the first amendment?

122
00:07:23.660 --> 00:07:28.440
And should these platforms be treated as essentially, uh,

123
00:07:28.560 --> 00:07:32.960
a town hall, like where anyone can speak and there's a, you know, a, a platform.

124
00:07:33.220 --> 00:07:35.880
And there's a real problem in that. There's not that many of 'em.

125
00:07:35.881 --> 00:07:38.080
This is a real problem. Yeah. The real problem is like,

126
00:07:38.660 --> 00:07:43.310
Twitter is the place where people go to argue and talk about. And then.

127
00:07:43.310 --> 00:07:46.390
<v 1>Twitter maybe has a competitor on Facebook, but YouTube certainly doesn't.</v>

128
00:07:46.390 --> 00:07:49.790
<v 0>Have a competitor. YouTube doesn't have any competitor. I mean, there's Vimeo,</v>

129
00:07:49.791 --> 00:07:54.590
there's a few other platforms, but realistically it's YouTube. Yeah.

130
00:07:54.610 --> 00:07:58.670
You know, YouTube, it's a giant giant platform.

131
00:07:59.450 --> 00:08:01.020
What is this alphabet reports?

132
00:08:01.021 --> 00:08:04.940
YouTube ad revenue for the first time video service generated

133
00:08:04.970 --> 00:08:08.860
15.1 billion in 2019. Holy.

134
00:08:09.980 --> 00:08:10.813
&lt;laugh&gt;.

135
00:08:11.760 --> 00:08:16.100
<v 2>In comparison. I just looked up, uh, Twitch ad revenue was, uh,</v>

136
00:08:16.780 --> 00:08:18.700
supposedly around 500 to 600 million.

137
00:08:19.630 --> 00:08:22.010
<v 0>That's a big difference. And what about Facebook?</v>

138
00:08:22.610 --> 00:08:27.530
Facebook is stupendously valuable probably way higher than.

139
00:08:27.530 --> 00:08:28.410
<v 2>That, but this is the first time.</v>

140
00:08:28.410 --> 00:08:29.450
<v 1>I report Facebook.</v>

141
00:08:29.530 --> 00:08:34.050
I don't pays like YouTube paid for my McDonald's burgers

142
00:08:34.051 --> 00:08:39.050
yesterday. Facebook's not right. Facebook is not in Twitter and Instagram.

143
00:08:39.170 --> 00:08:40.680
I don't think are paying you like.

144
00:08:40.920 --> 00:08:44.640
<v 0>Directly, but there's a lot of calls to break up Facebook. I'm not, I mean,</v>

145
00:08:44.660 --> 00:08:47.720
I'm on Facebook, but I'm not on it. I don't use it. I just,

146
00:08:47.790 --> 00:08:50.440
it's just connected to my Instagram. When I post something on Instagram,

147
00:08:50.441 --> 00:08:52.360
it goes to Facebook as well. I don't, I never go to.

148
00:08:52.480 --> 00:08:54.560
<v 1>Facebook. That's a Joe Rogan Facebook group. That's,</v>

149
00:08:55.880 --> 00:08:58.640
that's a dumpster fire of brilliant folks. Let me just put it that.

150
00:08:58.640 --> 00:09:03.440
<v 0>Way. &lt;laugh&gt; look at this Facebook's revenues amounted to 21.8 billion</v>

151
00:09:04.060 --> 00:09:07.160
is just the fourth quarter. Jesus Christ. Just the fourth quarter,

152
00:09:07.220 --> 00:09:09.360
the majority of which were generated through advertising.

153
00:09:09.361 --> 00:09:14.280
The company announced over 7 million active advertisers on Facebook during the

154
00:09:14.281 --> 00:09:15.750
third order of 2019. That.

155
00:09:15.910 --> 00:09:19.190
<v 2>Probably though also adds an Instagram. That thing with YouTube is just YouTube.</v>

156
00:09:19.290 --> 00:09:21.550
Not Google, not YouTube premium.

157
00:09:21.890 --> 00:09:22.723
<v 0>Not anything else.</v>

158
00:09:22.780 --> 00:09:26.710
<v 1>Just the address and, uh, to be fair. So the, the cash they have, they spend,</v>

159
00:09:27.150 --> 00:09:31.150
um, like Facebook, AI research groups, uh, some of the most brilliant,

160
00:09:31.220 --> 00:09:36.220
it's a huge group. That's doing general open ended research, Google, uh,

161
00:09:36.500 --> 00:09:39.420
research, Google brain, Google deep mind, they're doing open-ended research.

162
00:09:39.610 --> 00:09:43.900
Like they're not doing the ad stuff. They're really trying to build. It's it's,

163
00:09:43.901 --> 00:09:47.180
that's the cool thing about these companies having a lot of cash is they can

164
00:09:47.181 --> 00:09:50.540
bring some of the smartest people and let them work on whatever in case it comes

165
00:09:50.541 --> 00:09:54.420
up with a cool idea, like autonomous vehicles with Waymo. Yeah. It's like,

166
00:09:54.421 --> 00:09:57.090
let's see if we can make this work. Let's throw some money at it.

167
00:09:57.091 --> 00:10:01.170
Even if it doesn't make any money in the next 5, 10, 20 years,

168
00:10:01.260 --> 00:10:01.931
let's make it work.

169
00:10:01.931 --> 00:10:04.930
That's the positive sort of side of having that kind of money. Yeah.

170
00:10:04.930 --> 00:10:09.330
<v 0>That makes sense that there is, as long as they keep doing those kind of things.</v>

171
00:10:09.750 --> 00:10:10.930
The real concern though,

172
00:10:10.931 --> 00:10:15.610
is that they're actually severely influencing the democratic process.

173
00:10:16.900 --> 00:10:20.920
<v 1>So I, it, it is di it's difficult. I mean, certainly in Jack Dorsey,</v>

174
00:10:20.990 --> 00:10:24.360
Jack Dorsey in terms of the CEOs interact with,

175
00:10:24.400 --> 00:10:25.960
I think was one of the good guys. Yes.

176
00:10:26.120 --> 00:10:27.000
<v 0>I agree. Yeah.</v>

177
00:10:27.800 --> 00:10:28.840
<v 1>I mean, he's, uh.</v>

178
00:10:28.980 --> 00:10:30.160
<v 0>He wants a wild west Twitter.</v>

179
00:10:30.790 --> 00:10:33.400
<v 1>Well, he doesn't know he wants, he wants a good Twitter. He,</v>

180
00:10:33.401 --> 00:10:35.520
he's kind of thinking about wild west, but.

181
00:10:35.520 --> 00:10:38.950
<v 0>He wants he's his have 2 0 2 Twitter,</v>

182
00:10:39.250 --> 00:10:42.950
one that's filtered one that's like crypto, anything goes.

