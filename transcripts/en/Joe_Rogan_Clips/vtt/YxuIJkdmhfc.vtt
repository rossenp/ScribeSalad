WEBVTT

1
00:00:01.020 --> 00:00:04.680
<v 0>The Joe Rogan experience. I mean, if you do get to do this,</v>

2
00:00:04.681 --> 00:00:07.800
here's another problem. Okay. Here's a, is a big one for the ocean.

3
00:00:08.640 --> 00:00:12.280
We are depleting it of seafood of life. I mean, we, you know, I had, um,

4
00:00:12.980 --> 00:00:17.800
how do you say his name again? SA hoes, right? Louis SA hoes who, uh,

5
00:00:17.801 --> 00:00:21.070
directed the Cove on. Yeah. And we were talking about out, um,

6
00:00:21.290 --> 00:00:24.550
the replenishing replenishing of the wildlife in the, in the ocean.

7
00:00:24.970 --> 00:00:27.030
And when you start looking at it on a grand scale,

8
00:00:27.031 --> 00:00:30.710
like how much fish they're pulling outta the oceans, it's very sobering,

9
00:00:30.930 --> 00:00:31.830
you know? Sure.

10
00:00:31.960 --> 00:00:36.830
Maybe you can come up with a way to replenish fish in the ocean so

11
00:00:36.831 --> 00:00:38.830
we can continue eating sushi. What do you think?

12
00:00:39.190 --> 00:00:40.950
<v 1>&lt;Laugh&gt; so I,</v>

13
00:00:41.020 --> 00:00:44.460
so maybe just zoom out a bit and realize,

14
00:00:44.580 --> 00:00:47.540
so what is so because of course plastic pollution, you know,

15
00:00:47.541 --> 00:00:48.900
climate change overfishing,

16
00:00:49.580 --> 00:00:54.060
I think it's all part of one big problem to make civilization sustainable. And,

17
00:00:54.900 --> 00:00:58.180
um, the, the way I look at it is that, of course,

18
00:00:58.890 --> 00:01:03.850
over the past 200 years, humanity has made tremendous progress. So of course,

19
00:01:03.980 --> 00:01:07.570
since agriculture re revolution, 10,000 years ago, um,

20
00:01:08.010 --> 00:01:12.370
no has been kind of stagnant, no progress, or just very,

21
00:01:12.371 --> 00:01:16.770
very slow progress. Number of people, um, lifespan,

22
00:01:16.870 --> 00:01:21.370
it was all kind of flat, nothing really happened. And then since the,

23
00:01:21.470 --> 00:01:26.160
the Dawn of the industrial revolution and when we learned how to utilize science

24
00:01:26.460 --> 00:01:30.040
and, and our knowledge, uh, collective knowledge to, um,

25
00:01:30.300 --> 00:01:32.680
to turn that into progress, um,

26
00:01:32.710 --> 00:01:37.000
basically every possible metric for humanity has

27
00:01:37.920 --> 00:01:42.160
improved tremendously. And if you think of wealth, health, violence,

28
00:01:42.780 --> 00:01:45.990
education, uh, rights, all these things,

29
00:01:46.230 --> 00:01:48.030
I know you've had Steven Pinker on he's,

30
00:01:48.060 --> 00:01:52.950
he's much more knowledgeable on that topic than, than I am, um, yet.

31
00:01:53.350 --> 00:01:56.470
Um, so, so truly at this point in time, uh,

32
00:01:56.930 --> 00:02:01.030
it has never been a, a better time to be alive for humans than, than today.

33
00:02:01.370 --> 00:02:05.980
Not saying that it can't get better, but we have made tremendous progress,

34
00:02:06.580 --> 00:02:11.060
uh, by one hand, um, imagining things that don't exist yet.

35
00:02:11.061 --> 00:02:15.420
So inventing technologies and also inventing, uh, institutions.

36
00:02:16.200 --> 00:02:19.740
And on the other hand, uh, our human ability to,

37
00:02:19.800 --> 00:02:23.500
to collaborate effectively in large numbers, uh, which includes, you know, the,

38
00:02:23.710 --> 00:02:26.890
the corporation, which is a very effective way for, for people to,

39
00:02:26.910 --> 00:02:28.890
to work together. Now,

40
00:02:29.470 --> 00:02:34.090
all that progress has always has also had its negative side effects,

41
00:02:34.140 --> 00:02:38.370
which are most pronounced, of course, in the area of the environment where, um,

42
00:02:38.690 --> 00:02:40.810
I think we, we put things into an environment that,

43
00:02:40.840 --> 00:02:43.090
that shouldn't belong there and we take too much out of it,

44
00:02:43.280 --> 00:02:46.800
then they trick can replenish, which includes, you know, the fish.

45
00:02:46.860 --> 00:02:49.920
And on the other hand, you have the plastic going into the environment,

46
00:02:49.940 --> 00:02:53.720
et cetera. So, uh, so then the question is, well,

47
00:02:53.721 --> 00:02:58.440
how do we solve that? And of course, one hand is to say, okay, is kind of the,

48
00:02:59.500 --> 00:03:04.000
the, yeah, maybe Lou diet is maybe a bit of a negative way to,

49
00:03:04.001 --> 00:03:07.800
to phrase it, but the sort of reactionary approach of saying, okay,

50
00:03:07.980 --> 00:03:12.800
we should consume less. Um, you know, corporations are bad. Technology is bad,

51
00:03:12.860 --> 00:03:17.680
we should all get rid of all those things. And, um, I think, you know,

52
00:03:17.681 --> 00:03:20.310
the environment, modern environmental movement,

53
00:03:20.311 --> 00:03:24.950
which is really kinda this romantic movement, has this image of back in the day,

54
00:03:24.951 --> 00:03:27.430
everything was great. And we lived in harmony with nature.

55
00:03:27.930 --> 00:03:32.870
So let's get rid of all this modernity and try and return to that pure original

56
00:03:32.871 --> 00:03:37.030
state. What I, however, believe is that, first of all, that's,

57
00:03:37.510 --> 00:03:39.820
I don't think it's a very realistic, you know,

58
00:03:39.821 --> 00:03:43.260
people want to keep their iPhones and their cars and, you know,

59
00:03:43.261 --> 00:03:46.460
people want to move forward. Um, and at the same time,

60
00:03:46.461 --> 00:03:48.460
I don't think it's really the most effective way to,

61
00:03:48.480 --> 00:03:52.460
to solve these problems because, you know, it would be like fighting a,

62
00:03:52.580 --> 00:03:55.780
a leper tank with, with bone arrow. Uh, you know,

63
00:03:56.220 --> 00:04:00.330
technology is nothing more than an, an, an enabler of human and capabilities.

64
00:04:00.331 --> 00:04:01.650
It, it enhances our power.

65
00:04:02.350 --> 00:04:06.210
So why not use that power to also try and solve these, these, these,

66
00:04:06.211 --> 00:04:07.130
these problems as well.

67
00:04:07.270 --> 00:04:10.730
So rather than try and reject business reject technology,

68
00:04:10.890 --> 00:04:13.770
I truly believe that we should embrace those,

69
00:04:14.140 --> 00:04:15.930
those forces that make a human and,

70
00:04:16.070 --> 00:04:21.000
and has created this amazing world to also try and solve these,

71
00:04:21.010 --> 00:04:24.600
these negative side effects as well. And that's why I believe, you know,

72
00:04:24.660 --> 00:04:25.493
the over,

73
00:04:25.550 --> 00:04:30.120
over consumption of fish is not going to end like people all becoming vegan,

74
00:04:30.260 --> 00:04:34.320
but rather through, um, you know, fake meat. Uh,

75
00:04:34.480 --> 00:04:36.720
I think that the, uh,

76
00:04:37.840 --> 00:04:42.030
transport emissions are not going to be solved by people not flying anymore or

77
00:04:42.031 --> 00:04:45.350
not going anywhere anymore. Realistically people are going to fly more.

78
00:04:45.450 --> 00:04:49.990
So we better invent technologies that allow people to do that without harming

79
00:04:49.991 --> 00:04:52.190
the environment. And, um, you know,

80
00:04:52.191 --> 00:04:56.470
the same thing I think would be the case for, for, for, for plastic and,

81
00:04:56.490 --> 00:04:58.550
and really other energy uses as well.

82
00:04:59.090 --> 00:05:01.860
<v 0>No, I think that's a very wise way of looking at, and it's a,</v>

83
00:05:02.610 --> 00:05:05.220
it's a hopeful way of looking at it. And it's funny that

84
00:05:06.870 --> 00:05:09.580
today you kind of have, even though you're, you're,

85
00:05:09.581 --> 00:05:13.180
you're dealing with statistics and factual information,

86
00:05:13.210 --> 00:05:16.820
like the fact that it is safer to live today, there's less violent crime.

87
00:05:17.370 --> 00:05:22.090
It's easier to get by this more technology, more innovation, medical improve,

88
00:05:22.091 --> 00:05:24.690
radically, all these things are true, but you still have to say,

89
00:05:25.560 --> 00:05:29.090
it's not where we want it to be. Right. I'm not saying that the world's,

90
00:05:29.350 --> 00:05:32.810
you have to say that even you like, even though you're, I mean, look, it's,

91
00:05:32.811 --> 00:05:35.890
it's the worry about people barking at you,

92
00:05:36.310 --> 00:05:39.130
so worry about people it's still terrible and parts of the world,

93
00:05:39.320 --> 00:05:41.450
it's still terrible for people have color.

94
00:05:41.451 --> 00:05:45.360
It's still tell terrible for trans it's still I get it. I get it. I get it. No,

95
00:05:45.361 --> 00:05:47.440
one's saying that there's not room for improvement,

96
00:05:47.460 --> 00:05:50.880
but you have to say that like you, even though you felt compelled, you're like,

97
00:05:50.881 --> 00:05:52.240
it's still not perfect. Yeah.

98
00:05:52.580 --> 00:05:57.320
<v 1>And I, I wonder why it's so, so controversial. I,</v>

99
00:05:57.400 --> 00:06:01.800
I think it's important to, um, learn from the things that we do well,

100
00:06:02.140 --> 00:06:03.520
and then apply that. I.

101
00:06:03.520 --> 00:06:07.160
<v 0>Don't think it is that controversial. I think it's a trick. I think, uh,</v>

102
00:06:07.161 --> 00:06:10.720
there's just a lot of people looking for every single opportunity to complain,

103
00:06:11.070 --> 00:06:14.760
even to someone like you whos, you know, objectively done nothing but good.

104
00:06:15.500 --> 00:06:20.470
You say one thing, I mean, Steven Pinker took a ton of heat for saying that.

105
00:06:20.930 --> 00:06:24.910
And even though he's talking about actual scientific statistics, he's just,

106
00:06:25.180 --> 00:06:27.830
he's not saying the world's perfect and everyone should shut up.

107
00:06:27.860 --> 00:06:31.950
What he's saying is we should look at this, you know, from a, a bird's eye view,

108
00:06:32.180 --> 00:06:35.310
look down and understand it. Although there's much work to be done,

109
00:06:36.000 --> 00:06:38.980
we're in a great place in comparison to the rest of human history.

110
00:06:38.980 --> 00:06:42.460
<v 1>Sure. And it's, it's helpful to realize that progress is possible.</v>

111
00:06:42.570 --> 00:06:45.940
Just imagine that every there's something, um,

112
00:06:46.530 --> 00:06:48.220
that feels intuitively right,

113
00:06:48.360 --> 00:06:53.020
as if every step forward would also have to equal a step backward

114
00:06:53.021 --> 00:06:56.900
elsewhere. Yes. And I don't think that's, that's the case.

115
00:06:56.901 --> 00:07:01.650
There's plenty of things that you can invent that, um, that, that are,

116
00:07:02.150 --> 00:07:05.130
are not that like, and, and we see it for example, with,

117
00:07:05.131 --> 00:07:08.930
with carbon right now that there's countries where, you know, like,

118
00:07:08.960 --> 00:07:13.610
like Sweden GDP has grown a lot past 20 years, carbon emissions has gone down.

119
00:07:13.670 --> 00:07:16.610
So, um, they call that the, the decoupling.

120
00:07:16.670 --> 00:07:20.760
And I think really the, the main challenge in,

121
00:07:20.780 --> 00:07:24.840
in this century is to, um, you know, decouple, um,

122
00:07:24.930 --> 00:07:28.320
human progress from, uh, from those negative side effects.

123
00:07:28.340 --> 00:07:32.960
And I think the way to do that is not reactionary. It's really, again, um,

124
00:07:32.961 --> 00:07:37.280
through innovation and, um, through to collaboration.

125
00:07:37.920 --> 00:07:38.751
<v 0>I, I agree with you.</v>

126
00:07:38.751 --> 00:07:42.190
And I think that a lot of times people just assume that this is,

127
00:07:42.191 --> 00:07:45.190
these are the consequences of innovation that there's a pro and a con to

128
00:07:45.191 --> 00:07:48.390
everything, because there has been so many things,

129
00:07:48.480 --> 00:07:51.830
there have been so many things that are inventions that were,

130
00:07:51.831 --> 00:07:53.510
there are a pro and a con to it. Right.

131
00:07:53.650 --> 00:07:55.750
But that doesn't necessarily mean it has to be that way.

132
00:07:55.970 --> 00:07:59.540
<v 1>No. And even if things have a pro on a can,</v>

133
00:07:59.541 --> 00:08:03.460
it doesn't mean the pro is as big as the con. Right. So, um,

134
00:08:03.840 --> 00:08:06.460
and if that would be the case, all the technology,

135
00:08:07.030 --> 00:08:10.620
every technology would be neutral and it wouldn't Ima it wouldn't matter what

136
00:08:10.621 --> 00:08:11.660
your event, uh,

137
00:08:12.240 --> 00:08:15.860
but it would mean that an atomic bomb is

138
00:08:16.770 --> 00:08:21.170
morally as neutral as an ocean cleanup system,

139
00:08:21.171 --> 00:08:23.890
which I just don't find plausible.

140
00:08:23.910 --> 00:08:27.210
So I do believe that inventors entrepreneurs,

141
00:08:27.280 --> 00:08:31.530
they put certain morality into their creations, into their technology.

142
00:08:31.980 --> 00:08:34.130
There is, um, you know,

143
00:08:34.131 --> 00:08:38.170
there is a certain use that you prescribe with your invention. I mean,

144
00:08:38.171 --> 00:08:42.680
you don't use nuclear bombs to, um, wash your car, right. I mean,

145
00:08:42.820 --> 00:08:47.800
you use it, not for benign uses, unless maybe you want to Terra for Mars, which,

146
00:08:48.060 --> 00:08:51.800
you know, some people propose to do with atomic bombs. I don't know.

147
00:08:51.801 --> 00:08:56.000
What's a good, um, but, um, yeah, I,

148
00:08:56.080 --> 00:08:59.400
I don't think technology is neutral that it has a morality.

149
00:08:59.580 --> 00:09:03.960
So what that means is, is that as long as we conent, uh,

150
00:09:04.000 --> 00:09:08.440
consistently develop net positive technologies, you know,

151
00:09:08.441 --> 00:09:09.760
eventually, you know,

152
00:09:09.761 --> 00:09:14.400
the world does get better and better if say a technology is 60% good and

153
00:09:14.600 --> 00:09:18.750
maybe has 40% downside and okay.

154
00:09:19.370 --> 00:09:23.670
But then we can invent a solution for that 40%. And maybe that's again,

155
00:09:23.850 --> 00:09:27.110
net positive and you kinda get this, this cascade of,

156
00:09:27.250 --> 00:09:29.350
of ever improving world.

157
00:09:29.970 --> 00:09:32.270
<v 0>No, I, I think what you're saying sounds beautiful.</v>

158
00:09:32.530 --> 00:09:35.340
And if more people thought the way you're thinking, I think, uh,

159
00:09:35.341 --> 00:09:39.980
the world be a better place. I, I like the, the positivity. I like the optimism.

160
00:09:40.240 --> 00:09:43.340
And what, what you're thinking in terms, particularly in terms of what's,

161
00:09:43.440 --> 00:09:46.420
what's possible with innovation. Yeah.

162
00:09:46.850 --> 00:09:51.660
<v 1>Well, I, I just don't think that being against something is very</v>

163
00:09:52.910 --> 00:09:56.530
productive. Mm-hmm &lt;affirmative&gt;, um, I, yeah, it's,

164
00:09:57.230 --> 00:10:00.210
it doesn't really move us forward. Um, so

165
00:10:01.710 --> 00:10:05.890
rather than, you know, protesting against the things that I don't agree with,

166
00:10:05.950 --> 00:10:09.730
and there's certainly things I, I don't agree with. Um, but again,

167
00:10:09.810 --> 00:10:12.730
I don't think it's, it's very helpful rather than doing that.

168
00:10:12.930 --> 00:10:16.440
I much rather built towards the future that I, that I do agree with.

169
00:10:16.940 --> 00:10:20.680
<v 0>Listen, man. I think what you're saying is very, very logical.</v>

170
00:10:21.000 --> 00:10:25.000
I wish more people thought like you, you're a great role model for a lot of, uh,

171
00:10:25.030 --> 00:10:29.400
kids to use your energy in a positive direction, you know, can,

172
00:10:29.420 --> 00:10:31.880
it can be done. Yeah. Um.

173
00:10:33.150 --> 00:10:36.400
<v 1>Sure. I mean, I think we agree with your journey.</v>

174
00:10:36.900 --> 00:10:37.190
<v 0>Yeah.</v>

