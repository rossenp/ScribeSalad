WEBVTT

1
00:00:01.020 --> 00:00:03.120
<v 0>The Joe Rogan experience. Oh,</v>

2
00:00:03.240 --> 00:00:06.660
you know what I did want to talk to you about or watched the social dilemma?

3
00:00:07.790 --> 00:00:08.840
<v 1>Jeez dude,</v>

4
00:00:11.600 --> 00:00:13.190
that's a must-see kids.

5
00:00:13.280 --> 00:00:14.630
<v 2>I heard it's depressing.</v>

6
00:00:15.200 --> 00:00:19.640
<v 0>Oh, so real, so real, real life, Tom. What's in you.</v>

7
00:00:20.000 --> 00:00:24.500
Everything's going to be fine. Wear a mask. I live in communist, Russia, a mask.

8
00:00:24.530 --> 00:00:27.950
Trust me, Gavin Newsome. His penis tastes delicious.

9
00:00:28.340 --> 00:00:31.550
We're all going to be fine. We.

10
00:00:31.550 --> 00:00:34.880
<v 2>Are going to be fine. You think we exist on this plane and this plane only?</v>

11
00:00:35.600 --> 00:00:38.810
<v 0>Ooh, that's heavy. No, no, because I've done a lot of drugs.</v>

12
00:00:39.110 --> 00:00:41.780
I think there's probably something else. I think there's something,

13
00:00:41.840 --> 00:00:44.750
I think there's something else out there, but just unaccessible right now.

14
00:00:45.200 --> 00:00:49.760
But this social dilemma makes me very concerned about the

15
00:00:49.761 --> 00:00:53.630
future because all of these technologists and all of these people that have

16
00:00:53.631 --> 00:00:57.020
invented all this stuff that now are very unhappy.

17
00:00:57.470 --> 00:01:01.190
It's really fascinating to see them discussing their own creations.

18
00:01:01.191 --> 00:01:05.300
And see outsiders are also technologists who didn't didn't invent these things,

19
00:01:05.301 --> 00:01:09.380
but are seeing the patterns in these things and understand it from a really

20
00:01:10.010 --> 00:01:13.610
educated perspective. They're they're saying this could lead to civil war.

21
00:01:13.611 --> 00:01:17.210
Like people are getting more and more divided and it shows in the film how

22
00:01:17.211 --> 00:01:20.660
social media has made people far more polarized,

23
00:01:20.690 --> 00:01:24.830
far more divided than ever before the, the red and the blue and the, you know,

24
00:01:24.831 --> 00:01:27.950
it's like, it's disturbing. It's like, it's the most.

25
00:01:27.950 --> 00:01:30.350
<v 2>Dangerous part of it. And it can't be corrected.</v>

26
00:01:31.010 --> 00:01:35.330
<v 0>Well, there's a lot of dangerous parts about it, but the thought bubbles, the,</v>

27
00:01:35.331 --> 00:01:37.010
the fact that these people get in these,

28
00:01:37.040 --> 00:01:41.150
these bubbles of thought where everybody around you thinks your way and

29
00:01:41.151 --> 00:01:43.640
everybody who thinks a different way is the enemy. Yeah.

30
00:01:43.730 --> 00:01:48.620
This is a really dangerous part of the reality that we live in today because

31
00:01:49.100 --> 00:01:51.140
it's not what we anticipated.

32
00:01:51.170 --> 00:01:55.400
I thought that the internet and the age of information and all that we're

33
00:01:55.401 --> 00:01:59.570
experiencing right now would bring about an understanding in a nuanced

34
00:01:59.571 --> 00:02:01.940
perspective in life, in all ways.

35
00:02:01.941 --> 00:02:04.430
So you'd be able to see things from other people's perspectives more,

36
00:02:04.760 --> 00:02:07.550
more easily, because it'd be more readily available.

37
00:02:07.700 --> 00:02:11.240
And it would be more encouraged for you to seek out all this information,

38
00:02:11.241 --> 00:02:16.010
but a bunch of factors that happened at the

39
00:02:16.011 --> 00:02:20.870
same time, all have sort of made it worse than ever before.

40
00:02:20.930 --> 00:02:24.440
There's social media and the divide that comes.

41
00:02:24.441 --> 00:02:26.930
And this is where the social dilemma comes in place. Yeah.

42
00:02:26.990 --> 00:02:31.340
There's a divide that comes about because of the way they've engineered these

43
00:02:31.341 --> 00:02:33.620
algorithms, which is really disturbing.

44
00:02:33.830 --> 00:02:38.660
So whatever you're into it finds those things and accentuates them because

45
00:02:38.840 --> 00:02:42.500
it just wants you to stay on more. It wants you to engage more.

46
00:02:42.560 --> 00:02:45.620
She wants you to pay attention to the things. Now are,

47
00:02:45.621 --> 00:02:49.910
is your fear did a little bit of a study on this, a little bit of a test.

48
00:02:50.330 --> 00:02:54.620
And he only YouTube puppies. That's all he would YouTube,

49
00:02:54.650 --> 00:02:57.020
just YouTube puppies just, oh, just to see what happened.

50
00:02:57.290 --> 00:03:02.140
And all YouTube would send him yeah. His puppies,

51
00:03:02.170 --> 00:03:04.660
right? All they would all, they would show him all.

52
00:03:04.661 --> 00:03:06.100
They would suggest his puppy.

53
00:03:06.610 --> 00:03:11.140
So this idea that their engineering outrage is a little disingenuous,

54
00:03:11.141 --> 00:03:15.040
because what they're really doing is finding what you're interested in.

55
00:03:15.280 --> 00:03:19.480
And people have been shown to pay attention to what they disagree with

56
00:03:19.960 --> 00:03:23.980
far more than what they agree with. So that's how.

57
00:03:23.980 --> 00:03:26.170
<v 2>I, them spits you. Things that you disagree with.</v>

58
00:03:26.300 --> 00:03:31.000
<v 0>Exactly. Because you get engaged with that and you get angry, which is,</v>

59
00:03:31.210 --> 00:03:32.470
that was a part of the rage.

60
00:03:32.560 --> 00:03:35.050
It was showing how things that people disagree with.

61
00:03:35.051 --> 00:03:37.000
Things that make people upset, right?

62
00:03:37.030 --> 00:03:40.900
Those are the things that people are much more likely to engage with.

63
00:03:40.930 --> 00:03:45.500
And you're like, you liberals, or you. You racist. Everyone's racist, right?

64
00:03:45.820 --> 00:03:47.680
It's like, it's this thing.

65
00:03:47.710 --> 00:03:51.100
That is a part of being a person where you seek,

66
00:03:51.160 --> 00:03:55.660
especially when you don't feel like you're really you're being heard, right.

67
00:03:55.661 --> 00:03:58.600
When you're at home and you're sitting on the toilet and you're going through

68
00:03:58.601 --> 00:04:02.530
Facebook and you see some about what the burn, the flag, you mother,

69
00:04:03.130 --> 00:04:03.760
and you start,

70
00:04:03.760 --> 00:04:08.260
you start making these messages is you're more likely to do that than

71
00:04:08.320 --> 00:04:12.670
seeing some beautiful story about these parents that adopt this kid.

72
00:04:12.671 --> 00:04:16.180
And they give them a home and he comes from a bad part of the world like that.

73
00:04:16.690 --> 00:04:19.000
You're not going to go good way to go for you.

74
00:04:19.001 --> 00:04:22.810
Let me write down all the amazing things about what you're doing is a terrible,

75
00:04:22.840 --> 00:04:25.720
no you're going to, you can get mad. Or if you're on the left,

76
00:04:25.721 --> 00:04:28.750
you're going to get mad because the wildfires are going to blame on Trump and

77
00:04:28.751 --> 00:04:31.480
climate change, all these different things. And you.

78
00:04:31.600 --> 00:04:33.880
<v 2>Know, like, look, it gets your outrage going. Yeah.</v>

79
00:04:33.910 --> 00:04:36.670
<v 0>I mean, so many people think Trump's responsible for the wildfires. Listen,</v>

80
00:04:36.671 --> 00:04:40.390
folks, those fires were going to happen,

81
00:04:40.391 --> 00:04:42.790
regardless of who is president now,

82
00:04:42.791 --> 00:04:45.940
whether or not he is putting in policies is going to protect people 10,

83
00:04:45.941 --> 00:04:48.430
20 years from now. Right? That's a real argument,

84
00:04:48.760 --> 00:04:52.120
but the fires that are going on right now are not because of Trump.

85
00:04:52.210 --> 00:04:56.410
It takes a long time to turn that battleship. Absolutely. Well,

86
00:04:56.590 --> 00:05:01.090
Google is better. Yeah. They're much better at it. Yeah. And this, I use,

87
00:05:01.600 --> 00:05:04.330
uh, my apple, I have an apple phone and I have an Android phone.

88
00:05:04.660 --> 00:05:09.220
My apple phone is my primary phone and I use my Android phone more to

89
00:05:09.550 --> 00:05:12.490
around with anything, but I'm around with a few things on it.

90
00:05:12.491 --> 00:05:16.840
And one of the things is how well it picks up your voice and how well it

91
00:05:16.841 --> 00:05:19.180
transcribes it. So here's the argument, right?

92
00:05:19.600 --> 00:05:23.200
Apple is much better with your privacy. They're much better with your privacy.

93
00:05:23.201 --> 00:05:27.220
Like when you use apple maps, it's not sending your data to anyone. Right?

94
00:05:27.610 --> 00:05:32.260
But it's one of the reasons why apple maps is not as good as Google maps or

95
00:05:32.270 --> 00:05:35.650
better and sharing it. It's just better. They just it's,

96
00:05:35.710 --> 00:05:38.440
they're getting data constantly from you.

97
00:05:38.770 --> 00:05:42.040
They're getting data from all the other drivers. They're sharing that data.

98
00:05:42.041 --> 00:05:43.540
They're compiling that data.

99
00:05:43.780 --> 00:05:47.200
And they're also sending ads your way to profit off of this to make it

100
00:05:47.201 --> 00:05:50.140
profitable. So because of that,

101
00:05:50.170 --> 00:05:54.250
because Google is just sucking up data constantly,

102
00:05:54.580 --> 00:05:56.560
they can provide you with better services.

103
00:05:57.170 --> 00:06:00.230
So there they have an amazing search engine they have.

104
00:06:00.410 --> 00:06:02.510
But that was one of the things about the social dilemma.

105
00:06:02.570 --> 00:06:07.460
The search engine gives different results based on where you are, like say,

106
00:06:07.461 --> 00:06:09.530
if you type to use an example, climate change,

107
00:06:10.280 --> 00:06:15.050
climate change is it might say a hoax or climate

108
00:06:15.051 --> 00:06:18.800
change is a terrible threat, depending on where you live.

109
00:06:18.801 --> 00:06:23.000
Like you live in, right. It might give you one thing. But if you live in Waco,

110
00:06:23.001 --> 00:06:25.520
Texas, it might give you another thing. Yeah.

111
00:06:25.521 --> 00:06:28.070
And it's based on what it thinks you want to see.

112
00:06:29.060 --> 00:06:31.460
<v 2>That's crazy. It's not good. That's not good.</v>

113
00:06:31.640 --> 00:06:34.490
That's why reinforcing people's dumb ideas. That's why.

114
00:06:34.490 --> 00:06:39.090
<v 0>I use duck. Duck go, duck, duck go. Does not do any of that stuff. It also,</v>

115
00:06:39.130 --> 00:06:43.580
it gives you things, it's it doesn't send your data somewhere.

116
00:06:44.390 --> 00:06:45.530
It protects your privacy.

117
00:06:45.860 --> 00:06:49.250
<v 2>So you put climate change is on duck, duck. No, I put chicks with.</v>

118
00:06:55.270 --> 00:06:55.690
[inaudible].

119
00:06:55.690 --> 00:06:59.630
<v 0>You put whatever you want it just, yeah. The results are not curated.</v>

120
00:07:00.110 --> 00:07:04.490
So it's just giving you the most applicable results for the things that you're

121
00:07:04.491 --> 00:07:05.324
looking for.

122
00:07:05.450 --> 00:07:08.750
But it's not doing it in a way where it's curating it for your own interests.

123
00:07:08.751 --> 00:07:10.940
Like if you try to find things that are controversial and we've,

124
00:07:10.941 --> 00:07:14.480
we've tried to find that on the podcast before where Jamie will Google something

125
00:07:14.720 --> 00:07:18.980
and I'll know it to be correct, but Google will not show that it's correct.

126
00:07:18.981 --> 00:07:23.030
Because maybe the correct answer is not politically correct.

127
00:07:23.420 --> 00:07:27.140
So you have to go through several pages and maybe you even have to Google it in

128
00:07:27.141 --> 00:07:31.970
a very specific way to get to the heart of the science behind what's wrong

129
00:07:32.000 --> 00:07:35.660
with the consensus opinion, the consensus opinion might be wrong.

130
00:07:35.990 --> 00:07:39.320
Like that's the case with a lot of nutrition things. It's the case of a lot of,

131
00:07:39.590 --> 00:07:44.180
a lot of things regarding like anything controversial, anything where you,

132
00:07:44.181 --> 00:07:48.790
where there's a political motive to sway the argument one way. It's so.

133
00:07:48.790 --> 00:07:53.500
<v 2>Amazing how deep you have to dive to cross-reference stuff to really</v>

134
00:07:53.501 --> 00:07:57.610
try and assemble a truthful opinion. It's so hard.

135
00:07:57.820 --> 00:08:00.340
So with all this stuff, like, do you do the,

136
00:08:00.400 --> 00:08:03.880
and you have all these people from Twitter and Google and stuff who were saying

137
00:08:03.881 --> 00:08:06.220
like what we did or Facebook that was horrible.

138
00:08:06.221 --> 00:08:08.110
And we really kind of eff things up.

139
00:08:09.100 --> 00:08:12.850
Do they feel like they can also correct this problem?

140
00:08:13.810 --> 00:08:18.100
<v 0>I don't know if you can put the bow, the cap on the bottle.</v>

141
00:08:18.250 --> 00:08:21.100
And I don't know. I don't know if you can do that.

142
00:08:21.970 --> 00:08:24.130
I don't know if they know it either because they didn't think it was going to

143
00:08:24.131 --> 00:08:28.300
happen in the first time you remember Jack Dorsey was testifying.

144
00:08:28.330 --> 00:08:33.160
I think it was before Congress. And, um, he was saying that 12 years ago,

145
00:08:33.161 --> 00:08:34.150
when we created Twitter,

146
00:08:34.480 --> 00:08:38.920
we had no idea that this was going to be a situation that we had to anticipate.

147
00:08:38.980 --> 00:08:40.900
We know no one ever saw this coming.

148
00:08:40.901 --> 00:08:44.050
And if you go back and see the early Twitter,

149
00:08:44.110 --> 00:08:46.810
when you would remember you would do the app,

150
00:08:46.870 --> 00:08:51.520
it would always show your name in front of every tweet. So it'd be like at Tom,

151
00:08:51.521 --> 00:08:54.460
Papa is having pizza. Like you would say what you were doing.

152
00:08:54.461 --> 00:08:58.470
It was really like how people would use Twitter, you know? Um,

153
00:08:59.040 --> 00:09:03.840
but it was like not political. It was just fun. Yeah.

154
00:09:03.870 --> 00:09:05.670
People would just like, no one knew what to do with it.

155
00:09:06.060 --> 00:09:07.410
And then somewhere along the line,

156
00:09:07.411 --> 00:09:10.030
people started figuring out how to get an argument. No.

157
00:09:10.040 --> 00:09:14.390
<v 3>It's so bad. We just always were ruined everything. It can be so great.</v>

158
00:09:14.390 --> 00:09:16.310
<v 2>I remember when it first came out, it was like, wow,</v>

159
00:09:16.311 --> 00:09:19.730
there would probably never have been slavery if there had been Twitter because

160
00:09:19.731 --> 00:09:23.810
people would have exposed it so early. And it just seemed so hopeful.

161
00:09:24.020 --> 00:09:28.130
But of course all the scummy people get it and then just ruin it.

162
00:09:28.790 --> 00:09:32.900
<v 0>It makes me scummy too, because it makes people more polarized.</v>

163
00:09:32.901 --> 00:09:36.110
It makes people more aggressive in, um,

164
00:09:36.140 --> 00:09:40.550
reinforcing their idea what the truth is and trying to stop other people.

165
00:09:40.551 --> 00:09:44.840
And you're seeing so much suppression of other people's

166
00:09:45.170 --> 00:09:49.130
opinions and expression today, which is so strange. Yeah. It's just,

167
00:09:49.131 --> 00:09:53.450
it's one of the weirdest times ever to look at the way human beings communicate

168
00:09:53.810 --> 00:09:55.790
because of the tension we were talking about this earlier,

169
00:09:55.791 --> 00:09:58.460
we never really finished the thought, but you got Trump.

170
00:09:59.270 --> 00:10:01.760
Then you got the pandemic and then you get the economic collapse.

171
00:10:01.761 --> 00:10:05.090
You have all these things happening. So people are, they're desperate,

172
00:10:05.120 --> 00:10:05.901
they're sad.

173
00:10:05.901 --> 00:10:08.870
And then you've got looting and you've got the riots and you see get racial

174
00:10:08.871 --> 00:10:12.470
tension. You've got violence. You've got this anti-police sentiment,

175
00:10:12.650 --> 00:10:16.700
which also leads to more instability in the streets, more instability in,

176
00:10:16.990 --> 00:10:21.620
in the cities and less safety and fear, fear, fear of, uh,

177
00:10:21.950 --> 00:10:25.850
fear of police, fear of gangs, fear,

178
00:10:25.880 --> 00:10:30.140
fear of Antifa, fear of white supremacists, fear of everything.

179
00:10:30.141 --> 00:10:34.790
It's like all this fear and people arguing online

180
00:10:35.750 --> 00:10:40.310
really literally addicted people. Yeah. People are addicts, right?

181
00:10:40.460 --> 00:10:42.920
They're just as addicted as people who are gambling addicts,

182
00:10:43.070 --> 00:10:45.290
just as addicted to people that are sex addicts,

183
00:10:45.500 --> 00:10:48.050
they're addicted to Twitter and they're arguing,

184
00:10:48.051 --> 00:10:52.730
they're mentally ill people and they're constantly engaging in conflict. Right?

185
00:10:52.730 --> 00:10:55.160
<v 2>And they're coming up on people's posts and you don't know that you're</v>

186
00:10:55.161 --> 00:10:56.240
interacting with mentally.

187
00:10:56.240 --> 00:10:59.620
<v 3>Ill people or, or people from other countries that are trying.</v>

188
00:11:00.380 --> 00:11:03.830
<v 0>Assume you're interacting with mentally unwell people,</v>

189
00:11:03.831 --> 00:11:06.830
because almost everyone who's using it in that way.

190
00:11:06.831 --> 00:11:09.290
Is it in one way or another mentally ill.

191
00:11:09.770 --> 00:11:14.660
<v 2>Or you put your phone in the drawer and you go to the park</v>

192
00:11:15.050 --> 00:11:15.883
and all of a sudden.

193
00:11:16.010 --> 00:11:19.130
<v 0>Every you get chops down because you want to defund the police.</v>

194
00:11:21.500 --> 00:11:24.140
<v 3>Everything calms down and goes away. She goes.</v>

195
00:11:24.140 --> 00:11:28.370
<v 2>You're not in, you're not living in this weird reality.</v>

196
00:11:28.850 --> 00:11:32.630
You're just living in, in real life and you're not participating in it.

197
00:11:33.170 --> 00:11:34.250
<v 0>That's what's.</v>

198
00:11:34.400 --> 00:11:37.910
We would all hope for the problem is there's so many people that are doing it

199
00:11:38.360 --> 00:11:40.820
where they're doing it on Facebook or Twitter or arguing,

200
00:11:41.270 --> 00:11:43.460
whatever things they're arguing about. It's,

201
00:11:43.490 --> 00:11:45.890
it's spilling out into the real world, right? You're in.

202
00:11:45.890 --> 00:11:48.950
<v 2>The park being all Zen and all of a sudden, a flash mob shows up that.</v>

203
00:11:48.950 --> 00:11:52.400
<v 3>Organized on Twitter. And you're like, what the hell is happening here?</v>

204
00:11:52.640 --> 00:11:54.400
I was all Zen him and it ago.

205
00:11:55.330 --> 00:11:59.980
<v 0>Episodes of the Joe Rogan experience are now free on Spotify. That's right.</v>

206
00:12:00.220 --> 00:12:03.130
They're free from September 1st is December 1st.

207
00:12:03.131 --> 00:12:05.560
They're going to be available everywhere. But after December 1st,

208
00:12:05.561 --> 00:12:09.250
they will only be available on Spotify, but they will be free.

209
00:12:09.460 --> 00:12:13.270
That includes the video. The video will also be there. It'll also be free.

210
00:12:13.960 --> 00:12:18.070
That's all we're asking you. Just go download Spotify much. Buh-bye.

