WEBVTT

1
00:00:01.060 --> 00:00:02.360
<v 0>The Joe Rogan experience.</v>

2
00:00:02.950 --> 00:00:05.160
<v 1>Even though we just got through a horrible year,</v>

3
00:00:05.510 --> 00:00:10.080
that year exposed a lot of weird about our civilization. Yeah. It's it,

4
00:00:10.280 --> 00:00:15.200
it exposed a lot of weird about a bunch of really freaked out people that

5
00:00:15.201 --> 00:00:16.760
are just paranoid and,

6
00:00:17.040 --> 00:00:20.830
and schizophrenic and how many people that are just fragile.

7
00:00:21.300 --> 00:00:23.550
There's so many fragile people in our culture.

8
00:00:24.390 --> 00:00:28.030
<v 0>I was blown away by the politics of it. I'm a politician.</v>

9
00:00:28.090 --> 00:00:31.310
So this is what I analyze. And I was blown away that the,

10
00:00:31.330 --> 00:00:36.150
the conversation about how to deal with the last year became, or,

11
00:00:36.350 --> 00:00:40.950
or it, the division, the division fell upon partisan law,

12
00:00:41.430 --> 00:00:41.781
right?

13
00:00:41.781 --> 00:00:45.380
About whether to lock down or not to lockdown about whether people liked masks

14
00:00:45.381 --> 00:00:49.140
or didn't like masks. And at first that seems really odd.

15
00:00:49.200 --> 00:00:53.100
And so I spent a lot of time analyzing this cuz it shouldn't be that way. It,

16
00:00:53.101 --> 00:00:55.300
it should, it should be mixed. You would,

17
00:00:55.360 --> 00:00:58.700
you would think about just cuz what you're really talking about is somebody's

18
00:00:58.701 --> 00:01:01.180
risk assessment and you know,

19
00:01:01.181 --> 00:01:04.970
how they a perceive risk and how they want to deal with that and how they think

20
00:01:04.971 --> 00:01:09.450
everybody else should deal with that. And um, it's strange.

21
00:01:09.530 --> 00:01:10.970
I think there's a lot of factors involved. I,

22
00:01:11.050 --> 00:01:12.930
I do think that there was some political opportunism.

23
00:01:13.250 --> 00:01:15.970
I think that if Trump says something, people reactively say the opposite,

24
00:01:15.971 --> 00:01:19.450
that's a problem. Right. And then that's definitely part of it. However, um,

25
00:01:19.780 --> 00:01:23.560
after Trump lost the election, he, that, that didn't stop that,

26
00:01:23.830 --> 00:01:27.440
that movement Toga, you know, for pro lockdown movement never stopped.

27
00:01:27.500 --> 00:01:30.000
So it didn't, it was not clear to me then that, that was the only reason,

28
00:01:30.260 --> 00:01:31.000
but it, but it.

29
00:01:31.000 --> 00:01:35.080
<v 1>Is see the problem is once people get committed to an ideology or committed to a</v>

30
00:01:35.081 --> 00:01:38.640
narrative yeah. Just because Trump lost and now Bidens in power,

31
00:01:39.310 --> 00:01:42.360
it's not like everybody just abandons this narrative and,

32
00:01:42.790 --> 00:01:45.430
and creates a new reality based on objective truth.

33
00:01:45.530 --> 00:01:47.350
<v 0>But they'll never even do a, do a,</v>

34
00:01:47.370 --> 00:01:50.230
an after action report on it to the point to where, to.

35
00:01:50.230 --> 00:01:51.910
<v 1>Where it's ridiculous. I thought you were showing us something.</v>

36
00:01:52.150 --> 00:01:55.590
<v 0>&lt;Laugh&gt;. And, and so, so I, I put another, I put another,</v>

37
00:01:55.750 --> 00:01:58.950
a few factors in there. I, I think some of it is the fact that, uh,

38
00:01:59.190 --> 00:02:02.070
Democrats tend to congregate in urban areas and it might be, you know,

39
00:02:02.170 --> 00:02:05.020
the virus is more in your face, in an urban area than in a rural area.

40
00:02:05.260 --> 00:02:07.420
That might be some explanation there for sure. Right. But,

41
00:02:07.421 --> 00:02:10.420
but it really boils down to, and there's there's studies on this where our,

42
00:02:10.660 --> 00:02:13.540
our brains light up differently, uh, when assessing risk.

43
00:02:14.000 --> 00:02:16.700
Now it doesn't mean that the behavioral outcomes of these studies are,

44
00:02:16.900 --> 00:02:17.980
are change. Basically they would,

45
00:02:17.981 --> 00:02:20.380
they would take liberals and conservatives and they would give them a be what,

46
00:02:20.540 --> 00:02:24.690
what bounce to a be game and see how they react now that now the actual

47
00:02:24.691 --> 00:02:29.370
behavioral outcomes, what they choose wasn't didn't change all that much.

48
00:02:29.990 --> 00:02:31.730
But when they're doing the MRI scans,

49
00:02:31.920 --> 00:02:34.810
they see that their brains light up differently. So that's interesting.

50
00:02:34.811 --> 00:02:39.370
So we clearly assess risk differently somehow. Uh, so I looked at data on,

51
00:02:40.210 --> 00:02:43.930
uh, the kind of jobs that we choose and it turns out, and this is I,

52
00:02:44.280 --> 00:02:45.113
if you would guess this,

53
00:02:45.670 --> 00:02:50.280
that the vast majority of conservat or vast majority of dangerous jobs are,

54
00:02:50.560 --> 00:02:54.920
are mostly populated by conservatives, lumbered, jacking, uh, hard labor,

55
00:02:55.720 --> 00:02:58.000
uh, military law enforcement. So there's,

56
00:02:58.230 --> 00:03:03.120
it's obvious that we're choosing to engage in risk differently just overall

57
00:03:03.300 --> 00:03:04.133
in the aggregate.

58
00:03:04.900 --> 00:03:08.640
And so I think that gets at why we think differently about this.

59
00:03:08.680 --> 00:03:12.680
I think we're truly wired differently. And at, on top of that, the,

60
00:03:12.880 --> 00:03:15.640
the natural disposition of a liberal to believe in some sort of collective

61
00:03:15.641 --> 00:03:16.474
action,

62
00:03:16.550 --> 00:03:20.870
whereas the natural disposition of a derivative is to believe that government

63
00:03:20.871 --> 00:03:23.750
can only do so much right there, there's life out there.

64
00:03:23.751 --> 00:03:25.150
And it's sometimes it's dangerous.

65
00:03:25.150 --> 00:03:28.270
And it's up to you as an individual to generally assess that.

66
00:03:28.271 --> 00:03:31.270
And that's also the most efficient way to do things in order to get the best

67
00:03:31.271 --> 00:03:32.430
outcomes in the aggregate.

68
00:03:32.650 --> 00:03:35.110
So these are two dispositions that are always present and,

69
00:03:35.310 --> 00:03:38.790
and they manifest in policy outcomes all the time. And in this case,

70
00:03:38.791 --> 00:03:42.980
it obvious how they manifested into the, into the way we dealt with coronavirus.

71
00:03:44.140 --> 00:03:47.380
Um, and, and I think that kind of explains it cuz you know,

72
00:03:47.560 --> 00:03:50.100
and think about this way too, when a,

73
00:03:50.220 --> 00:03:53.140
when a more left leaning public health official talks about it,

74
00:03:53.410 --> 00:03:55.020
they always give you the worst case scenario. Well,

75
00:03:55.021 --> 00:03:58.340
it's possible that if you're 15, you could die. Well, yeah, it's possible.

76
00:03:58.400 --> 00:04:02.210
But it's also far more unlikely than even if you got the flu, they don't,

77
00:04:02.330 --> 00:04:04.450
they say they leave out that part. They leave out the context,

78
00:04:04.560 --> 00:04:05.810
they leave out the probabilities.

79
00:04:05.811 --> 00:04:08.530
This is why I've been so frustrated with our public health officials. Yeah.

80
00:04:09.040 --> 00:04:12.530
Give us the whole truth. Right? Don't just give us the most dangerous truth.

81
00:04:12.531 --> 00:04:16.650
Don't don't tell us the tail end of the probability, uh, scale, like that's,

82
00:04:16.651 --> 00:04:19.930
that's not useful information to us. Very,

83
00:04:20.480 --> 00:04:23.120
it's been very frustrating to watch how we've dealt with this over the last year

84
00:04:23.180 --> 00:04:25.000
around the world. Not just in America, frankly,

85
00:04:25.001 --> 00:04:27.080
we've had it better than a lot of countries. I, I.

86
00:04:27.080 --> 00:04:32.080
<v 1>Think people tend to try to find a group that they can attach</v>

87
00:04:32.081 --> 00:04:35.040
themselves to. And Chris rock has a great bit about this.

88
00:04:35.900 --> 00:04:39.520
You know what you can find, uh, Maynard,

89
00:04:39.740 --> 00:04:44.670
who is one of the matchmaker for the UFC posted this on his

90
00:04:44.671 --> 00:04:49.430
Instagram, um, go to, uh, M you got it. Yeah.

91
00:04:49.540 --> 00:04:53.070
Okay. So Maynard posted this from the great and powerful Chris rock.

92
00:04:53.650 --> 00:04:56.150
And this is, we can watch this cuz it's on his Instagram.

93
00:04:56.151 --> 00:05:00.910
And if Chris will holler at me, if it's it's an issue, but I love Chris.

94
00:05:01.280 --> 00:05:05.500
And this is one of the, the best points. It might not be one of his best bits,

95
00:05:06.560 --> 00:05:08.220
but it's one of his best points ever,

96
00:05:08.540 --> 00:05:11.940
cuz he's so it's so accurate because he is talking about reality, man.

97
00:05:12.120 --> 00:05:13.980
<v 2>We all got a gang mentality.</v>

98
00:05:14.330 --> 00:05:19.300
Republicans are idiots and Democrats are idiots and conservatives

99
00:05:19.460 --> 00:05:21.420
are idiots and liberals are idiots.

100
00:05:22.350 --> 00:05:27.210
Anyone that makes up their mind before they hear the issue is a fool.

101
00:05:27.440 --> 00:05:28.273
Okay.

102
00:05:34.000 --> 00:05:38.930
Everybody now hold up. Everybody's so busy wanting to be down with a gang.

103
00:05:38.990 --> 00:05:43.640
I'm a conservative, I'm liberal. I'm concerned. It's. Be a person.

104
00:05:44.220 --> 00:05:45.053
Listen,

105
00:05:45.860 --> 00:05:50.240
let it swirl around your head then form your opinion.

106
00:05:50.620 --> 00:05:55.400
No normal decent person is one thing. Okay.

107
00:05:55.760 --> 00:06:00.640
I got some I'm concerned about. I got some I'm liberal about crime.

108
00:06:01.140 --> 00:06:04.520
I'm conservative prostitution. I'm liberal. &lt;laugh&gt;.

109
00:06:06.420 --> 00:06:10.120
<v 1>It goes on, but that's where the clip ends. But that's so accurate.</v>

110
00:06:10.140 --> 00:06:13.960
Is that what a lot of people are afraid of is being alone.

111
00:06:13.961 --> 00:06:17.310
They're afraid of being attacked. And one of the things about today's culture,

112
00:06:17.590 --> 00:06:21.190
particularly with, uh, social media is that it's an attack culture.

113
00:06:21.300 --> 00:06:22.270
It's a bully culture.

114
00:06:22.850 --> 00:06:25.870
And a lot of these people that are doing the attacking and the doing the

115
00:06:25.990 --> 00:06:29.230
bullying, they've been bullied in the real world. So they want payback.

116
00:06:29.510 --> 00:06:31.990
So they're trying to bully people online and that's what you see.

117
00:06:32.020 --> 00:06:35.390
There's a lot of like low status males.

118
00:06:35.790 --> 00:06:40.780
A lot of like really people who have never really overcome physical

119
00:06:40.890 --> 00:06:43.660
adversity or they're not successful,

120
00:06:44.120 --> 00:06:48.980
but they've found a way online to gather up a group of people that resonate

121
00:06:49.210 --> 00:06:52.660
with some of their opinions and they can attack people and they do it all day

122
00:06:52.661 --> 00:06:53.340
long. You.

123
00:06:53.340 --> 00:06:56.340
<v 0>Know, there's I find that to be the most &lt;laugh&gt;, it's fascinating. The,</v>

124
00:06:56.341 --> 00:06:58.170
the most probable it's somebody's, uh,

125
00:06:58.171 --> 00:07:02.970
saying something extremely crude and awful to me online. It's it's probably a,

126
00:07:03.530 --> 00:07:04.970
a younger man. It.

127
00:07:05.210 --> 00:07:06.043
<v 1>Doesn't necessarily,</v>

128
00:07:06.080 --> 00:07:09.610
sometimes it's older men who have failed their life and they've,

129
00:07:10.000 --> 00:07:12.690
they've decided that this is their stand. This is their line,

130
00:07:12.691 --> 00:07:13.570
the sand they're gonna draw.

131
00:07:14.050 --> 00:07:17.850
Whether now they're gonna be anti-racist or they're gonna be, you know, uh,

132
00:07:18.040 --> 00:07:21.360
anti homophobic or anti transphobic or whatever it is.

133
00:07:21.361 --> 00:07:24.880
And they're gonna attack all these people. Yeah. Whether or, or anti, you know,

134
00:07:24.881 --> 00:07:29.480
there's so many different ideological pathways that you could choose that you

135
00:07:29.481 --> 00:07:31.120
could get a group of people that agree with you.

136
00:07:31.380 --> 00:07:35.400
And then you fight against anyone that opposes these ideas and you do it in a

137
00:07:35.401 --> 00:07:38.440
really aggressive and nasty way,

138
00:07:38.880 --> 00:07:41.910
which is something that we should push back against period.

139
00:07:42.580 --> 00:07:47.430
Like ideas should be something that you should be able to discuss

140
00:07:48.810 --> 00:07:52.710
and, and debate and analyze. You should be able to sit down and go,

141
00:07:52.970 --> 00:07:55.910
why do you believe in the simulation theory? You know? And,

142
00:07:56.070 --> 00:07:58.990
and we shouldn't be like, well, you're a idiot, Joe Rogan. That's why you agree.

143
00:07:59.460 --> 00:08:02.220
Yeah. It should be like, yeah, I'm a idiot. Yeah.

144
00:08:02.480 --> 00:08:05.380
But that's not why I agree with this. That's not why I look at this.

145
00:08:05.660 --> 00:08:09.460
I look at this cuz I'm curious. And I, I, I see all the various components,

146
00:08:09.720 --> 00:08:12.300
you know, I think there's a lot of there's,

147
00:08:12.301 --> 00:08:14.500
there's a lot of truth on both sides,

148
00:08:14.800 --> 00:08:18.020
but the problem is when you ignore the truth on a side that doesn't fit with

149
00:08:18.021 --> 00:08:20.450
your ideology, then you're not interested in truth.

150
00:08:20.550 --> 00:08:22.450
You're interested in what Chris rock is talking about.

151
00:08:22.451 --> 00:08:23.930
You're interested in supporting your gang.

152
00:08:24.930 --> 00:08:26.850
<v 0>So yeah. And that's your gang or team. I,</v>

153
00:08:26.890 --> 00:08:29.370
I always say that you're either wearing a blue Jersey or a red Jersey and then

154
00:08:29.371 --> 00:08:32.850
you act accordingly and you repeat these sort of mantras that you think you're

155
00:08:32.970 --> 00:08:36.650
supposed to repeat in order to, to gain favor within that group,

156
00:08:36.680 --> 00:08:39.490
make them make sure they know you're part of the loyalists there.

157
00:08:40.580 --> 00:08:42.440
If you don't say the things, then, then,

158
00:08:42.720 --> 00:08:44.280
then that group gets distrustful of you. We,

159
00:08:44.520 --> 00:08:46.040
but this is a problem we have on the right. Right.

160
00:08:46.041 --> 00:08:48.600
So I think the left is power hungry. I think the right is paranoid.

161
00:08:49.060 --> 00:08:52.320
And we tend to look for, for betrayers in our, in our midst, right?

162
00:08:52.321 --> 00:08:53.480
We look for wait, who's paranoid.

163
00:08:53.760 --> 00:08:54.320
<v 1>Who's power hungry. Which.</v>

164
00:08:54.320 --> 00:08:56.040
<v 0>One? The right, the right is more paranoid. Can you.</v>

165
00:08:56.040 --> 00:08:58.600
<v 1>Switch it back and forth though? Left is paranoid sometimes too.</v>

166
00:08:58.670 --> 00:09:01.360
<v 0>Look, look, everybody's on a spectrum. Right. So, so,</v>

167
00:09:02.040 --> 00:09:03.680
and I should say that in the beginning, right? I'm talking,

168
00:09:03.740 --> 00:09:07.280
I'm analyzing in the aggregate, the coronavirus. Why, why,

169
00:09:07.540 --> 00:09:10.240
why people fell on partisan lines on that? But it mean, look,

170
00:09:10.280 --> 00:09:13.840
I recognize that not all liberals are risk averse to a dis extraordinary degree.

171
00:09:14.160 --> 00:09:17.070
I read, recognize that we're all on a spectrum. Um, and from,

172
00:09:17.310 --> 00:09:19.750
from the left to the right, but, but in the, in the aggregate,

173
00:09:19.751 --> 00:09:23.870
this is sort of what we see. And then in politics, as you're talking about, we,

174
00:09:24.030 --> 00:09:27.550
we put on these jerseys and so I'm just talking in generalities, right? Okay.

175
00:09:27.790 --> 00:09:31.830
Of, of course the left can be ultra paranoid. Um, and of course the right and,

176
00:09:31.990 --> 00:09:36.540
and it's extreme form can, can, can exhibit power, hungry tendencies, but it,

177
00:09:36.960 --> 00:09:38.380
but it tends not to be. And if you,

178
00:09:38.540 --> 00:09:41.340
and if we look at the policies actually being implemented,

179
00:09:41.620 --> 00:09:44.740
that that tends not to be the case, but what I see on my side,

180
00:09:44.940 --> 00:09:48.660
cause I'm always dealing with my side. We, we tend to be looking instead of,

181
00:09:48.661 --> 00:09:50.820
instead of thinking how to persuade,

182
00:09:50.821 --> 00:09:54.740
this is the problem I have and I'm trying to change the, that I have.

183
00:09:54.780 --> 00:09:57.810
I mean that, I think we have, I,

184
00:09:58.390 --> 00:10:00.810
we talk about fighting all the time and I say, look,

185
00:10:00.811 --> 00:10:04.370
we have to define fighting as persuasion. Persuasion is,

186
00:10:04.650 --> 00:10:06.450
is the name of the game in politics. Look, I can,

187
00:10:06.770 --> 00:10:10.930
I can go charge a hill as a seal and that's fighting, right. I mean,

188
00:10:11.210 --> 00:10:13.450
and it looks cool, but I'm gonna die. Okay.

189
00:10:14.170 --> 00:10:18.680
What I really should do is communicate, uh, maneuver and, and kill the enemy.

190
00:10:18.790 --> 00:10:23.120
That enemy, that way in politics, the, the fight must be persuasion. And it,

191
00:10:23.240 --> 00:10:27.360
and too often, we, we get more concerned with, you know, saying the things,

192
00:10:27.500 --> 00:10:31.080
saying the slogan, saying the things that make us feel good that make us that,

193
00:10:32.120 --> 00:10:32.680
that, that,

194
00:10:32.680 --> 00:10:36.830
that help us recognize one another as part of the same team wearing named

195
00:10:36.831 --> 00:10:41.030
Jersey. And I think that's what he's getting at it. If somebody veers from that,

196
00:10:41.060 --> 00:10:42.550
well, they're a trader, uh,

197
00:10:42.551 --> 00:10:45.710
they're not one of you and then they're automatically wrong. And he, and,

198
00:10:45.910 --> 00:10:48.910
and instead of saying, uh, I knew you were gonna say that, let me,

199
00:10:49.130 --> 00:10:51.550
let me tell you why it's wrong. Let me explain it to you.

200
00:10:51.551 --> 00:10:52.550
Let's have a debate about it.

201
00:10:52.551 --> 00:10:56.790
We get really mad and we go online and we call names because we haven't actually

202
00:10:56.791 --> 00:11:01.060
done in the background work to at least understand why we think what we think.

203
00:11:01.520 --> 00:11:03.580
And when you understand why you think what you think,

204
00:11:03.581 --> 00:11:05.060
that's how you can persuade people.

205
00:11:05.870 --> 00:11:09.900
<v 1>Catch new episodes that Joe Rogan experience for free only on Spotify,</v>

206
00:11:10.470 --> 00:11:13.300
watch back catalog J E videos on Spotify,

207
00:11:13.810 --> 00:11:18.410
including clips easily seamlessly between video

208
00:11:18.830 --> 00:11:20.850
and audio experience on Spotify.

209
00:11:21.310 --> 00:11:25.650
You can listen to the JRE in the background while using other apps and can

210
00:11:25.890 --> 00:11:29.530
download episodes to save on data cost. All for free.

211
00:11:29.960 --> 00:11:32.410
Spotify is absolutely free.

212
00:11:32.670 --> 00:11:36.210
You don't have to have a premium account to watch new JRE episodes.

213
00:11:36.550 --> 00:11:39.880
You just need to search for the J on your Spotify app.

214
00:11:40.380 --> 00:11:45.080
Go to Spotify now to get this full episode of the Joe Rogan experience.

