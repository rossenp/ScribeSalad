WEBVTT

1
00:00:01.020 --> 00:00:02.490
<v 0>The Joe Rogan experience.</v>

2
00:00:02.760 --> 00:00:05.970
Now you pay attention to like Boston dynamics and all these,

3
00:00:06.330 --> 00:00:09.270
all these different robotic creations that they've made.

4
00:00:09.810 --> 00:00:14.010
<v 1>Well, they seem to have a pension as far as doing the really sinister looking,</v>

5
00:00:14.550 --> 00:00:15.120
but.

6
00:00:15.120 --> 00:00:17.520
<v 0>I think all robots that are, you know,</v>

7
00:00:17.521 --> 00:00:20.500
anything that looks autonomous is kind of sinister looking if it can be.

8
00:00:20.880 --> 00:00:21.810
<v 1>I see the Japan. Yeah.</v>

9
00:00:22.140 --> 00:00:25.140
When like the Japanese have these like big guides rounded.

10
00:00:25.200 --> 00:00:27.690
So it's a different type trying to trick us. Boston dynamics is,

11
00:00:27.691 --> 00:00:30.630
I guess they want the Pentagon to give them funding or something.

12
00:00:31.590 --> 00:00:36.000
<v 0>Yeah. They look like they're developing terminators. Yeah. Yeah. But,</v>

13
00:00:36.001 --> 00:00:40.860
but what I was thinking is if we do

14
00:00:41.220 --> 00:00:45.870
eventually come to a time where those things are going to war for us,

15
00:00:46.650 --> 00:00:50.340
instead of us, like if we get involved in robot wars,

16
00:00:51.000 --> 00:00:53.340
our robots versus their robots,

17
00:00:53.490 --> 00:00:57.390
and this becomes the next motivation for increased technological innovation to

18
00:00:57.391 --> 00:01:01.530
try to deal with superior robots by the Soviet union or by China.

19
00:01:02.190 --> 00:01:02.640
Like these,

20
00:01:02.640 --> 00:01:06.990
these are more things that could be threats that could push people to some crazy

21
00:01:07.230 --> 00:01:10.770
level of technological innovation. Yeah.

22
00:01:11.170 --> 00:01:12.390
<v 1>It could. I mean,</v>

23
00:01:12.391 --> 00:01:17.040
I think there are other drivers for technological innovation as well. Um,

24
00:01:17.700 --> 00:01:20.760
that, that seems so plenty. Um,

25
00:01:20.790 --> 00:01:24.750
strong commercial drivers. Let us say, um,

26
00:01:24.840 --> 00:01:27.450
that we wouldn't have to rely on,

27
00:01:27.480 --> 00:01:32.250
on war or the threat of war to kind of stay innovative. Um,

28
00:01:32.610 --> 00:01:34.440
and I mean, there,

29
00:01:34.441 --> 00:01:39.420
hasn't been this effort to try to see if it would be possible to have some

30
00:01:39.421 --> 00:01:44.370
kind of ban on lethal autonomous weapons. Um, just as,

31
00:01:44.400 --> 00:01:47.520
I mean, there are, there are a few set technologies that we have, like there is,

32
00:01:47.580 --> 00:01:51.180
has been a relatively successful ban on chemical and biological weapons,

33
00:01:52.650 --> 00:01:57.210
um, which by and large been, you know, uh, honored and upheld,

34
00:01:58.710 --> 00:02:02.550
um, there, there are kind of treaties on nuclear weapons,

35
00:02:02.551 --> 00:02:06.720
which has limited proliferation. Yes, there are now maybe I dunno,

36
00:02:06.960 --> 00:02:09.180
a dozen, I don't know the exact number,

37
00:02:09.450 --> 00:02:14.130
but it's certainly a lot better than 50 or a hundred countries. Um,

38
00:02:14.490 --> 00:02:19.020
and some other weapons as well, uh, blinding lasers, um,

39
00:02:19.230 --> 00:02:22.590
landmines, cluster munitions. And so, so, so some people think, man,

40
00:02:22.600 --> 00:02:26.130
maybe we could do something like this with, um, lethal autonomous weapons,

41
00:02:26.131 --> 00:02:30.540
killer bots that do we, is that really what humanity needs most now?

42
00:02:30.541 --> 00:02:33.750
Like another arms race to develop like killer bots.

43
00:02:33.900 --> 00:02:37.680
It seems arguably the answer to that is not. Um,

44
00:02:38.700 --> 00:02:41.310
but I've, I've, I've kind of,

45
00:02:42.150 --> 00:02:43.830
it was a lot of my friends are supportive.

46
00:02:43.860 --> 00:02:48.450
I kind of stayed a little bit on the sidelines on that particular campaign

47
00:02:48.630 --> 00:02:53.460
being a little unsure, um, exactly what it is that w I mean,

48
00:02:53.461 --> 00:02:57.300
certainly I'd say it's better if we refrain from having some arms race to

49
00:02:57.301 --> 00:03:01.030
develop these that not. But if you start to look in more detail,

50
00:03:01.060 --> 00:03:03.670
what precisely is the thing that you're hoping to ban?

51
00:03:04.000 --> 00:03:06.190
So if the idea is the autonomous bit,

52
00:03:06.430 --> 00:03:09.550
like the robot should not be able to make its own firing decision. Well,

53
00:03:09.970 --> 00:03:13.810
if the alternative to that is, um,

54
00:03:14.140 --> 00:03:18.250
there is some 19 year old guys sitting in some office building and his job is

55
00:03:18.280 --> 00:03:22.720
whenever the screen flashes fire. Now he has to press a red button.

56
00:03:23.500 --> 00:03:25.220
And then exactly the same thing happens. I mean, I,

57
00:03:25.221 --> 00:03:29.410
I'm not sure how much he's gained by having that extra step. Um.

58
00:03:29.620 --> 00:03:31.990
<v 0>But it is something, it feels better for us,</v>

59
00:03:32.020 --> 00:03:34.120
for some reason somebody's pushing the button.

60
00:03:34.480 --> 00:03:38.350
<v 1>But exactly. What does that mean, like in an ever particular fire decision?</v>

61
00:03:38.351 --> 00:03:41.440
Or is it like some, well,

62
00:03:41.580 --> 00:03:44.770
you've got to attack this group of surface ships here and in like,

63
00:03:44.771 --> 00:03:47.860
here are the general parameters and you're not allowed to fire outside this,

64
00:03:48.100 --> 00:03:51.370
these coordinates. Like, I don't know. I mean,

65
00:03:51.371 --> 00:03:55.330
another is the question of, it would be better if we had no wars,

66
00:03:55.360 --> 00:03:59.800
but if there is going to be a war, maybe it is better if it's robots,

67
00:03:59.830 --> 00:04:04.000
the robots, or if, if there's going to be bumping like maybe a one,

68
00:04:04.030 --> 00:04:07.630
the bombs have high precision rather than male precision,

69
00:04:07.810 --> 00:04:09.760
like get fewer civilian casualties.

70
00:04:10.270 --> 00:04:12.340
<v 0>And operating under artificial intelligence. So.</v>

71
00:04:13.920 --> 00:04:17.020
<v 1>It depends exactly on how so. I dunno. I mean, on the other hand,</v>

72
00:04:17.021 --> 00:04:19.600
you could imagine it kind of reduces the threshold for going to war.

73
00:04:19.601 --> 00:04:22.690
If you think that you, you wouldn't fare any casualties,

74
00:04:22.720 --> 00:04:26.430
maybe you would be more eager to do it or are,

75
00:04:26.440 --> 00:04:28.450
or if it proliferates and you have these kinds of,

76
00:04:28.480 --> 00:04:31.780
most Skeeto sized killer bots that terrorists have. And

77
00:04:33.550 --> 00:04:34.390
you know, it's not,

78
00:04:34.620 --> 00:04:37.600
it doesn't seem like a good thing to have a society where you have like a facial

79
00:04:37.601 --> 00:04:40.900
recognition thing and then the bot flies out and you just have a kind of

80
00:04:40.901 --> 00:04:42.910
dystopia. So.

81
00:04:44.620 --> 00:04:48.160
<v 0>Um, yeah, I think we're thinking rationally, if you were thinking rationally,</v>

82
00:04:48.550 --> 00:04:53.200
look given the overall view of the human race that we want peace and

83
00:04:53.530 --> 00:04:56.380
everything to be well, but realistically,

84
00:04:56.530 --> 00:05:00.670
if you were someone who is trying to attack someone militarily,

85
00:05:00.671 --> 00:05:03.520
you would want the best possible weapons that give you the best possible

86
00:05:03.521 --> 00:05:04.354
advantage.

87
00:05:04.840 --> 00:05:08.650
And that's why we had to develop the atomic bomb first.

88
00:05:08.800 --> 00:05:11.110
That's probably why we'll develop the,

89
00:05:11.500 --> 00:05:15.130
or we'll try to develop the killer autonomous robot first.

90
00:05:16.030 --> 00:05:19.210
<v 1>Yeah, yeah. Let me have it right. It's a fair that the others,</v>

91
00:05:19.690 --> 00:05:22.810
so this is why it's basically a coordination problem.

92
00:05:23.320 --> 00:05:28.150
Like it's hard for any one country unilaterally to make

93
00:05:28.151 --> 00:05:33.010
sure that the world is, is peaceful on short and kind, right. It, it, it,

94
00:05:33.011 --> 00:05:37.780
it requires everybody to synchronize their actions and then you can have

95
00:05:37.900 --> 00:05:39.940
successes like we've had with some of these treaties,

96
00:05:39.970 --> 00:05:43.660
like we've not had a big arms race in biological weapons, uh,

97
00:05:43.690 --> 00:05:46.360
or in chemical weapons. I mean, there have been that were,

98
00:05:46.780 --> 00:05:49.030
there were cheaters even on the biological warfare program,

99
00:05:49.031 --> 00:05:53.290
like the Soviet union had massive efforts there, but still probably, uh,

100
00:05:53.291 --> 00:05:57.010
less use of that and less development. And if there had been no such treaty,

101
00:05:58.490 --> 00:06:03.380
um, so, and are just look at the amount of money being wasted, uh,

102
00:06:04.080 --> 00:06:04.081
every,

103
00:06:04.081 --> 00:06:07.580
every year to maintain these large arsenals so that we can kill one another.

104
00:06:07.581 --> 00:06:11.000
If one day we decide to do it like that, there's gotta be a better way.

105
00:06:11.990 --> 00:06:12.980
Getting there is hard. Yeah.

106
00:06:13.070 --> 00:06:16.070
<v 0>We would hope that we would get to some point where all this would be irrelevant</v>

107
00:06:16.460 --> 00:06:17.690
because there's no more war.

108
00:06:18.260 --> 00:06:21.260
<v 1>Yeah. And so if you look at the, uh,</v>

109
00:06:21.810 --> 00:06:25.100
th the biggest efforts so far to, to make that top of this,

110
00:06:25.101 --> 00:06:29.000
after the first world war, people were really aware of that said, this,

111
00:06:29.001 --> 00:06:32.960
this sucks like war. I mean, look at this like a whole generation,

112
00:06:32.961 --> 00:06:34.910
just ground up machine guns, like this,

113
00:06:35.810 --> 00:06:37.130
gotta make sure this never happens again.

114
00:06:37.131 --> 00:06:39.110
So they tried to do the league of nation,

115
00:06:39.710 --> 00:06:43.760
but then didn't really invest it with very much power. Um,

116
00:06:44.180 --> 00:06:47.480
and then the second war, second world war happened. And so then again,

117
00:06:47.510 --> 00:06:50.360
just after that, it's fresh in people's memories saying, well, never again,

118
00:06:50.450 --> 00:06:55.190
this is it, uh, the United nations. And in Europe, the European union,

119
00:06:55.700 --> 00:06:59.120
it's kind of both designed as ways to try to prevent this. But again,

120
00:06:59.121 --> 00:07:02.150
with kind of, maybe in the case of the United nations,

121
00:07:02.151 --> 00:07:06.260
quite limited powers to actually enforce the agreements. And there's a veto,

122
00:07:06.261 --> 00:07:10.070
which makes it hard if it's two of the major powers that are at loggerheads.

123
00:07:11.270 --> 00:07:14.930
Um, so it might be that if that were a third,

124
00:07:15.470 --> 00:07:18.350
big conflagration, that then people would say, well, you know, this time,

125
00:07:18.800 --> 00:07:20.600
you know, we've got to really, uh,

126
00:07:21.530 --> 00:07:24.560
we've got to really put something kind of institutional solution in place that,

127
00:07:24.561 --> 00:07:27.860
that has enough enforcement power. That, that we don't try this.

128
00:07:27.980 --> 00:07:32.030
<v 0>Yeah. Again, so we don't have a second robot war. So once we get to.</v>

129
00:07:33.080 --> 00:07:36.260
<v 1>I mean, but, but it kind of memories fade, right? So that's, that's the problem.</v>

130
00:07:36.860 --> 00:07:40.370
Um, so even the cold war, I mean, I grew up, I I'm Swedish.

131
00:07:40.400 --> 00:07:44.060
I remember we were kind of in between. Right. And,

132
00:07:44.061 --> 00:07:47.630
and we were taught to the schools about nuclear fall out and stuff like that.

133
00:07:47.750 --> 00:07:50.600
It's like a very palpable sense that at any given point in time,

134
00:07:51.080 --> 00:07:55.820
that could be some miscalculation or crisis or some something. Um,

135
00:07:56.480 --> 00:07:57.560
and,

136
00:07:58.280 --> 00:08:02.230
and like all the way up to see seniors statesman at the time, this is,

137
00:08:02.231 --> 00:08:05.720
this was like a very real and very serious. And I, I feel this,

138
00:08:05.870 --> 00:08:09.620
that memory of just how bad it is to live in that kind of, um,

139
00:08:09.740 --> 00:08:13.880
hair-trigger nuclear arms, race called war situation has kind of faded.

140
00:08:13.881 --> 00:08:17.060
And now it's like, wow, you know, the world didn't blow up.

141
00:08:17.061 --> 00:08:19.190
So maybe it wasn't so bad after all. Well,

142
00:08:19.191 --> 00:08:23.010
I think that would be the wrong lesson to learn. It's a bit like you're,

143
00:08:23.011 --> 00:08:26.720
you're playing Russian roulette and you survive one and you're saying, well,

144
00:08:26.780 --> 00:08:28.640
it isn't so dangerous at all to play Russian,

145
00:08:28.641 --> 00:08:30.500
rather that I think I'm gonna have another guy, like,

146
00:08:31.340 --> 00:08:32.180
you've got to realize like, well,

147
00:08:32.181 --> 00:08:36.350
maybe that was a 10% chance or a 30% chance that the world would prove up during

148
00:08:36.351 --> 00:08:37.520
the cold war. And we were lucky,

149
00:08:37.850 --> 00:08:39.470
but it doesn't mean we want to have another one.

150
00:08:40.010 --> 00:08:42.890
<v 0>When I was in high school, it was a real threat. When I was in high school,</v>

151
00:08:42.891 --> 00:08:45.230
everyone was terrified that we were going to go to war with Russia.

152
00:08:46.060 --> 00:08:47.120
It was the big thing. And,

153
00:08:47.870 --> 00:08:50.510
and you talked to people from my generation about that,

154
00:08:50.511 --> 00:08:51.770
and everybody remembers it.

155
00:08:51.900 --> 00:08:56.460
Remember that feeling that you had in high school that like, we, there any,

156
00:08:56.520 --> 00:08:58.410
at any day, something could go wrong.

157
00:08:58.500 --> 00:09:03.390
We could be at war with another country. That's a nuclear superpower. Yeah.

158
00:09:03.660 --> 00:09:06.840
But that's all gone now. Like that, that feeling, that fear,

159
00:09:07.170 --> 00:09:09.990
people are so confident that that's not going to happen. That the sun,

160
00:09:09.991 --> 00:09:11.490
even in people's consciousness. Yeah.

161
00:09:12.420 --> 00:09:13.253
<v 1>And</v>

162
00:09:15.210 --> 00:09:18.270
then a number of maneuvers are made.

163
00:09:18.271 --> 00:09:23.040
And then you find yourself in a kind of situation where there's like

164
00:09:23.160 --> 00:09:26.130
honored stake and reputation on you feel it can't back down.

165
00:09:26.131 --> 00:09:27.450
And then another thing happens.

166
00:09:27.451 --> 00:09:30.870
And you get into this place where if you even say something kind about the other

167
00:09:30.871 --> 00:09:33.090
side, you saying to be like, you know,

168
00:09:33.091 --> 00:09:36.140
you're a soft or a pinky or alive and on,

169
00:09:36.300 --> 00:09:37.650
on both sides and the other side as well,

170
00:09:37.651 --> 00:09:40.860
obviously they're going to have the same internal dynamic and each side says bad

171
00:09:40.861 --> 00:09:43.300
things about the other. It makes the other side hate them even more.

172
00:09:43.890 --> 00:09:45.570
And these things are done hard to reverse.

173
00:09:45.571 --> 00:09:48.870
Like once you find this dynamic happening, it's kind of almost, oh,

174
00:09:48.871 --> 00:09:50.370
it's not too late. You can try it, but,

175
00:09:50.980 --> 00:09:52.710
but it can be very hard to back out of that.

176
00:09:53.250 --> 00:09:57.440
And so if it can prevent yourself from going down that path to begin with that,

177
00:09:57.441 --> 00:09:58.980
that's much preferable.

178
00:09:59.250 --> 00:10:01.920
<v 0>When you see Boston dynamics and you see those robots,</v>

179
00:10:01.950 --> 00:10:05.790
is there something comparable that's being developed either in the Soviet union

180
00:10:05.791 --> 00:10:09.900
or in China or somewhere else in the world where there's similar type robots?

181
00:10:10.680 --> 00:10:10.861
<v 1>Well,</v>

182
00:10:10.861 --> 00:10:15.750
I think a lot of the Boston dynamic thing seems more showy than actually useful

183
00:10:16.470 --> 00:10:20.810
that these kind of, um, animal like things the top around and,

184
00:10:20.811 --> 00:10:24.200
and with 150 decibel or something second,

185
00:10:24.201 --> 00:10:26.970
like if I were a special ops trying to sneak in, like,

186
00:10:26.971 --> 00:10:31.600
I wouldn't want that this kind of big alarm, but,

187
00:10:31.601 --> 00:10:35.310
but I think a lot of action would be more in terms of, um,

188
00:10:35.340 --> 00:10:39.990
flying drones, uh, maybe a submarine stuff, missiles,

189
00:10:41.310 --> 00:10:42.090
um, that kind of stuff.

