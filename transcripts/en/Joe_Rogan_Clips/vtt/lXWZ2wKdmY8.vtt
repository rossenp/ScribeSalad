WEBVTT

1
00:00:01.020 --> 00:00:02.760
<v 0>The Joe Rogan experience.</v>

2
00:00:03.180 --> 00:00:06.330
Artificial intelligence is something that people are terrified of as an

3
00:00:06.331 --> 00:00:09.060
existential threat. They think of it as one day,

4
00:00:09.061 --> 00:00:11.880
you're going to turn something on and it's going to be sentient.

5
00:00:11.970 --> 00:00:16.110
And it's going to be able to create other forms of artificial intelligence that

6
00:00:16.111 --> 00:00:18.840
are exponentially more powerful than the one that we created.

7
00:00:19.350 --> 00:00:24.210
And that we'll have unleashed this beast that we cannot control what my

8
00:00:24.211 --> 00:00:28.290
concern is with all this. Yeah, that's my concern. My concern is that this,

9
00:00:28.291 --> 00:00:29.124
this is a,

10
00:00:29.690 --> 00:00:33.660
a slow acceptance of drowning.

11
00:00:34.410 --> 00:00:38.640
That's like a slow or, okay. I'm only up to my knees. Uh, it's fine.

12
00:00:38.670 --> 00:00:43.110
It's just to my waist high. I can still boiling water. Right? Exactly.

13
00:00:43.230 --> 00:00:46.410
Exactly. It seems like this is like.

14
00:00:46.940 --> 00:00:51.770
<v 1>Have to fight back to reclaim our autonomy and freewill from the</v>

15
00:00:51.771 --> 00:00:56.750
machines. I mean, one clear it's very much the matrix.

16
00:00:56.751 --> 00:00:57.584
It's neat.

17
00:00:58.030 --> 00:01:01.160
And one of my favorite lines is actually when the Oracle says to Neo and don't

18
00:01:01.161 --> 00:01:03.770
worry about the vase and he says, what face? And he knocks it over.

19
00:01:03.890 --> 00:01:05.480
It's just that face. And so it's like,

20
00:01:05.690 --> 00:01:08.750
she's the AI who sees so many moves ahead on the chess board.

21
00:01:08.870 --> 00:01:12.140
She can say something which will cause him to do the thing that verifies the

22
00:01:12.141 --> 00:01:15.650
thing that she predicted what happened. Yeah. That's what AI is doing now,

23
00:01:15.680 --> 00:01:19.760
except it's pointed at our nervous system and figuring out the perfect thing to

24
00:01:19.761 --> 00:01:23.030
dangle in front of our dopamine system and get the thing to happen,

25
00:01:23.031 --> 00:01:26.420
which instead of knocking off devices to be outraged at the other political side

26
00:01:26.421 --> 00:01:28.910
and be fully certain that you're right. Even though it's just a machine,

27
00:01:28.911 --> 00:01:31.880
that's calculating, that's gonna make you, you know, do the thing.

28
00:01:32.240 --> 00:01:33.500
<v 0>When you're concerned about this.</v>

29
00:01:33.530 --> 00:01:37.580
How much time do you spend thinking about simulation theory, the simulation?

30
00:01:37.880 --> 00:01:38.240
Yeah,

31
00:01:38.240 --> 00:01:43.010
the idea that if not currently one day there'll be a simulation that's

32
00:01:43.011 --> 00:01:46.820
indiscernible from regular reality. And it seems we're on that path.

33
00:01:46.850 --> 00:01:49.460
I don't know if you mess around with VR at all, but, well.

34
00:01:49.460 --> 00:01:51.590
<v 1>This is the point about, you know, the virtual chat-bots out,</v>

35
00:01:51.591 --> 00:01:55.490
competing on the wheel, for instance, the technology, you know, I mean, that's,

36
00:01:55.491 --> 00:02:00.200
what's happening is that reality is getting more and more virtual because we

37
00:02:00.201 --> 00:02:04.070
interact with a virtual news system. That's all this sort of clickbait economy,

38
00:02:04.071 --> 00:02:05.150
outrage machine.

39
00:02:05.240 --> 00:02:08.480
That's already a virtual pilot political environment that then translates into

40
00:02:08.481 --> 00:02:10.790
real world action then becomes real. And that's the weird.

41
00:02:11.330 --> 00:02:12.830
<v 0>Go back to 1990,</v>

42
00:02:12.831 --> 00:02:16.790
whatever it was when the internet became mainstream or at least started becoming

43
00:02:17.030 --> 00:02:17.863
mainstream.

44
00:02:17.930 --> 00:02:21.590
And then the small amount of time that it took that 20 plus years to get to

45
00:02:21.591 --> 00:02:24.980
where we are now. And then think what,

46
00:02:25.300 --> 00:02:27.230
what about the virtual world?

47
00:02:27.350 --> 00:02:32.300
And once this becomes something that's has

48
00:02:32.301 --> 00:02:35.870
the same sort of rate of growth that the internet has experienced or that we've

49
00:02:35.871 --> 00:02:37.520
experienced through the internet. I mean,

50
00:02:37.630 --> 00:02:41.660
we we're looking at like 20 years from now being unrecognizable. Yeah.

51
00:02:41.780 --> 00:02:42.980
We're looking at, I mean, it's,

52
00:02:43.010 --> 00:02:47.930
it almost seems like that is what life does the same way bees

53
00:02:47.931 --> 00:02:50.930
create beehives, uh, you know, a, uh,

54
00:02:50.960 --> 00:02:53.720
caterpillar doesn't know what the going on when it gets into that cocoon,

55
00:02:53.721 --> 00:02:55.010
but it's becoming a butterfly. Yeah.

56
00:02:55.310 --> 00:03:00.010
We seem to be a thing that creates

57
00:03:00.250 --> 00:03:04.430
newer and better objects. Correct. More effective. You.

58
00:03:04.430 --> 00:03:08.610
<v 1>Have to realize AI is not conscious and won't be conscious the way we are.</v>

59
00:03:08.710 --> 00:03:11.190
And so many people, um, think that.

60
00:03:11.400 --> 00:03:15.150
<v 0>It is consciousness essential. I think so to us.</v>

61
00:03:15.690 --> 00:03:18.000
<v 1>I don't know, essential in the sense of we're the only ones who have it now.</v>

62
00:03:18.001 --> 00:03:19.350
I don't know that theory.

63
00:03:19.440 --> 00:03:22.170
<v 0>Well, there might be more things that have consciousness.</v>

64
00:03:23.230 --> 00:03:26.850
<v 1>Is it essential? I mean, it's the, to the extent that choice exists,</v>

65
00:03:26.910 --> 00:03:29.490
it would exist through some kind of consciousness.

66
00:03:29.580 --> 00:03:31.620
And the choice is choice essential.

67
00:03:33.030 --> 00:03:36.570
<v 0>It's essential to us as we know it, like as life as we know it.</v>

68
00:03:37.170 --> 00:03:40.350
But my worry is that we're in essential.

69
00:03:40.351 --> 00:03:44.100
W w w we're thinking now, like single celled organisms being like, Hey,

70
00:03:44.101 --> 00:03:47.370
I don't want to gang up with a bunch of other people and become an object that

71
00:03:47.371 --> 00:03:50.850
can walk. I like being a single cell organism. This is a lot of fun.

72
00:03:51.270 --> 00:03:53.010
<v 1>I mean, I hear you saying, you know,</v>

73
00:03:53.130 --> 00:03:56.640
are we a bootloader for the AI that then runs the levels?

74
00:03:56.670 --> 00:03:58.920
That's the lines perspective. I mean,

75
00:03:58.921 --> 00:04:01.940
I think this is a really dangerous way to think. I mean, we have to yet.

76
00:04:01.941 --> 00:04:04.580
So are we then the species? Yeah. I mean.

77
00:04:05.250 --> 00:04:09.090
<v 0>But what if the next version of life is better being.</v>

78
00:04:09.090 --> 00:04:12.720
<v 1>Run by machines that have no values that don't care that don't have choice and</v>

79
00:04:12.721 --> 00:04:15.960
are just maximizing for things that were programmed in by our little miniature

80
00:04:15.961 --> 00:04:16.740
brains anyway.

81
00:04:16.740 --> 00:04:19.530
<v 0>But they don't cry. They don't commit suicide.</v>

82
00:04:19.560 --> 00:04:22.830
<v 1>But then consciousness in life dies. That could be the future.</v>

83
00:04:23.580 --> 00:04:27.180
I think this is the last chance to try to snap out of that warden.

84
00:04:27.180 --> 00:04:30.450
<v 0>In the eyes of the universe that we do that I don't know. It feels important.</v>

85
00:04:30.600 --> 00:04:35.160
How does it feels important? But I'm a mom, I'm a monkey, you know,

86
00:04:35.190 --> 00:04:38.790
the monkeys like I'm staying in this tree, man, you guys are out of your mind.

87
00:04:38.910 --> 00:04:41.580
<v 1>I mean, this is the weird paradox of being human. Is that again,</v>

88
00:04:41.581 --> 00:04:44.730
we have these lower level emotions. We care about social approval.

89
00:04:44.731 --> 00:04:47.010
We can't not care at the same time. Like I said,

90
00:04:47.310 --> 00:04:48.540
there's this weird proposition here.

91
00:04:48.541 --> 00:04:51.240
We're the only species that if this were to happen to us,

92
00:04:51.720 --> 00:04:55.440
we would have the self-awareness to even know that it was happening, right?

93
00:04:55.441 --> 00:04:57.390
Like we can consent like this two hour interview.

94
00:04:57.391 --> 00:05:01.380
We can conceptualize that this, this thing has happened to us, right?

95
00:05:01.410 --> 00:05:04.170
That we have built this matrix. It's external object,

96
00:05:04.260 --> 00:05:07.230
which has like AI and supercomputers and voted all versions of each of us.

97
00:05:07.830 --> 00:05:11.760
And it has perfectly figured out how to predictably move each of us in this

98
00:05:11.761 --> 00:05:12.390
matrix.

99
00:05:12.390 --> 00:05:15.660
<v 0>Let me pose this to you. We are what we are now.</v>

100
00:05:15.750 --> 00:05:19.950
Human beings homosapiens in 2020, we are this thing that, uh,

101
00:05:19.980 --> 00:05:22.170
if you believe in evolution, I'm pretty sure you do.

102
00:05:22.500 --> 00:05:26.100
We've evolved over the course of millions of years to become who we are right

103
00:05:26.101 --> 00:05:30.300
now. Should we stop right here? Are we done? No. Right.

104
00:05:31.050 --> 00:05:34.350
We should keep it evolving. What does that look like?

105
00:05:37.350 --> 00:05:41.280
Yeah. What does it look like? If we go ahead, just forget about social media.

106
00:05:41.610 --> 00:05:46.290
What would you like us to be in a thousand years

107
00:05:46.320 --> 00:05:49.620
or a hundred thousand years or 500,000 years?

108
00:05:49.621 --> 00:05:52.200
You certainly wouldn't want us to be what we are right now, right?

109
00:05:52.830 --> 00:05:53.790
No one would know.

110
00:05:54.780 --> 00:05:57.470
<v 1>This is what visions of star Trek and things like that. We're trying to ask,</v>

111
00:05:57.471 --> 00:05:59.780
right? Like, Hey, let's imagine humans do make it.

112
00:05:59.810 --> 00:06:02.270
And we become the most enlightened we can be.

113
00:06:02.480 --> 00:06:06.050
And we actually somehow make peace with these other, you know, alien tribes.

114
00:06:06.051 --> 00:06:09.560
And we figured out, you know, space travel and all of that. I mean,

115
00:06:10.560 --> 00:06:14.030
actually a good heuristic that I think people can ask is on an enlightened

116
00:06:14.031 --> 00:06:17.590
planet where we did figure this out. What would that have looked like? Isn't.

117
00:06:17.590 --> 00:06:21.310
<v 0>It always weird that those movies it's, people are just people,</v>

118
00:06:22.000 --> 00:06:25.360
but they're in some weird future, but they haven't really changed that much.

119
00:06:26.740 --> 00:06:27.573
<v 1>Right? I mean,</v>

120
00:06:27.580 --> 00:06:31.930
which is to say that the fundamental way that we work is just

121
00:06:31.931 --> 00:06:35.260
unchanging, but we are artists, such things as more wise societies,

122
00:06:35.261 --> 00:06:38.110
more sustainable societies, more peaceful or harmonious societies,

123
00:06:38.140 --> 00:06:40.780
but the Janes, you know, well, intimately, biologically.

124
00:06:40.780 --> 00:06:43.960
<v 0>We have to evolve as well. But our version of that,</v>

125
00:06:43.961 --> 00:06:48.400
like the best version of that is probably the gray aliens. Right? Maybe.

126
00:06:48.580 --> 00:06:49.600
So that's the ultimate future.

127
00:06:49.900 --> 00:06:54.150
<v 1>I mean, we're going to get into gene editing and becoming more perfect, perfect.</v>

128
00:06:54.310 --> 00:06:56.470
On the sense of, you know, that, but, uh,

129
00:06:56.620 --> 00:07:00.910
we are going to start optimizing for what are the outcomes that we value?

130
00:07:00.911 --> 00:07:04.900
I think the question is how do we actually come up with brand new values that

131
00:07:04.901 --> 00:07:08.860
are wiser than we've ever thought of before that actually are able to transcend

132
00:07:08.861 --> 00:07:12.490
the win, lose games that lead to Omni lose, lose that everyone loses.

133
00:07:12.700 --> 00:07:15.730
If we keep playing the win lose game at greater and greater scales.

134
00:07:16.210 --> 00:07:20.200
<v 0>I like you have a vested interest in the biological existence of human beings.</v>

135
00:07:20.230 --> 00:07:22.900
I think people are pretty cool. I love being around them.

136
00:07:22.901 --> 00:07:24.010
I enjoy talking to you today.

137
00:07:24.550 --> 00:07:28.990
My fear is that we are we're,

138
00:07:29.920 --> 00:07:32.740
we're a model T right. You know, and there's,

139
00:07:32.790 --> 00:07:36.250
there's no sense in making those things anymore. The brakes are terrible.

140
00:07:36.730 --> 00:07:40.360
They smell like. When you drive them, they don't go very fast.

141
00:07:41.050 --> 00:07:42.090
We need a better version.

142
00:07:42.580 --> 00:07:46.120
<v 1>You know, the funny thing is God, there's some quote by someone. I think like,</v>

143
00:07:46.750 --> 00:07:47.950
I wish I could remember it.

144
00:07:47.951 --> 00:07:51.430
It's something about how much would be solved if we were at peace with

145
00:07:51.431 --> 00:07:56.230
ourselves. Like if we were able to just be okay with nothing,

146
00:07:56.470 --> 00:08:00.520
like just being okay with living and breathing, I don't mean to be, you know,

147
00:08:00.610 --> 00:08:01.960
playing the woo new age card.

148
00:08:01.961 --> 00:08:06.550
I just genuinely mean how much of our lives is just running away from, you know,

149
00:08:06.580 --> 00:08:08.320
anxiety and discomfort and aversion.

150
00:08:09.700 --> 00:08:14.350
<v 2>Episodes of the Joe Rogan experience are now free on Spotify. That's right.</v>

151
00:08:14.590 --> 00:08:17.500
They're free from September 1st is December 1st.

152
00:08:17.501 --> 00:08:19.930
They're going to be available everywhere, but after December 1st,

153
00:08:19.931 --> 00:08:23.620
they will only be available on Spotify, but they will be free.

154
00:08:23.830 --> 00:08:27.640
That includes the video. The video will also be there. It'll also be free.

155
00:08:28.330 --> 00:08:32.440
That's all we're asking. Go download Spotify. Buh-bye.

