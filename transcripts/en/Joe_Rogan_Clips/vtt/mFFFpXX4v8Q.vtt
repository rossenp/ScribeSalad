WEBVTT

1
00:00:01.020 --> 00:00:03.440
<v 0>The Joe Rogan experience. Now,</v>

2
00:00:03.620 --> 00:00:07.040
one thing that exists now that really didn't exist when Obama was running for

3
00:00:07.041 --> 00:00:09.320
president is the impact of social media. Yeah.

4
00:00:09.321 --> 00:00:11.200
It's just tenfold what it used to be.

5
00:00:11.660 --> 00:00:16.560
But with that also comes this reality that we're

6
00:00:16.561 --> 00:00:17.361
living in right now,

7
00:00:17.361 --> 00:00:20.200
where there's only a few companies that are controlling the discourse in this

8
00:00:20.201 --> 00:00:24.630
country. Yeah. I mean, you really have essentially a Facebook, Google,

9
00:00:25.060 --> 00:00:29.270
YouTube, Twitter, and Facebook owns Instagram. That's right. And there's,

10
00:00:29.271 --> 00:00:33.070
you know, a couple other small ones, but that that's the bulk of our discourse.

11
00:00:33.100 --> 00:00:33.933
Yeah.

12
00:00:34.780 --> 00:00:39.230
What concern do you have about these private companies

13
00:00:39.860 --> 00:00:44.740
controlling vast majority of communication between people on social media?

14
00:00:44.890 --> 00:00:46.660
<v 1>Yeah. It's extremely dangerous.</v>

15
00:00:47.450 --> 00:00:49.140
It's extremely dangerous when you think about it.

16
00:00:49.141 --> 00:00:53.100
And there's a few things there's gosh, with, with Facebook, uh,

17
00:00:53.160 --> 00:00:57.420
and Google for that matter, you know, they, they can set their algorithms,

18
00:00:58.180 --> 00:01:02.650
uh, mark Zuckerberg with Facebook and set his algorithm to what information is

19
00:01:02.651 --> 00:01:04.570
coming across our newsfeed in Facebook.

20
00:01:05.280 --> 00:01:08.970
What are the stories that we're seeing Instagram, same thing, uh,

21
00:01:08.971 --> 00:01:11.290
with Google they can control when you punch in something,

22
00:01:11.291 --> 00:01:14.410
what are the first stories that you're gonna see, uh,

23
00:01:14.590 --> 00:01:17.450
on the first page that pops up, uh,

24
00:01:17.960 --> 00:01:21.770
when you think about that kind of power, uh,

25
00:01:22.150 --> 00:01:24.320
influence that it has on the American people,

26
00:01:24.321 --> 00:01:27.520
literally being held within the hands of a couple of people,

27
00:01:28.270 --> 00:01:32.600
unchecked and without oversight or transparency, it's incredibly dangerous.

28
00:01:33.690 --> 00:01:34.840
Let's talk about free speech.

29
00:01:35.390 --> 00:01:39.760
There's just been news recently about Facebook banning certain individuals,

30
00:01:40.680 --> 00:01:45.270
uh, from having Facebook book accounts because of their speech,

31
00:01:45.271 --> 00:01:48.030
they disagree with the speech that they're they're using or the things that

32
00:01:48.031 --> 00:01:50.910
they're talking about, the ideas that they're pushing forward,

33
00:01:51.820 --> 00:01:55.910
unchecked first amendment rights, going completely out the window. And.

34
00:01:56.010 --> 00:01:58.830
<v 0>The argument is they don't apply because it's a private company, right?</v>

35
00:02:00.050 --> 00:02:04.100
<v 1>Yes. But they're trying to get the best of both worlds. Um,</v>

36
00:02:04.960 --> 00:02:08.140
the fact that, that, you know, they're claiming to say, Hey,

37
00:02:08.141 --> 00:02:12.060
this is a free space for open communication for everyone, right?

38
00:02:12.550 --> 00:02:15.140
While at the same time going and saying, actually, you know what, Joe,

39
00:02:15.141 --> 00:02:16.540
I don't like what you're saying about this.

40
00:02:16.560 --> 00:02:20.540
So we're gonna ban you and whoever your friends are from,

41
00:02:20.770 --> 00:02:25.610
from this conversation. Um, I think that's, that's a big problem. It undermines,

42
00:02:26.270 --> 00:02:29.410
it undermines our first amendment rights. And you look at privacy,

43
00:02:29.790 --> 00:02:33.410
the privacy concerns of all the information that they're collecting in Facebook

44
00:02:33.480 --> 00:02:36.930
from us, all the information that they're collecting from us, uh,

45
00:02:37.040 --> 00:02:41.010
with Google and how they're monetizing that and selling,

46
00:02:41.030 --> 00:02:44.600
or sharing that information with other people with really, without out our,

47
00:02:45.060 --> 00:02:48.000
our knowledge or agreement, that's.

48
00:02:48.000 --> 00:02:48.833
<v 0>The part, right? The,</v>

49
00:02:48.900 --> 00:02:53.800
the agreement that most people didn't understand that your data is a huge

50
00:02:53.801 --> 00:02:56.800
commodity that's right. And we signed up for these things. I mean, who,

51
00:02:56.860 --> 00:03:00.080
who reads terms of service agreements? Yeah. Have you ever read one? Yeah. No,

52
00:03:00.081 --> 00:03:02.360
I've never read one of 'em. I just say, okay. Yeah. All right.

53
00:03:02.560 --> 00:03:05.440
<v 1>I start, I get through like the first two paragraphs, like, okay. I hope.</v>

54
00:03:05.440 --> 00:03:07.400
<v 0>It's not ugly. Hope. There's nothing terrible in there.</v>

55
00:03:07.420 --> 00:03:12.120
<v 1>You know what I think back to there is a, a south park episode &lt;laugh&gt; that?</v>

56
00:03:12.121 --> 00:03:16.880
That is specifically about the terms and conditions &lt;laugh&gt; and, uh, I,

57
00:03:17.040 --> 00:03:18.240
I don't watched it a long time ago,

58
00:03:18.241 --> 00:03:21.630
but every time I see one of those things pop up, sign terms, things, I'm like,

59
00:03:21.631 --> 00:03:22.950
oh man. &lt;laugh&gt; yeah.

60
00:03:23.820 --> 00:03:26.710
<v 0>That was Cartman, right? Yes. Yeah. Something happened. Yeah.</v>

61
00:03:26.850 --> 00:03:30.910
We can't talk about it. Um, but there's,

62
00:03:30.911 --> 00:03:32.590
there's a thing that they're doing. There's,

63
00:03:32.591 --> 00:03:33.430
there's a couple things they're doing,

64
00:03:33.431 --> 00:03:36.310
but one of the things they're doing with this, um, with your data,

65
00:03:36.700 --> 00:03:38.350
they find out what you're interested in.

66
00:03:38.351 --> 00:03:41.180
They find out what you're interested in, engaging on. Yeah.

67
00:03:41.320 --> 00:03:45.100
And for many people that's outrage. Mm-hmm &lt;affirmative&gt;. So for many people,

68
00:03:45.250 --> 00:03:47.620
it's the things that you off the most. Like,

69
00:03:47.640 --> 00:03:50.420
if you have a real problem with Catholic priests,

70
00:03:50.421 --> 00:03:52.340
getting away with having sex with little boys,

71
00:03:52.640 --> 00:03:55.620
you will think that that's happening every minute of every day,

72
00:03:55.720 --> 00:03:58.500
all across the world. Yeah. Because it's gonna be in your newsfeed constantly.

73
00:03:58.501 --> 00:04:01.330
Cause they know that's what makes you in gauge. Right? So your algorithm,

74
00:04:01.470 --> 00:04:05.610
the algorithm is your, your feed is gonna be very different than my feed. Yeah.

75
00:04:05.830 --> 00:04:08.090
Cuz I engage on different things than you do. Yeah.

76
00:04:08.110 --> 00:04:11.690
And the problem with that is even if they're not calculating, if,

77
00:04:11.691 --> 00:04:14.410
if it's not on purpose, they're not trying to get people outraged.

78
00:04:14.411 --> 00:04:17.810
It's not only they're trying to ramblerous but what they are doing is they,

79
00:04:18.360 --> 00:04:22.160
because they have an adporteded mile model, they gravitate towards the,

80
00:04:22.161 --> 00:04:25.040
the outrageous, because that's what people get excited about.

81
00:04:25.220 --> 00:04:27.320
And that's what people make multiple posts about. Exactly.

82
00:04:27.380 --> 00:04:30.280
And that's how they make the revenue. Exactly. Like it's a bad ad model.

83
00:04:30.430 --> 00:04:34.920
It's an, an ad model that inadvertently supports outrage. Yeah.

84
00:04:35.060 --> 00:04:37.080
And it makes people think the worldiviveness.

85
00:04:37.220 --> 00:04:37.680
<v 1>And, and all.</v>

86
00:04:37.680 --> 00:04:41.910
<v 0>Of it. And this, the, the, the tribal boundary between the,</v>

87
00:04:41.970 --> 00:04:45.190
the two sides yeah. On these issues are more tense. And,

88
00:04:45.210 --> 00:04:48.990
and you would think that discourse and the ability to freely communicate would

89
00:04:48.991 --> 00:04:52.310
kind of open that up and people would kind of understand each other better.

90
00:04:52.510 --> 00:04:53.430
Yeah. But it's not happening. Yeah.

91
00:04:53.431 --> 00:04:58.350
It's like think Twitter is a garbage fire all day long. It's just fire. Yeah.

92
00:04:58.351 --> 00:05:01.900
Like you can't post anything about anything and there's people is jumping on

93
00:05:01.901 --> 00:05:03.220
people and yeah. It's,

94
00:05:03.730 --> 00:05:08.580
it's a crazy thing that has happened that we gravitate towards the outrageous.

95
00:05:08.850 --> 00:05:12.380
Yeah. I don't think that should be rewarded financially. Yeah.

96
00:05:12.500 --> 00:05:16.300
I think that's not, if this is just what people go to organically,

97
00:05:16.301 --> 00:05:21.140
that's one thing. But when you're cultivating feeds,

98
00:05:21.280 --> 00:05:25.530
so, or at least your algorithm is cultivating feeds so that people get off,

99
00:05:25.630 --> 00:05:27.690
you're making the country a shittier place. Yeah.

100
00:05:27.840 --> 00:05:30.330
Like you're literally like making things worse.

101
00:05:30.480 --> 00:05:34.570
<v 1>Yeah. Yeah. I mean, this is one of the reasons why, um,</v>

102
00:05:35.330 --> 00:05:38.010
I think a couple of things should happen and, and uh,

103
00:05:38.290 --> 00:05:42.010
I think his name is Chris Hughes who co-founded Facebook with Zuckerberg. I saw.

104
00:05:42.350 --> 00:05:45.480
Yeah. Did you see his article? Yes. Calling for Facebook to be broken up.

105
00:05:45.780 --> 00:05:47.640
<v 0>Yes. And which is crazy. Yeah.</v>

106
00:05:47.641 --> 00:05:51.400
You hear about the person who founded it and he's saying this is out of control.

107
00:05:51.900 --> 00:05:56.320
<v 1>And that's the point right now is, is seeing how powerful,</v>

108
00:05:56.420 --> 00:06:00.400
as you said, guys, like mark Zuckerberg have become and uh,

109
00:06:00.660 --> 00:06:02.600
and how outta control things are.

110
00:06:02.710 --> 00:06:04.840
<v 0>Well, some of it just doesn't make sense. Like here's one,</v>

111
00:06:05.230 --> 00:06:08.960
they just banned Alex Jones. Yep. Not only did they ban Alex Jones,

112
00:06:09.020 --> 00:06:12.640
but you can't talk about Alex Jones. If you wrote,

113
00:06:12.641 --> 00:06:17.310
Alex Jones might be nuts, but is he cute? If you wrote that it would,

114
00:06:17.311 --> 00:06:20.670
you would get a message that says only you can see this message,

115
00:06:21.420 --> 00:06:26.190
this message is stopped at the border from entering into the

116
00:06:26.350 --> 00:06:29.630
Facebook universe. Like what the? It's crazy. You can't just like,

117
00:06:29.631 --> 00:06:32.630
what if you wanna say something funny? Yeah. You can't say something funny. No,

118
00:06:32.690 --> 00:06:35.950
you can only say something if you're criticizing him. Right.

119
00:06:36.100 --> 00:06:39.460
This is what they've said. Wow. So they're telling you how to think,

120
00:06:40.110 --> 00:06:41.700
which is insane.

121
00:06:41.760 --> 00:06:45.660
It is insane because that's not just free SP that's not just like a violation of

122
00:06:45.661 --> 00:06:49.700
free speech. You're literally directing speech. Yeah.

123
00:06:49.930 --> 00:06:52.100
Like that's insane. Yeah. So you're, you're,

124
00:06:52.101 --> 00:06:56.620
you're not even blocking people from doing something hateful or evil.

125
00:06:56.680 --> 00:07:00.810
You're block people from saying something that you disagree with that's right.

126
00:07:01.140 --> 00:07:04.970
Which is, uh, people have sent me messages that said, uh, God bless Alex Jones.

127
00:07:05.310 --> 00:07:08.930
And they say, I, you could only see this message. Facebook sends them a thing,

128
00:07:09.210 --> 00:07:13.010
blocking that message. Wow. That's crazy. Yeah. Like the,

129
00:07:13.011 --> 00:07:16.090
the idea that you think you can do that. Yeah. That is nuts. Yeah. I mean,

130
00:07:16.380 --> 00:07:21.360
being able to ban any one arbitrarily without any just violated terms of

131
00:07:21.361 --> 00:07:25.120
service. There you go. What does that mean? Yep. Okay. Be specific. Yeah.

132
00:07:25.121 --> 00:07:28.480
And how come? I mean, they've they took a, a bunch of people out, right?

133
00:07:28.481 --> 00:07:32.400
Like Louis farracon was one Alex Jones, Paul, Joseph Watson was like,

134
00:07:32.401 --> 00:07:36.000
what did that guy ever do? I mean, people don't agree with them. Like,

135
00:07:37.080 --> 00:07:38.280
I think what's happening.

136
00:07:38.280 --> 00:07:42.990
You is there was some serious concern that Facebook was used

137
00:07:43.570 --> 00:07:46.310
to influence the last election. Mm-hmm &lt;affirmative&gt; in a, again,

138
00:07:46.311 --> 00:07:49.430
whether against their knowledge or in a,

139
00:07:49.590 --> 00:07:53.910
a way where they were negligent about the type of filtering they use that stops

140
00:07:53.911 --> 00:07:58.910
people from posting propaganda and particularly stops these things like the,

141
00:07:59.140 --> 00:07:59.800
I R a,

142
00:07:59.800 --> 00:08:04.700
the internet research agency in Russia that literally creates thousands of

143
00:08:04.780 --> 00:08:08.700
profiles and pages. And they'll have a black lives matter page.

144
00:08:08.760 --> 00:08:10.700
That's just designed to with cops.

145
00:08:11.000 --> 00:08:14.260
And then they'll have a Procop page that's just designed to with black lives

146
00:08:14.261 --> 00:08:16.660
matter. Mm-hmm &lt;affirmative&gt; all they want to do is create anger.

147
00:08:16.661 --> 00:08:20.450
Mm-hmm &lt;affirmative&gt; and, and they're doing this engineering, these arguments,

148
00:08:20.690 --> 00:08:21.211
I mean, this is,

149
00:08:21.211 --> 00:08:25.730
this is a hundred percent proven fact Renee doreta who had been on my podcast,

150
00:08:26.080 --> 00:08:29.930
went over the details of how it's set up and how they do it and the memes and

151
00:08:29.931 --> 00:08:31.050
the memes that they create. Like,

152
00:08:31.051 --> 00:08:35.330
this is an organized effort that they channeled through Facebook in particular.

153
00:08:35.610 --> 00:08:37.890
Yeah. And then Instagram and a couple other social media sites. Yeah.

154
00:08:38.390 --> 00:08:41.480
<v 1>You know, what's interesting about Renee, she worked for new knowledge. Right?</v>

155
00:08:42.200 --> 00:08:45.680
I do not know. I think she was a director for new knowledge. Um,

156
00:08:47.830 --> 00:08:48.663
they,

157
00:08:49.750 --> 00:08:53.960
this company new knowledge that the DNC has tapped as one of their,

158
00:08:54.920 --> 00:08:56.280
um, I don't know,

159
00:08:56.281 --> 00:09:01.280
disinformation campaign experts and cyber experts was the very same

160
00:09:01.281 --> 00:09:06.240
company that created false accounts and pretended to

161
00:09:06.241 --> 00:09:10.960
be Russian bots in order to influence a us Senate election in

162
00:09:10.961 --> 00:09:12.280
Alabama. Yeah.

163
00:09:12.300 --> 00:09:15.830
<v 0>I'd heard of that. Right. That, so that's something that she was not a part of,</v>

164
00:09:15.831 --> 00:09:17.030
but it happened time. I don't Don know.

165
00:09:17.250 --> 00:09:17.790
<v 1>Her. She.</v>

166
00:09:17.790 --> 00:09:18.830
<v 0>Was a director of research.</v>

167
00:09:18.930 --> 00:09:20.590
<v 1>It says at new knowledge. Right.</v>

168
00:09:21.010 --> 00:09:24.670
<v 0>And that, but that's post this happening. Correct. I don't, no,</v>

169
00:09:24.690 --> 00:09:25.950
she sent me an email about it.

170
00:09:26.070 --> 00:09:28.350
I cuz I questioned her about it after it happened. Yeah.

171
00:09:28.610 --> 00:09:30.230
And I don't think she's full of. Yeah.

172
00:09:30.340 --> 00:09:31.910
<v 1>Well I, yeah, I I've never met her,</v>

173
00:09:31.911 --> 00:09:35.740
but I know that that company mm-hmm &lt;affirmative&gt; is one that is often as a,

174
00:09:35.760 --> 00:09:39.780
as a so-called expert and was a company that was cited, um,

175
00:09:40.780 --> 00:09:45.460
uh, to try to smear my campaign as somehow being a, a,

176
00:09:45.480 --> 00:09:50.020
an engine for the Russians or something like that. Which, which to me, again,

177
00:09:50.021 --> 00:09:52.660
just points to, well, let's look at, let's look at the expert,

178
00:09:52.661 --> 00:09:57.490
the so-called experts that you're citing and this company knowledge and the

179
00:09:57.491 --> 00:09:59.170
kinds of actions that they've been taking,

180
00:10:00.110 --> 00:10:03.050
the very same ones that they're criticizing others for doing it's a dirty.

181
00:10:03.050 --> 00:10:04.370
<v 0>World out there. &lt;laugh&gt;.</v>

182
00:10:05.080 --> 00:10:07.770
<v 1>It's a dirty wild, wild west than the internet. &lt;laugh&gt; it.</v>

183
00:10:07.970 --> 00:10:11.170
<v 0>Is what's it is a wild west in a sense that, I mean,</v>

184
00:10:12.050 --> 00:10:14.090
I think there should be regulation. Like, I mean,

185
00:10:14.091 --> 00:10:15.930
I don't think you should be able to put child porn everywhere.

186
00:10:16.040 --> 00:10:18.000
Don't think you should be able to docs people. Yeah. But it's like,

187
00:10:18.001 --> 00:10:21.880
where does that border stop? Yeah. Where does that regulation border stop? Yeah.

188
00:10:21.900 --> 00:10:23.200
And I think it's a very good question.

189
00:10:23.500 --> 00:10:27.360
Do you think that these social media platforms,

190
00:10:27.361 --> 00:10:30.480
whether it's Google or Twitter or whatever Facebook do you think that they

191
00:10:30.481 --> 00:10:34.680
should be treated as a public utility where everyone essentially has the right

192
00:10:34.681 --> 00:10:37.510
to use them, you have the right to use water.

193
00:10:37.650 --> 00:10:40.870
You don't have the right to take a hose and smash your neighbor's window and

194
00:10:40.871 --> 00:10:41.830
flood his house. That's right.

195
00:10:42.310 --> 00:10:43.143
<v 1>I do.</v>

196
00:10:43.190 --> 00:10:46.990
I do think that they should be regulated like that and they should be subject to

197
00:10:47.210 --> 00:10:52.150
the very same antitrust laws that have been used to make sure that we don't

198
00:10:52.151 --> 00:10:56.950
have other monopolies in other industries or in other areas, uh, to,

199
00:10:57.480 --> 00:11:00.380
to, to break them up. Mm-hmm &lt;affirmative&gt;. And I think that was something that,

200
00:11:00.660 --> 00:11:02.380
uh, that Chris Hughes outlined in his article,

201
00:11:03.120 --> 00:11:07.300
the very first step that could be taken is just to say, Hey, you've got a, uh,

202
00:11:07.360 --> 00:11:11.820
you've got Facebook needs to let go of Instagram and WhatsApp mm-hmm

203
00:11:11.860 --> 00:11:13.500
&lt;affirmative&gt; because that was some that,

204
00:11:13.501 --> 00:11:17.810
that acquisition created an even stronger monopoly that really shouldn't and

205
00:11:17.811 --> 00:11:21.850
allowed to take place in the beginning. And so there are, there are, uh,

206
00:11:21.851 --> 00:11:25.330
concerns about the kind of power being, um, you know,

207
00:11:25.770 --> 00:11:29.010
consolidated into the hands of a very few people, uh,

208
00:11:29.110 --> 00:11:33.090
as well as how that's impacting, uh, any kind of, uh,

209
00:11:33.200 --> 00:11:36.610
competition and squashing that competition from coming up and saying, Hey,

210
00:11:36.950 --> 00:11:37.251
you know,

211
00:11:37.251 --> 00:11:40.640
you've got Facebook book and then you've got this other new techn social media

212
00:11:40.641 --> 00:11:41.474
technology.

213
00:11:41.590 --> 00:11:46.120
They've got better privacy standards and better service for the consumer than

214
00:11:46.360 --> 00:11:50.160
Facebook. But anytime that, that tries to happen, uh, you know,

215
00:11:50.161 --> 00:11:53.720
they're quickly squashed by companies like Facebook or Google for that matter or

216
00:11:53.721 --> 00:11:54.720
bought up or bought up.

217
00:11:54.720 --> 00:11:56.440
<v 0>Exactly. Yeah. That's, that's,</v>

218
00:11:57.580 --> 00:12:01.640
that's the big concern is that there's just not enough variety.

219
00:12:02.180 --> 00:12:07.160
And there's also a big concern that I have a big concern that there's a bunch of

220
00:12:07.161 --> 00:12:10.520
people that don't seem to understand the consequences of what they're calling

221
00:12:10.750 --> 00:12:13.590
deplatforming people, which basically censoring people,

222
00:12:14.450 --> 00:12:18.270
taking people out of the public discussion. Mm. And when,

223
00:12:18.460 --> 00:12:23.030
when you do that and you create a bubble or you create

224
00:12:23.350 --> 00:12:26.910
a one party leaning institution, one party leaning,

225
00:12:27.210 --> 00:12:28.950
conglomeration of human beings,

226
00:12:29.210 --> 00:12:33.940
you're gonna develop some real anger yeah. On the other side. It, yeah. And it,

227
00:12:33.960 --> 00:12:36.900
it does the opposite of what you want it to do.

228
00:12:36.901 --> 00:12:38.660
What you wanted to do was make the world a better place.

229
00:12:38.990 --> 00:12:43.060
Let's take some of these angry voices out of the mix and let's make the world a

230
00:12:43.061 --> 00:12:44.500
better place. Yeah. It just makes them more angry.

231
00:12:44.560 --> 00:12:46.580
<v 1>And encourage, encourage discourse,</v>

232
00:12:46.730 --> 00:12:49.580
encourage these kinds of conversations where you can,

233
00:12:49.920 --> 00:12:53.810
you can engage with people who might have a different view on an issue or have a

234
00:12:53.811 --> 00:12:56.530
different experience that they bring to the conversation and to do.

235
00:12:56.531 --> 00:13:00.650
So that actually helps increase the knowledge and understanding, um,

236
00:13:00.651 --> 00:13:01.484
that we have.

237
00:13:01.840 --> 00:13:05.850
<v 0>Yeah. I think we have to reward civil discourse as well. Yeah.</v>

238
00:13:05.890 --> 00:13:07.970
I think we have to be kinder to each other and we have to,

239
00:13:08.190 --> 00:13:12.850
we have to be more upset at people that are acting like shitheads

240
00:13:12.870 --> 00:13:17.120
online for no reason. Like, you're just, you think that it's just online. Yeah.

241
00:13:17.121 --> 00:13:19.120
But it, what it is, is communication. Yep.

242
00:13:19.260 --> 00:13:23.640
And if you're interested in communication online, you're just a communicator.

243
00:13:23.840 --> 00:13:27.520
Yeah. Okay. All this calling it, being a troll and all this different things,

244
00:13:27.521 --> 00:13:31.560
all these different labels that people put on to make it cuter and whitewash it,

245
00:13:31.630 --> 00:13:34.400
it's not good. It's not good for anybody. Yeah. And,

246
00:13:34.830 --> 00:13:39.750
and if people could figure out how to be less angry in their

247
00:13:39.770 --> 00:13:44.030
online lives and, and communicate about issues,

248
00:13:44.110 --> 00:13:47.430
I think we'd find that we meet more in the middle than we think we do.

249
00:13:47.630 --> 00:13:50.110
I think there's also a problem that people have where they become married to

250
00:13:50.111 --> 00:13:52.790
their ideas and they dig their heels in and they,

251
00:13:52.791 --> 00:13:56.420
they support their ideology and they very rigid about it. Yeah.

252
00:13:56.560 --> 00:14:00.500
And that is only strengthened when you silence people. Yeah. Yeah.

253
00:14:00.530 --> 00:14:03.540
It's it does the opposite of what you're hoping it's gonna do.

254
00:14:03.600 --> 00:14:06.220
It makes the world a worse place. It makes look,

255
00:14:06.240 --> 00:14:07.980
if people are saying things that you don't like,

256
00:14:08.000 --> 00:14:09.500
you don't have to read that mm-hmm &lt;affirmative&gt;.

257
00:14:09.600 --> 00:14:12.700
But if you tell people that no one can read that they'll they're gonna go.

258
00:14:12.880 --> 00:14:16.490
Why do you get to decide exactly. And who are you? Who are you?

259
00:14:16.491 --> 00:14:19.210
You are a regular person. Mm-hmm &lt;affirmative&gt; how wise are you? Yeah.

260
00:14:19.270 --> 00:14:21.530
How many steps ahead have you played this game? That's right.

261
00:14:21.930 --> 00:14:24.330
Cause I'm looking at this game, I'm looking at civil war. Yeah.

262
00:14:24.470 --> 00:14:27.970
I'm looking at worst case scenario. This goes to that. That goes to this. Yeah.

263
00:14:27.971 --> 00:14:31.090
This guy attacks that, that guy attacks this, they can't talk anymore.

264
00:14:31.270 --> 00:14:33.810
You got people fighting in the streets. Exactly. That happens. Exactly.

265
00:14:33.811 --> 00:14:35.130
That's a human characteristic.

266
00:14:35.130 --> 00:14:39.400
<v 1>That's right. That's right. And that's, that is where this path ends up.</v>

267
00:14:39.470 --> 00:14:42.800
This path that we're on of this, this hyperpartisanship yes.

268
00:14:42.870 --> 00:14:47.600
This extreme divisiveness where it's either you're in my tribe or you're in the

269
00:14:47.601 --> 00:14:52.200
other tribe and the arrows are pointed at each other without any willingness to

270
00:14:52.201 --> 00:14:54.040
once again, just let's just have a conversation.

271
00:14:54.060 --> 00:14:56.200
Let me hear where you're coming from here, where I'm coming from.

272
00:14:56.700 --> 00:14:58.760
We can disagree without being disagreeable.

273
00:14:58.780 --> 00:15:01.000
We can even have a heated conversation and a debate.

274
00:15:01.300 --> 00:15:04.200
<v 0>And I would, the end of it, say what you're saying is patriotic. Yeah.</v>

275
00:15:04.350 --> 00:15:08.120
This is patriotic. And I think it's unpatriic to be partisan. Yeah.

276
00:15:08.360 --> 00:15:10.200
Because I think we're supposed to be on a team together. Yeah.

277
00:15:10.510 --> 00:15:13.310
This is supposed to be team America. Right? Exactly. Exactly. What are we doing?

278
00:15:13.311 --> 00:15:13.840
Exactly.

279
00:15:13.840 --> 00:15:18.230
We're we're fighting over nonsense and MIS mischaracterizing people's positions

280
00:15:18.231 --> 00:15:21.710
just to suit our own ideas. That's right. It's, it's foolish.

281
00:15:22.140 --> 00:15:24.830
<v 1>It's it's foolish. And as you said, it's extremely dangerous.</v>

282
00:15:25.010 --> 00:15:29.350
And the American people are the ones who, who ultimately lose in all of this.

283
00:15:39.180 --> 00:15:39.300
<v 2>I.</v>

