WEBVTT

1
00:00:01.020 --> 00:00:02.760
<v 0>The Joe Rogan experience.</v>

2
00:00:03.390 --> 00:00:06.780
<v 1>This is, uh, an odd idea,</v>

3
00:00:06.810 --> 00:00:11.550
but I think what it's doing is it's forcing us to

4
00:00:11.551 --> 00:00:15.330
adapt and evolve our ability to detect, to detect.

5
00:00:16.650 --> 00:00:21.390
And it's doing it almost like an immune system response,

6
00:00:21.990 --> 00:00:22.823
like what,

7
00:00:22.870 --> 00:00:27.180
what we're reacting to and what we're recognizing from all this stuff is like,

8
00:00:27.210 --> 00:00:28.980
oh, we didn't know what this was.

9
00:00:29.580 --> 00:00:33.990
And this has resulted in this riot,

10
00:00:34.680 --> 00:00:36.660
whatever you want to call the Capitol hill attack.

11
00:00:37.560 --> 00:00:42.090
And now we're looking at more censorship on social

12
00:00:42.091 --> 00:00:45.240
media. We're looking at, uh, you know,

13
00:00:45.241 --> 00:00:49.140
like them trying to Batten down the hatches and figure out how to handle

14
00:00:49.380 --> 00:00:53.610
something like Q Anon or the people that were

15
00:00:53.611 --> 00:00:56.970
allegedly promoting these ideas.

16
00:00:57.300 --> 00:00:58.890
A lot of them that are banned from social media,

17
00:00:58.930 --> 00:01:00.600
the stuff that you highlighted in your show,

18
00:01:01.050 --> 00:01:03.630
we have to figure out what's true and what's not true.

19
00:01:03.900 --> 00:01:08.850
And so there's been some sort of draconian measures that have been, um,

20
00:01:09.060 --> 00:01:11.310
suggested, you know, like, uh,

21
00:01:11.340 --> 00:01:16.080
hiring some sort of a team that goes over social media

22
00:01:16.320 --> 00:01:20.700
and make sure that everything is according to what they deemed to be correct or

23
00:01:20.701 --> 00:01:25.140
incorrect, which obviously is subject to biases.

24
00:01:25.170 --> 00:01:28.530
And we're, we're very aware that that's going on today.

25
00:01:28.531 --> 00:01:31.710
That there's a lot of that going on today where the necessarily a truth doesn't

26
00:01:31.711 --> 00:01:35.580
like the hunter Biden laptop story is a great example of that, right? Like, uh,

27
00:01:35.820 --> 00:01:37.800
the social media platforms,

28
00:01:38.490 --> 00:01:42.150
they censored news from the New York post.

29
00:01:42.151 --> 00:01:46.770
One of the oldest newspapers in America on the hunter Biden laptop story,

30
00:01:46.920 --> 00:01:49.080
because they decided that somehow or another,

31
00:01:49.081 --> 00:01:51.000
it was propaganda or somehow or another,

32
00:01:51.001 --> 00:01:55.170
it was not good to get that information, but it was news. It was real news.

33
00:01:55.980 --> 00:01:59.820
It was a real story. And they decided it was too close to the election.

34
00:02:00.150 --> 00:02:02.460
This could hurt Biden. We don't want Trump to win.

35
00:02:02.461 --> 00:02:03.930
So you're dealing with biases.

36
00:02:03.931 --> 00:02:08.340
This is not just like simply here's information that we know to be true or

37
00:02:08.550 --> 00:02:10.230
here's information that we know to be alive.

38
00:02:10.231 --> 00:02:12.720
We're going to stop that from getting through. No, they knew it to be true,

39
00:02:13.170 --> 00:02:18.000
but they decided to stop it because it wasn't convenient or it didn't fit the

40
00:02:18.001 --> 00:02:18.870
narrative they were trying to.

41
00:02:19.680 --> 00:02:21.990
<v 0>Right. It's very it's. I mean,</v>

42
00:02:21.991 --> 00:02:25.890
how do you find a neutral arbiter of the truth? Um, if,

43
00:02:25.891 --> 00:02:29.640
if you are going to entrust someone with that responsibility,

44
00:02:29.880 --> 00:02:33.450
and I think that that's, it's just an incredibly slippery slope,

45
00:02:33.480 --> 00:02:37.860
incredibly slippery. And, and what these big tech companies have,

46
00:02:37.890 --> 00:02:41.940
have, uh, suggested is that, well, maybe we don't use humans,

47
00:02:41.970 --> 00:02:46.050
maybe we use algorithms, right. You know, to, to, to moderate everything. And,

48
00:02:46.320 --> 00:02:48.510
you know, the algorithms had had that,

49
00:02:48.780 --> 00:02:52.710
that in many ways had bolstered something like cube because they're,

50
00:02:52.770 --> 00:02:57.210
they're basically a sociopathic when it comes to just trying to

51
00:02:57.570 --> 00:02:59.860
drive attention as much possible.

52
00:03:00.070 --> 00:03:03.800
So now they can kind of invert those algorithms and,

53
00:03:03.801 --> 00:03:07.360
and punish those who, um, talk about, uh, that kind of content.

54
00:03:07.361 --> 00:03:11.410
And oftentimes even if, even if their goal was just to predict, prevent,

55
00:03:11.440 --> 00:03:13.930
I dunno, a conversation around, um, Q and on,

56
00:03:13.931 --> 00:03:17.260
because they consider it to be problematic. What else gets swept up?

57
00:03:17.530 --> 00:03:22.000
What else gets swept up with that? Um, I mean, I saw,

58
00:03:22.090 --> 00:03:27.010
I saw a lot of people who were reporting on Q1 on maybe coming from the, uh,

59
00:03:27.040 --> 00:03:31.450
the side of critiquing it, their videos were being wiped out. Um, you know,

60
00:03:31.480 --> 00:03:33.880
people who were documenting January the sixth,

61
00:03:33.910 --> 00:03:35.440
their content was being wiped out.

62
00:03:35.500 --> 00:03:38.440
People who are critical of Q1 on they had websites that were, um,

63
00:03:38.470 --> 00:03:41.260
sort of on the other side that was being wiped out as well.

64
00:03:41.261 --> 00:03:43.840
And that's because of course, it's, uh,

65
00:03:43.990 --> 00:03:48.610
it's sort of this blunt force that, that an algorithm wield.

66
00:03:48.640 --> 00:03:49.331
<v 1>So people,</v>

67
00:03:49.331 --> 00:03:53.380
even people that were analyzing the move and from a critical standpoint,

68
00:03:53.410 --> 00:03:56.260
people who are looking at like how ridiculous this is, look at this,

69
00:03:56.470 --> 00:03:59.800
they had their channels wiped out as well. Yes.

70
00:03:59.950 --> 00:04:03.580
So any content on Q and not, they just want to erase it from the internet.

71
00:04:04.810 --> 00:04:07.720
<v 0>That seemed to be the, the initial response. Yeah.</v>

72
00:04:08.140 --> 00:04:10.840
<v 1>It's move together in sync.</v>

73
00:04:10.930 --> 00:04:14.110
<v 0>I mean, I, I think that if I did not have, you know,</v>

74
00:04:14.200 --> 00:04:18.010
HBO in my sales with this project, it wouldn't have seen the light of day.

75
00:04:18.190 --> 00:04:20.290
<v 1>Really. Or did you try to put it on YouTube? You think?</v>

76
00:04:20.920 --> 00:04:23.590
<v 0>Oh yeah. Yeah. I mean, when we, even, when we,</v>

77
00:04:23.830 --> 00:04:28.060
so when we first released the series, um, you know, there was,

78
00:04:28.180 --> 00:04:31.120
there was, there were some articles floating around like, oh,

79
00:04:31.121 --> 00:04:34.240
maybe this is going to make it things worse. Um,

80
00:04:34.300 --> 00:04:38.770
if I typed in Q into the storm into YouTube, it wouldn't,

81
00:04:39.580 --> 00:04:40.211
auto-populate at a certain point,

82
00:04:40.211 --> 00:04:44.890
it started out auto-populating and then that went away. So, um,

83
00:04:45.460 --> 00:04:49.690
yeah, I, I wouldn't feel confident at all that, um, you know,

84
00:04:49.691 --> 00:04:53.860
if we didn't have a gorilla in our corner, uh, that, um,

85
00:04:54.250 --> 00:04:58.990
that this story that revealed ultimately who was behind Q Anon, uh,

86
00:04:59.230 --> 00:05:03.460
would have been seen, would have been able to find an audience. Um.

87
00:05:03.520 --> 00:05:05.380
<v 1>And that HBO shout.</v>

88
00:05:05.380 --> 00:05:07.060
<v 0>Out to HBO. I mean, they really had my back, so.</v>

89
00:05:07.510 --> 00:05:10.780
<v 1>And amazing for decades, you know, you really think about it. I mean,</v>

90
00:05:10.781 --> 00:05:14.170
they're the people that when bill Maher's show politically incorrect got pulled

91
00:05:14.171 --> 00:05:18.910
off of what was it on ABC? I forget network television. Um,

92
00:05:18.940 --> 00:05:21.310
they immediately took it, brought it over,

93
00:05:21.370 --> 00:05:24.880
turn it into real time and made it even better. You know, it was uncensored now.

94
00:05:24.881 --> 00:05:28.120
And, and it's, in my opinion, real time with bill,

95
00:05:28.410 --> 00:05:32.290
Maher's probably one of the very best social commentary

96
00:05:32.560 --> 00:05:33.970
shows and,

97
00:05:33.971 --> 00:05:37.990
and comedy shows that like really doesn't pull any punches on any network.

98
00:05:38.110 --> 00:05:42.010
<v 0>However you want to hear something crazy. So, yes,</v>

99
00:05:42.040 --> 00:05:44.410
just yesterday I was talking with, uh,

100
00:05:44.440 --> 00:05:48.760
someone who's helping distribute the film and I said, well,

101
00:05:48.761 --> 00:05:49.960
what about Amazon? You know,

102
00:05:49.961 --> 00:05:52.780
are we gonna be able to put it out on Amazon internationally? And they said,

103
00:05:52.781 --> 00:05:56.320
well, as of the last year,

104
00:05:56.380 --> 00:06:00.410
they've stopped taking documentaries, all documentaries,

105
00:06:00.830 --> 00:06:04.250
what you cannot publish a documentary on their platform.

106
00:06:04.251 --> 00:06:09.080
And the reason was because that I was told is because, you know, there was,

107
00:06:09.110 --> 00:06:11.630
there was all this conspiracy flat earth stuff, and they were,

108
00:06:11.631 --> 00:06:13.460
they were getting blow back that eventually they said,

109
00:06:13.461 --> 00:06:17.180
we don't want to have to decide what we publish and what we don't, what's real.

110
00:06:17.181 --> 00:06:19.610
And what's not, we're just not going to publish anything.

111
00:06:20.330 --> 00:06:25.160
And the example they gave me that they couldn't get published was

112
00:06:25.190 --> 00:06:27.580
the Cove. I don't know if you saw that documentary,

113
00:06:27.581 --> 00:06:30.800
the dolphin documentary documentary, it won an Oscar.

114
00:06:31.400 --> 00:06:34.910
And I just to just to check it, you know, I looked it up and sure enough,

115
00:06:34.940 --> 00:06:38.120
the code wasn't available on, on Amazon. So, you know,

116
00:06:38.130 --> 00:06:41.330
those who said that there wouldn't be a slow creep of censorship, you know,

117
00:06:41.331 --> 00:06:45.050
kind of starting with things that I think everybody agrees should be, you know,

118
00:06:45.051 --> 00:06:49.400
they wouldn't like to be in society. You know, things like the daily Stormer,

119
00:06:49.430 --> 00:06:51.770
maybe a lot of people don't want eight Chan, you know, it's a,

120
00:06:52.130 --> 00:06:56.450
there's a progression, you know, until you end up, it seems like something like,

121
00:06:56.990 --> 00:07:01.310
you know, the, the Cove can't find an audience on a, on a major platform. Um,

122
00:07:01.311 --> 00:07:05.840
and I don't want to conflate government censorship of the corporate

123
00:07:05.841 --> 00:07:08.270
censorship too much. However,

124
00:07:08.271 --> 00:07:12.110
in a lot of ways it does feel like the government has passed the buck to these

125
00:07:12.111 --> 00:07:14.990
corporations to do what they legally can't,

126
00:07:15.500 --> 00:07:19.640
which I think is the same thing we saw the government do with privacy, right?

127
00:07:19.640 --> 00:07:23.030
Like they wouldn't have been able to get all of this data from us directly,

128
00:07:23.180 --> 00:07:26.330
but if you give it to a Facebook or a Twitter,

129
00:07:27.080 --> 00:07:30.140
it's very easy for the government to then go and get access to that information.

130
00:07:30.530 --> 00:07:33.950
So I think what we saw happen with the fourth amendment,

131
00:07:33.951 --> 00:07:37.670
we're now seeing happen with the first amendment where they can say, well, look,

132
00:07:37.840 --> 00:07:40.250
we, we couldn't restrict conversation around certain topics.

133
00:07:40.251 --> 00:07:44.210
We couldn't directly, um, decide what's true, or what's not,

134
00:07:44.211 --> 00:07:46.760
we're going to put that in the hands of these companies.

135
00:07:47.030 --> 00:07:51.590
And of course these companies have intimate relationships with, um,

136
00:07:51.680 --> 00:07:54.260
with many members of, of the government, you know,

137
00:07:54.261 --> 00:07:58.880
there's a revolving door there. So I, I,

138
00:07:58.940 --> 00:08:03.920
when people want to talk about what limiting what we can say online or

139
00:08:03.921 --> 00:08:05.960
limiting, um, disinformation.

140
00:08:06.100 --> 00:08:09.230
So the things I think that it's almost the wrong place to start.

141
00:08:09.680 --> 00:08:13.190
I think we have to go back to the privacy issues.

142
00:08:13.790 --> 00:08:17.210
And I actually think if we had not let privacy be eroded online,

143
00:08:17.211 --> 00:08:20.060
we wouldn't be having this debate. You know, cause if,

144
00:08:21.020 --> 00:08:25.730
if these gigantic companies hadn't collected thousands of data points on us,

145
00:08:26.120 --> 00:08:30.710
you know, didn't know our fears, our desires, um,

146
00:08:31.460 --> 00:08:33.950
if they hadn't built these psychometric profiles,

147
00:08:34.220 --> 00:08:37.250
they wouldn't have been able to manipulate us,

148
00:08:37.280 --> 00:08:39.950
use these algorithms to drive us into echo chambers,

149
00:08:40.310 --> 00:08:43.430
which have really created these disparate realities.

150
00:08:43.970 --> 00:08:48.290
And now these disparate realities can't agree necessarily on a set of facts.

151
00:08:49.220 --> 00:08:53.210
Um, sometimes you're, you're considered, um, you know,

152
00:08:53.211 --> 00:08:57.600
sometimes people will be ostracized or even talking to somebody from the quote

153
00:08:57.601 --> 00:09:00.460
unquote other side. Right. Um, and,

154
00:09:00.630 --> 00:09:03.570
and so now there's this conversation about what should be allowed to be set

155
00:09:03.571 --> 00:09:08.250
online. Uh, and I think that that's simply a by-product of, of,

156
00:09:08.850 --> 00:09:12.360
um, you know, a privacy having been eroded. So, you know,

157
00:09:12.361 --> 00:09:16.140
if I was to do anything about these issues, I would start by restoring rights.

158
00:09:16.380 --> 00:09:18.450
I would go back and say, all right, well, how do we get, you know,

159
00:09:18.451 --> 00:09:22.470
how do we get, um, ownership and privacy rights, um,

160
00:09:22.560 --> 00:09:24.990
online when it comes to our, our personal data,

161
00:09:24.991 --> 00:09:28.890
let's start there before we start, you know, going after the speech itself.

162
00:09:29.370 --> 00:09:32.910
<v 1>Watch the entire episode for free only on Spotify.</v>

