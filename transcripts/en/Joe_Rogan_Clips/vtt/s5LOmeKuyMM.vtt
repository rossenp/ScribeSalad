WEBVTT

1
00:00:01.020 --> 00:00:02.760
<v 0>The Joe Rogan experience.</v>

2
00:00:03.090 --> 00:00:07.110
The thing we have to hone in on is the asymmetry of power. Um, you know,

3
00:00:07.111 --> 00:00:10.830
as I say, in the film, it's like, we're bringing this ancient brain hardware,

4
00:00:10.860 --> 00:00:14.310
the prefrontal cortex, which is like what you use to do, um,

5
00:00:14.790 --> 00:00:18.870
goal directed action. Self-control willpower holding back, you know,

6
00:00:19.050 --> 00:00:21.660
marshmallow tests. Don't do the get, don't get the marshmallow.

7
00:00:21.670 --> 00:00:23.580
Now wait later for the two marshmallows later,

8
00:00:23.850 --> 00:00:25.800
all of that is through our prefrontal cortex.

9
00:00:26.190 --> 00:00:29.850
And when you're sitting there and you think, okay, I'm gonna go watch,

10
00:00:29.880 --> 00:00:33.030
I'm gonna look at this one thing on Facebook because my friend invited me to

11
00:00:33.031 --> 00:00:35.430
this event, or it's this one post I have to look at.

12
00:00:35.760 --> 00:00:36.593
And the next thing you know,

13
00:00:36.630 --> 00:00:39.510
you're find yourself scrolling through the thing for like an hour. And you say,

14
00:00:39.511 --> 00:00:43.500
man, that was on me. I should have had more self-control but there,

15
00:00:43.650 --> 00:00:47.610
behind the screen behind that glass lab is like a supercomputer pointed at your

16
00:00:47.611 --> 00:00:52.080
brain that is predicting the perfect thing to show you

17
00:00:52.350 --> 00:00:55.500
next. And you can feel it like it's, this is really important.

18
00:00:55.501 --> 00:00:58.200
So like if I'm Facebook and when you flick your finger,

19
00:00:58.230 --> 00:01:00.660
you think when you're using Facebook,

20
00:01:00.840 --> 00:01:02.910
it's just going to show me the next thing that my friend said,

21
00:01:03.330 --> 00:01:05.400
but it's not doing that when you flick your finger,

22
00:01:05.401 --> 00:01:09.120
it actually literally wakes up this sort of super computer avatar blue doll live

23
00:01:09.121 --> 00:01:13.830
version of Joe and the voodoo doll of Joe is, um, you know, the,

24
00:01:14.180 --> 00:01:17.460
the more clicks you ever made on Facebook is like adding a little hair to the

25
00:01:17.461 --> 00:01:20.220
voodoo doll. And the more likes you've ever made,

26
00:01:20.221 --> 00:01:24.180
adds little clothing to the voodoo doll. And the more, um, you know,

27
00:01:24.181 --> 00:01:26.940
watch time on videos you've ever had adds little, um, you know,

28
00:01:26.941 --> 00:01:29.580
shoes to the food. And also the Buddha is getting more and more accurate.

29
00:01:29.820 --> 00:01:32.070
The more things you click on this is in the film, the social dilemma,

30
00:01:32.071 --> 00:01:36.750
like if you notice like the character, you know, as he's using this thing, uh,

31
00:01:37.020 --> 00:01:40.680
it builds a more and more accurate model that the is the three AI's behind the

32
00:01:40.681 --> 00:01:41.940
screen are kind of manipulating.

33
00:01:42.540 --> 00:01:46.050
And the idea is it can actually predict and prick the voodoo doll with this

34
00:01:46.051 --> 00:01:49.050
video or that post from your friends or this other thing.

35
00:01:49.051 --> 00:01:52.530
And it'll figure out the right thing to show you that it knows will keep you

36
00:01:52.531 --> 00:01:57.090
there because it's already seen how that same video or that same post has kept

37
00:01:57.091 --> 00:01:58.710
200 million other verticals there.

38
00:01:59.280 --> 00:02:01.500
Cause you just looked like another voodoo doll. So here's an example.

39
00:02:02.450 --> 00:02:03.960
This works the same on all the platforms.

40
00:02:03.961 --> 00:02:08.190
If you are a teen girl and you opened a dieting video on YouTube,

41
00:02:09.480 --> 00:02:09.900
um,

42
00:02:09.900 --> 00:02:13.590
70% of YouTube watch time comes from the recommendations on the right hand side,

43
00:02:14.070 --> 00:02:16.590
right? So the things that are showing recommended videos next,

44
00:02:17.340 --> 00:02:20.610
and it will show you it'll show,

45
00:02:20.640 --> 00:02:23.880
what did it show that the girls who watched the teen dieting video,

46
00:02:24.420 --> 00:02:29.070
it showed anorexia videos because those were better at keeping the teen girls

47
00:02:29.071 --> 00:02:31.650
attention. Not because it said these are good for them.

48
00:02:31.680 --> 00:02:33.390
These are helpful for them.

49
00:02:33.391 --> 00:02:36.300
It just says these tend to work at keeping their attention.

50
00:02:36.690 --> 00:02:39.690
<v 1>So again, these tend to work if you are already watching diet.</v>

51
00:02:39.690 --> 00:02:42.660
<v 0>Videos. So if you're a 13 year old girl and you watch the diet video,</v>

52
00:02:42.740 --> 00:02:46.080
YouTube wakes up it's voodoo doll version of that girl and says, Hey,

53
00:02:46.081 --> 00:02:49.560
I've got like a hundred million other duty dolls of 13 year old girls. Right.

54
00:02:49.920 --> 00:02:52.740
And they all tend to watch these, these other videos. I don't know.

55
00:02:52.860 --> 00:02:56.130
I just know that they have this word thin SPO inspiration is the name for it.

56
00:02:56.460 --> 00:02:59.260
I'm ready to be inspired for anorexia. Yeah. It's a real thing. Um,

57
00:02:59.290 --> 00:03:01.180
YouTube address this problem a couple of years ago,

58
00:03:01.181 --> 00:03:05.350
but when you let the machine run blind, all it's doing is picking stuff.

59
00:03:05.351 --> 00:03:06.690
That's engaging. Why did it, I.

60
00:03:06.720 --> 00:03:11.400
<v 1>Choose to not let the machine run blind with one thing like anorexia?</v>

61
00:03:11.880 --> 00:03:12.000
Well.

62
00:03:12.000 --> 00:03:15.480
<v 0>So now we're getting into the Twitter censorship conversation in the moderation</v>

63
00:03:16.050 --> 00:03:17.340
conversation. So the real,

64
00:03:17.610 --> 00:03:21.210
this is why I don't focus on censorship and moderation because the real issue is

65
00:03:21.211 --> 00:03:23.430
if you blur your eyes and zoom way out and say,

66
00:03:23.431 --> 00:03:27.660
how does the whole machine tend to operate? Like no matter what I start with,

67
00:03:27.690 --> 00:03:32.640
what is it going to recommend next? So, um, you know, if you started with,

68
00:03:33.210 --> 00:03:35.250
um, you know, uh, a world war II video,

69
00:03:35.430 --> 00:03:38.910
YouTube would recommend a bunch of Holocaust denial videos, right?

70
00:03:39.270 --> 00:03:41.610
If you started teen girls with a dieting video,

71
00:03:41.611 --> 00:03:45.540
it would recommend these anorexia videos. Uh, in Facebook's case,

72
00:03:45.570 --> 00:03:46.620
if you joined,

73
00:03:47.460 --> 00:03:50.310
there's so many different examples here because Facebook recommends groups to

74
00:03:50.311 --> 00:03:53.190
people based on what it thinks is most engaging for you.

75
00:03:53.250 --> 00:03:56.910
So if you were a new mom, you had grenade. I rest on my friends on this podcast,

76
00:03:57.480 --> 00:04:00.120
we've done a bunch of work together and she has this great example of as a new

77
00:04:00.121 --> 00:04:05.040
mom. She joined one Facebook group for mothers who do do it yourself,

78
00:04:05.070 --> 00:04:09.090
baby food, like organic baby food. And then Facebook has this sidebar. It says,

79
00:04:09.120 --> 00:04:12.180
here's some other groups you might recommend you might want to join.

80
00:04:12.630 --> 00:04:14.430
And what do you think was the most engaging of those?

81
00:04:14.550 --> 00:04:17.850
Cause Facebook again is picking on which group, if I got you to join,

82
00:04:17.851 --> 00:04:21.570
it would cause you to spend the most time here. Right?

83
00:04:22.050 --> 00:04:24.360
So force some do it yourself, baby food groups.

84
00:04:24.390 --> 00:04:27.720
Which group do you think it's selected? Probably something about vaccines.

85
00:04:28.140 --> 00:04:32.850
Exactly. Vaccines for moms. Yeah. Uh, okay. So then if you join that group,

86
00:04:32.910 --> 00:04:36.930
now it does the same run the process again. So then, so now look at Facebook.

87
00:04:37.020 --> 00:04:38.130
So I says, Hey, I've got these voodoo dolls.

88
00:04:38.220 --> 00:04:40.620
I've got like a hundred million voodoo dolls and they're all they just joined

89
00:04:40.621 --> 00:04:42.270
this anti-vaccine moms group.

90
00:04:42.930 --> 00:04:46.380
And then what do they tend to engage with for very long time,

91
00:04:46.410 --> 00:04:47.670
if I get them to join these other groups,

92
00:04:47.671 --> 00:04:48.780
which of those other groups would show up.

93
00:04:51.300 --> 00:04:54.090
<v 2>Chem trails? Oh, Pizzagate, flat.</v>

94
00:04:54.090 --> 00:04:58.290
<v 0>Earth, flat earth. Absolutely. Yup. And YouTube recommended.</v>

95
00:04:58.291 --> 00:05:01.560
So I'm interchangeably going from YouTube to Facebook because it's the same

96
00:05:01.561 --> 00:05:04.950
dynamic they're competing for attention and YouTube recommended flat earth,

97
00:05:04.951 --> 00:05:09.360
conspiracy theories, hundreds of millions of times. And so when you,

98
00:05:09.420 --> 00:05:13.860
when you're a parent during COVID and you sit your kids in front of YouTube,

99
00:05:13.950 --> 00:05:16.110
because you're like, I'm, I've got, this is the digital pacifier.

100
00:05:16.170 --> 00:05:17.940
It got to let them do their thing. I got to do work.

101
00:05:18.360 --> 00:05:20.910
And then you come back to the dinner table and your kid says, you know,

102
00:05:20.911 --> 00:05:22.980
the Holocaust didn't happen in the earth is flat.

103
00:05:23.520 --> 00:05:25.920
And people are wondering why it's because of this.

104
00:05:26.490 --> 00:05:29.250
And now to your point about this sort of moderation thing,

105
00:05:29.580 --> 00:05:33.150
we can take the whack-a-mole stick after the public yells and Rene and I,

106
00:05:33.210 --> 00:05:33.361
you know,

107
00:05:33.361 --> 00:05:36.240
make a bunch of noise or something and large community by the way of people

108
00:05:36.241 --> 00:05:40.440
making noise about this. And they'll say, okay, shoot, you're right flat earth.

109
00:05:40.500 --> 00:05:41.640
We got to deal with that. And so they'll,

110
00:05:41.700 --> 00:05:44.910
they'll tweak the algorithm and then people make a much a noise about the

111
00:05:44.970 --> 00:05:49.260
inspiration videos for anorexia, for kids, and they'll deal with that problem.

112
00:05:49.710 --> 00:05:53.790
But then they start doing it based reactively. But again, if you zoom out,

113
00:05:54.000 --> 00:05:57.710
it's just still recommending stuff. That's kind of from the crazy town section.

114
00:05:57.770 --> 00:06:01.000
<v 1>Of this problem, the recommendation that's, I,</v>

115
00:06:01.010 --> 00:06:05.990
I don't mind that people have ridiculous ideas about hollow

116
00:06:05.991 --> 00:06:10.040
earth. Cause I think it's humorous, but I'm also a 53 year old man. Right?

117
00:06:10.220 --> 00:06:15.200
Right. I'm not, I'm not a 12 year old boy with limited education.

118
00:06:15.380 --> 00:06:17.720
That is like, oh my God, the government's lying to us.

119
00:06:17.721 --> 00:06:21.200
There's lizard people that live under the earth. Right. But if that's the,

120
00:06:21.201 --> 00:06:24.350
the real argument about these conspiracy theories is that they can influence

121
00:06:24.351 --> 00:06:26.750
young people or the easily impressionable or,

122
00:06:27.050 --> 00:06:31.160
or people that maybe don't have a sophisticated sense of vetting out.

123
00:06:31.900 --> 00:06:32.320
Right.

124
00:06:32.320 --> 00:06:32.531
<v 0>Well,</v>

125
00:06:32.531 --> 00:06:37.300
and the algorithms aren't making a distinction between who is just laughing at

126
00:06:37.301 --> 00:06:40.360
it and who's deeply vulnerable to it. And generally it's just,

127
00:06:40.600 --> 00:06:42.850
it's it just says who's vulnerable to it. Because another example, I,

128
00:06:42.860 --> 00:06:46.270
the way I think about this as if you're driving down the highway and,

129
00:06:46.600 --> 00:06:48.850
and you know, there's Facebook and Google trying to figure out, like,

130
00:06:48.851 --> 00:06:51.010
what should I give you based on what tends to keep your attention.

131
00:06:51.340 --> 00:06:53.860
If you look at a car crash and everybody drive in the highway,

132
00:06:53.861 --> 00:06:55.930
they look at the car crash, according to Facebook. And Google is like,

133
00:06:55.960 --> 00:06:57.980
the whole world wants car crashes. So we just,

134
00:06:57.990 --> 00:07:01.660
just feed them car crashes after car crash is under car crashes and what the

135
00:07:01.661 --> 00:07:05.950
algorithms do as [inaudible] in the film says, who's the YouTube, uh,

136
00:07:06.700 --> 00:07:10.240
whistleblower from the YouTube recommendation system is they find the perfect

137
00:07:10.241 --> 00:07:13.330
little rabbit hole for you that it knows will keep you there for five hours.

138
00:07:13.390 --> 00:07:14.680
And the conspiracy theory,

139
00:07:14.800 --> 00:07:19.060
like dark corners of YouTube were the dark corners that tends to keep people

140
00:07:19.061 --> 00:07:19.960
there for five hours.

141
00:07:20.590 --> 00:07:23.980
And so you have to realize that we're now something like 10 years in to this

142
00:07:23.981 --> 00:07:27.060
vast psychology experiment where it's been, you know,

143
00:07:27.061 --> 00:07:29.490
in every language and hundreds of countries, right? And every,

144
00:07:29.500 --> 00:07:33.330
in hundreds of languages, it's been steering people towards the crazy town.

145
00:07:33.610 --> 00:07:35.440
When I say crazy town, I think of, you know,

146
00:07:35.441 --> 00:07:39.970
imagine there's a spectrum on YouTube and there's on one side you have like the

147
00:07:39.971 --> 00:07:44.500
calm Walter Cronkite, Carl Sagan, you know, slow, you know,

148
00:07:44.650 --> 00:07:47.470
kind of boring, but like educational material or something.

149
00:07:48.040 --> 00:07:50.740
And the other side of the spectrum, you have, you know,

150
00:07:50.741 --> 00:07:55.630
the craziest stuff you can find, um, crazy town, no matter where you start,

151
00:07:55.631 --> 00:07:58.750
you could start in Walter Cronkite or you could start in crazy town.

152
00:07:59.020 --> 00:08:01.630
But if I'm YouTube and I want you to watch more,

153
00:08:01.900 --> 00:08:04.270
am I going to steer you towards the calm stuff?

154
00:08:04.271 --> 00:08:07.390
Or am I going to steer you more towards crazy town, crazy dumb,

155
00:08:07.450 --> 00:08:08.380
always more towards crazy town.

156
00:08:08.381 --> 00:08:12.160
So then you imagine just tilting the floor of humanity just by like three

157
00:08:12.161 --> 00:08:16.840
degrees, right? And then you just step back and you let society run its course,

158
00:08:16.870 --> 00:08:19.660
as Jaron linear says in the film, if you just tilt society by one degree,

159
00:08:19.661 --> 00:08:20.494
two degrees.

160
00:08:20.770 --> 00:08:22.450
<v 3>That's the whole world, that's the, that's.</v>

161
00:08:22.480 --> 00:08:26.260
<v 0>What everyone is thinking and believing. And so if you look at the,</v>

162
00:08:26.320 --> 00:08:29.920
at the degree to which people are deep into rabbit hole,

163
00:08:29.921 --> 00:08:31.690
conspiracy thinking right now, and again,

164
00:08:31.691 --> 00:08:34.570
I want to acknowledge COINTELPRO operation Mockingbird.

165
00:08:34.750 --> 00:08:36.590
Like there's a lot of real stuff, right? So, I mean,

166
00:08:36.591 --> 00:08:38.080
I'm not categorically dismissing it,

167
00:08:38.081 --> 00:08:43.030
but we're asking what is the basis upon which we're believing the things we are

168
00:08:43.031 --> 00:08:44.850
about the world. And increasingly that's,

169
00:08:44.851 --> 00:08:47.830
that's based on technology and we can get into, you know,

170
00:08:47.980 --> 00:08:49.330
what's going on in Portland. Well,

171
00:08:49.390 --> 00:08:51.940
the only way I know that as I'm looking at my social media feed,

172
00:08:51.970 --> 00:08:54.940
and according to that, it looks like the entire city's on fire and it's a war.

173
00:08:55.770 --> 00:08:58.350
But if you, I called a friend there the other day and he said,

174
00:08:58.380 --> 00:09:00.120
it's a beautiful day. There's,

175
00:09:00.150 --> 00:09:01.740
there's actually no violence anywhere near where I am.

176
00:09:01.741 --> 00:09:04.020
It's just like these two blocks or something like that. And,

177
00:09:04.021 --> 00:09:06.510
and this is the thing is warping our view of reality.

178
00:09:07.260 --> 00:09:09.390
And I think that's what really, for me,

179
00:09:09.391 --> 00:09:13.080
the social dilemmas was really trying to accomplish as a film. And I, you know,

180
00:09:13.081 --> 00:09:15.690
the director Jeff Orlowski was trying to accomplish is,

181
00:09:15.750 --> 00:09:19.920
is how did this society get go crazy everywhere all at once,

182
00:09:20.460 --> 00:09:21.900
you know, seemingly you know, they, this,

183
00:09:21.910 --> 00:09:25.080
this didn't happen by accident happened by design of this business model.

184
00:09:25.640 --> 00:09:27.620
<v 1>When did the business model get implemented?</v>

185
00:09:27.621 --> 00:09:30.500
Like when did they start using these algorithms to recommend things?

186
00:09:30.680 --> 00:09:34.340
Cause initially YouTube was just a series of videos and it didn't have that

187
00:09:34.400 --> 00:09:36.800
recommended correct section. When was that?

188
00:09:37.700 --> 00:09:41.000
<v 0>Yeah, that's a good question. I mean, I, um, you know,</v>

189
00:09:41.360 --> 00:09:44.450
they originally YouTube was just post a video and you can get people to,

190
00:09:44.600 --> 00:09:48.050
you know, go to that URL and send it around. Uh,

191
00:09:48.380 --> 00:09:52.970
they needed to figure out once the competition for attention got more intense,

192
00:09:53.030 --> 00:09:55.580
they needed to figure out how am I going to keep you there?

193
00:09:55.700 --> 00:09:58.040
And so recommending those videos on the right hand side,

194
00:09:58.041 --> 00:10:01.370
I think that was there pretty early. I remember actually, uh, because that's,

195
00:10:01.400 --> 00:10:03.860
that was some of the innovation is like keeping people within this YouTube

196
00:10:03.890 --> 00:10:06.860
wormhole. And once people were in the YouTube warm hole,

197
00:10:06.880 --> 00:10:10.150
constantly seeing videos, that was what the, they could,

198
00:10:10.250 --> 00:10:14.900
they could offer the promise to a new video uploader. Hey, if you post it here,

199
00:10:14.901 --> 00:10:18.080
you're going to get way more views than if you post it on Vimeo. Right.

200
00:10:18.081 --> 00:10:20.930
And that's, that's the thing. I open up tech talk right now on my phone.

201
00:10:21.020 --> 00:10:25.100
Do you have to talk on your phone? Um, well, I'm not supposed to obviously,

202
00:10:25.101 --> 00:10:26.600
but it's more for research purposes, right?

203
00:10:26.990 --> 00:10:30.740
<v 1>Research. Do you know how to tech talk at all? No. Okay.</v>

204
00:10:30.770 --> 00:10:32.960
My 12 year old is obsessed. Oh really? Oh yeah.

205
00:10:32.990 --> 00:10:36.740
She can't even sit around if she's standing still for five minutes,

206
00:10:36.741 --> 00:10:41.390
she just starts like she starts talking. And that's the thing, I mean,

207
00:10:42.200 --> 00:10:46.040
2012, 2012. So the Mayans were right, right. 2012,

208
00:10:46.041 --> 00:10:49.460
the platform announced an update to discovery system, uh,

209
00:10:49.490 --> 00:10:50.810
designed to identify the videos.

210
00:10:50.811 --> 00:10:53.930
People actually want to watch by prioritizing videos that hold attention

211
00:10:53.931 --> 00:10:54.680
throughout,

212
00:10:54.680 --> 00:10:57.950
as well as increasing the amount of time a user spends on the platform overall

213
00:10:58.490 --> 00:10:58.670
Yuto,

214
00:10:58.670 --> 00:11:02.930
YouTube could assure advertisers that it was providing a valuable high quality

215
00:11:02.931 --> 00:11:05.900
experience for people. Yeah. So, um,

216
00:11:06.020 --> 00:11:08.570
that that's beginning of the end. Yep.

217
00:11:08.720 --> 00:11:12.170
<v 0>So 2012 and YouTube timeline, I mean, um, you know,</v>

218
00:11:12.171 --> 00:11:14.960
the Twitter and Facebook world,

219
00:11:15.050 --> 00:11:19.730
I think introduces the retweet and reshare buttons in the 2009 to 2010 kind of

220
00:11:19.731 --> 00:11:20.564
time period.

221
00:11:20.840 --> 00:11:24.680
So you end up with this world where the things that we're most paying attention

222
00:11:24.681 --> 00:11:28.160
to are based on algorithms choosing for us.

223
00:11:28.220 --> 00:11:31.280
And so this sort of deeper argument that's in the film that I'm not sure

224
00:11:31.281 --> 00:11:36.170
everyone picks up on is these technology systems have taken control

225
00:11:36.200 --> 00:11:37.550
of human choice.

226
00:11:37.700 --> 00:11:41.600
They take control of humanity because they're controlling the information that

227
00:11:41.601 --> 00:11:46.040
all of us are getting right. Think about every election. Like, um, I think, uh,

228
00:11:46.041 --> 00:11:47.570
Facebook has kind of a voting machine,

229
00:11:47.810 --> 00:11:51.620
but it's a sort of indirect voting machine because it controls the information

230
00:11:51.621 --> 00:11:53.980
for four years that you're in charge tire getting,

231
00:11:54.520 --> 00:11:57.460
and then everyone votes based on that information. Now you could say, well,

232
00:11:57.461 --> 00:12:01.330
hold on, radio and television were there and were partisan before that,

233
00:12:01.630 --> 00:12:03.340
but actually TV, um,

234
00:12:03.580 --> 00:12:07.750
radio and TV are often getting their news stories from Twitter and Twitter is

235
00:12:07.930 --> 00:12:09.730
rec is recommending things based on these algorithms.

236
00:12:10.150 --> 00:12:13.960
So when you control the information that an entire population is getting,

237
00:12:13.961 --> 00:12:17.200
you're controlling their choices. I mean, literally in military theory,

238
00:12:17.201 --> 00:12:18.640
if I want to screw up your military,

239
00:12:18.641 --> 00:12:20.320
I want to control the information that it's getting.

240
00:12:20.321 --> 00:12:21.940
I want to confuse the enemy.

241
00:12:22.420 --> 00:12:26.050
And that information funnel is the very thing that's been corrupted.

242
00:12:26.140 --> 00:12:28.120
And it's like the Flint water supply for our minds.

243
00:12:29.070 --> 00:12:33.750
<v 4>Episodes of the Joe Rogan experience are now free on Spotify. That's right.</v>

244
00:12:33.960 --> 00:12:36.900
They're free from September 1st is December 1st.

245
00:12:36.901 --> 00:12:39.330
They're going to be available everywhere, but after December 1st,

246
00:12:39.331 --> 00:12:43.020
they will only be available on Spotify, but they will be free.

247
00:12:43.230 --> 00:12:47.040
That includes the video. The video will also be there. It'll also be free.

248
00:12:47.730 --> 00:12:50.970
That's all we're asking. Just go download Spotify much though.

