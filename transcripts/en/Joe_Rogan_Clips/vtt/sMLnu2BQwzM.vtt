WEBVTT

1
00:00:00.990 --> 00:00:03.720
<v 0>The Joe Rogan experience. Pleasure to meet you.</v>

2
00:00:03.780 --> 00:00:07.200
<v 1>How did you get started on this and how did,</v>

3
00:00:07.230 --> 00:00:08.790
how did you get interested in this subject?

4
00:00:09.380 --> 00:00:13.400
<v 2>I got interested in this subject through climate change and climate denial</v>

5
00:00:13.410 --> 00:00:18.200
specifically, I'm an environmental attorney. And back in the 1990s,

6
00:00:18.201 --> 00:00:22.460
I worked for the state of Minnesota and we found ourselves very

7
00:00:22.461 --> 00:00:23.300
briefly,

8
00:00:23.600 --> 00:00:27.560
sort of on the front lines of the scientific debate over climate change.

9
00:00:27.561 --> 00:00:32.150
And the way that happened was the state had passed a law saying that

10
00:00:32.151 --> 00:00:32.751
utilities,

11
00:00:32.751 --> 00:00:37.610
regulators should try to estimate the cost to the environment of generating

12
00:00:37.611 --> 00:00:42.230
electricity. We get most of our power from coal, or we did then. Um,

13
00:00:42.740 --> 00:00:45.380
and so we looked at coal emissions.

14
00:00:45.381 --> 00:00:48.800
We looked at the traditional pollutants that we had regulated for a long time.

15
00:00:48.801 --> 00:00:53.060
And my client was the pollution control agency. So I was familiar with those,

16
00:00:53.750 --> 00:00:55.760
what we also looked at though.

17
00:00:55.790 --> 00:01:00.530
And I wasn't familiar with was CO2 and its effect on climate

18
00:01:00.531 --> 00:01:05.060
change because, uh, while that was a big issue globally,

19
00:01:05.061 --> 00:01:08.420
there was already a global treaty signed, um,

20
00:01:08.930 --> 00:01:11.990
to fight climate change. States had not taken a look at that.

21
00:01:12.080 --> 00:01:17.060
And what happened was we struck a nerve with the coal industry

22
00:01:17.330 --> 00:01:22.100
and they sent to Minnesota, a bunch of witnesses, a bunch of scientists,

23
00:01:23.000 --> 00:01:27.350
uh, to testify that we did not have to worry about climate change and it wasn't

24
00:01:27.410 --> 00:01:31.970
going to happen. Or if it did be just, just a little and we'd like it,

25
00:01:32.360 --> 00:01:35.390
and that all of those scientists,

26
00:01:35.510 --> 00:01:39.350
the IPC intergovernmental panel on climate change, uh,

27
00:01:39.380 --> 00:01:42.200
those scientists, uh, that the rest of the world,

28
00:01:42.201 --> 00:01:46.760
including the U S government and the treaty signed by George H w Bush, uh,

29
00:01:46.880 --> 00:01:51.590
the ones that they were relying on, those scientists were basically biased.

30
00:01:51.620 --> 00:01:55.520
They were biased because they, they were in it for the money.

31
00:01:55.521 --> 00:02:00.320
Somehow they wanted research grants, or they had some political agenda.

32
00:02:00.321 --> 00:02:03.500
It was kind of vague. Um, but, but it was clear.

33
00:02:03.501 --> 00:02:06.590
They did not want us worrying about this issue. They,

34
00:02:06.650 --> 00:02:08.960
they told you that it would be just a little and that.

35
00:02:08.960 --> 00:02:12.140
<v 3>You would like it. What do you mean by that? Well.</v>

36
00:02:12.340 --> 00:02:14.360
<v 2>Uh, a couple of things, one of the arguments,</v>

37
00:02:14.361 --> 00:02:19.190
and you will still hear this sometimes is that CO2 is a

38
00:02:19.191 --> 00:02:21.950
plant fertilizer, which is true. Um,

39
00:02:22.010 --> 00:02:26.960
and therefore more CO2 makes the world a happier place for plants

40
00:02:27.170 --> 00:02:30.170
and, and therefore better for everybody else. And to the,

41
00:02:30.171 --> 00:02:33.800
to the point where one of the coal interests who were in that, uh,

42
00:02:33.860 --> 00:02:38.600
who were parties had put out a video saying that the earth was deficient

43
00:02:38.690 --> 00:02:43.310
in CO2 and by digging up the colon burning it, we were, we were correcting that.

44
00:02:44.510 --> 00:02:47.510
Um, yeah, so that was one of the arguments. The other was.

45
00:02:47.540 --> 00:02:49.910
<v 0>You know, it'll be mild, it'll be warm.</v>

46
00:02:49.910 --> 00:02:54.320
<v 2>The winters won't be as cold. And, and Hey, this is Minnesota. So, you know,</v>

47
00:02:54.321 --> 00:02:58.010
you guys are going to appreciate those warmer winters. Um, so yeah,

48
00:02:58.011 --> 00:03:02.230
there was a lot of crazy stuff that, that hasn't gone away. In fact, many,

49
00:03:02.380 --> 00:03:04.330
in many ways it's gotten a lot worse,

50
00:03:04.840 --> 00:03:07.930
but there were certainly enough to leave me shocked.

51
00:03:08.220 --> 00:03:12.780
<v 1>Was that the first time you were ever aware that corporations do send in people</v>

52
00:03:12.781 --> 00:03:17.160
to try to diffuse arguments or pollute the wall? I.

53
00:03:17.160 --> 00:03:18.840
<v 2>Don't think I was quite that naive,</v>

54
00:03:18.870 --> 00:03:21.300
but I'd certainly never seen anything like this. I mean,

55
00:03:21.301 --> 00:03:24.070
these were people under oath, you know, and,

56
00:03:24.071 --> 00:03:28.590
and they were saying things that were pretty extreme, uh, and,

57
00:03:28.640 --> 00:03:31.380
and many of which would just get a lot more extreme.

58
00:03:31.590 --> 00:03:33.750
And there were scientists that many, yes,

59
00:03:33.930 --> 00:03:36.330
they're the ones I cross-examined where mainly the scientists,

60
00:03:36.360 --> 00:03:39.990
they also sentenced some other witnesses as well. So they didn't,

61
00:03:39.991 --> 00:03:42.240
they didn't actually work in a coal company.

62
00:03:42.241 --> 00:03:44.700
They were hired by the coal industry to come in and testify.

63
00:03:45.210 --> 00:03:49.890
<v 1>And the scientists that presumably they were paid to do this. Yes.</v>

64
00:03:50.190 --> 00:03:53.940
So is that, I mean, how do you track that? Like, if you,

65
00:03:53.941 --> 00:03:57.150
if you have scientists and they come in and they say things that, you know,

66
00:03:57.180 --> 00:03:59.760
are not accurate or deceptive,

67
00:04:00.180 --> 00:04:03.560
how do you find out what their motivation is? Do you do,

68
00:04:03.630 --> 00:04:05.460
did you ask them if they'd been paid.

69
00:04:06.000 --> 00:04:09.870
<v 2>We're able to put some things in the record regarding how much money they had</v>

70
00:04:09.871 --> 00:04:12.600
gotten from different fossil fuel interests over the years.

71
00:04:12.601 --> 00:04:16.680
So we definitely did a point to that argue about that.

72
00:04:17.010 --> 00:04:21.600
We didn't realize some of the witnesses had a much deeper history than we

73
00:04:21.601 --> 00:04:24.620
understood in science denial. One of the witnesses was, uh,

74
00:04:25.150 --> 00:04:27.870
a pretty prominent scientist, uh,

75
00:04:27.960 --> 00:04:31.980
named Frederick sites who has since died. But, um,

76
00:04:32.070 --> 00:04:35.280
but we didn't know what, I didn't know when I cross examined him. I mean,

77
00:04:35.281 --> 00:04:37.440
this was a shoestring operation,

78
00:04:37.740 --> 00:04:40.710
was that he had spent a lot of time actually consulting for the tobacco

79
00:04:40.711 --> 00:04:45.090
industry. Um, so that would have been nice too, to bring up. We had talked.

80
00:04:45.090 --> 00:04:47.850
<v 1>About just before the podcast, the film merchants of doubt,</v>

81
00:04:47.851 --> 00:04:50.540
and that's how it kind of got into your work, right? That,

82
00:04:50.541 --> 00:04:52.140
that film touches on that,

83
00:04:52.141 --> 00:04:56.910
how people who worked for the tobacco industry eventually went to work to deny

84
00:04:56.940 --> 00:04:59.010
the manmade climate change.

85
00:04:59.280 --> 00:05:03.180
<v 2>Right? Well, in his case, he had actually been a physicist who was, uh,</v>

86
00:05:03.330 --> 00:05:05.940
very involved in the cold war weapons program.

87
00:05:05.950 --> 00:05:08.220
So he kind of came at it from that direction.

88
00:05:08.760 --> 00:05:13.380
And it wasn't until really he had retired from his remain scientific and

89
00:05:13.381 --> 00:05:17.610
academic work that he was brought in to work for the tobacco industry.

90
00:05:17.611 --> 00:05:20.940
But what happened was this handful of scientists, um,

91
00:05:21.900 --> 00:05:26.040
profiled in that movie and in the book by the same name, um,

92
00:05:26.220 --> 00:05:29.910
they would also then work with these nonprofit groups,

93
00:05:29.911 --> 00:05:33.900
these free market groups that were strongly opposed to regulation of industries.

94
00:05:33.901 --> 00:05:34.530
And so,

95
00:05:34.530 --> 00:05:39.150
and those same groups then would address lots of different issues from tobacco

96
00:05:39.151 --> 00:05:43.590
to ozone and, and now to climate change and, and really a lot of other,

97
00:05:44.340 --> 00:05:47.580
uh, scientific issues as well for industries facing regulation.

98
00:05:47.880 --> 00:05:52.470
<v 1>Another really divisive aspect of this is that it's become some sort of a</v>

99
00:05:52.500 --> 00:05:54.960
left versus right ideological issue.

100
00:05:55.140 --> 00:05:58.460
Like there's a lot of people in the right that had conversations with people

101
00:05:58.461 --> 00:06:01.070
that really don't have any idea what they're talking about,

102
00:06:01.071 --> 00:06:04.280
where they instantly deny that climate change is a real issue.

103
00:06:04.640 --> 00:06:06.530
And when you press them on it. And just,

104
00:06:06.590 --> 00:06:11.390
that's one of the benefits of having the sort of long form conversations is

105
00:06:11.391 --> 00:06:15.020
that if you're doing this on CNN and it's one of those talking head things where

106
00:06:15.021 --> 00:06:17.570
you only have seven minutes and it's three people shouting over each other,

107
00:06:17.750 --> 00:06:20.960
it's very hard to get to the heart of why do you believe this? Yeah.

108
00:06:21.230 --> 00:06:25.880
But when you're talking over long podcasts hours long,

109
00:06:26.480 --> 00:06:30.950
you get to these people and they'll adamantly

110
00:06:30.951 --> 00:06:32.840
deny that it's an issue,

111
00:06:33.470 --> 00:06:37.190
but they don't know why do you know what I'm saying? It's like a thing,

112
00:06:37.191 --> 00:06:40.520
if you're a right wing pundit or right wing person, and you're saying,

113
00:06:40.521 --> 00:06:43.310
right-wing things you're going to say climate change is not our issue.

114
00:06:43.430 --> 00:06:45.200
What are you right now? Is the economy, right?

115
00:06:45.280 --> 00:06:47.300
What we've got to do right now is support jobs and people,

116
00:06:47.330 --> 00:06:48.920
there's a lot of people that need to put food on the table.

117
00:06:49.010 --> 00:06:49.880
There's a lot of people that need to,

118
00:06:50.030 --> 00:06:54.440
and then they get this sort of a ranting raving pro

119
00:06:54.500 --> 00:06:56.000
economic standpoint.

120
00:06:56.210 --> 00:07:01.070
And it becomes a denial of environmental problems that's becomes

121
00:07:01.100 --> 00:07:05.630
left versus right. It's very strange. I don't understand why anyone,

122
00:07:05.780 --> 00:07:08.000
like how can not beneath that, not be a universal issue.

123
00:07:08.180 --> 00:07:12.740
How could anyone not want the world to be better for our grandchildren?

124
00:07:12.960 --> 00:07:15.170
How could anybody not want less pollution?

125
00:07:15.620 --> 00:07:20.540
But it becomes this thing where we have all these different categories that are

126
00:07:20.541 --> 00:07:23.900
left and right. And once you're on one side,

127
00:07:24.080 --> 00:07:28.920
you automatically seem to oppose those things that are there in the

128
00:07:29.090 --> 00:07:30.790
other party's ideas. Well.

129
00:07:30.790 --> 00:07:33.220
<v 2>In fact, there's one, uh,</v>

130
00:07:33.790 --> 00:07:38.650
survey I cite in here that showed that climate change was the most

131
00:07:38.680 --> 00:07:41.950
polarized issue in the American political landscape.

132
00:07:41.980 --> 00:07:44.200
Even more than abortion really. Um,

133
00:07:44.680 --> 00:07:47.950
and more than Bush now that was a snapshot in time.

134
00:07:47.951 --> 00:07:49.660
And I think maybe that's changing.

135
00:07:49.661 --> 00:07:53.560
Certainly you see with younger Republicans a lot more concerned about climate

136
00:07:53.561 --> 00:07:56.020
change, but you're absolutely right. I mean,

137
00:07:56.021 --> 00:08:00.460
it remains very polarized and I don't think you can understand it, you know,

138
00:08:00.461 --> 00:08:03.760
it's not in, I don't think it makes sense from an ideological standpoint.

139
00:08:03.761 --> 00:08:07.720
I think it makes sense from a tribal standpoint that we have divided and,

140
00:08:08.170 --> 00:08:12.190
and it feels good to believe the same things as the people you are affiliated

141
00:08:12.191 --> 00:08:16.330
with. And it's tense to, to not believe the same things that just saw,

142
00:08:16.390 --> 00:08:19.690
that's a source of, of hardship. And, you know,

143
00:08:19.750 --> 00:08:23.620
the reason it's such a big problem here is that this isn't just about making the

144
00:08:23.621 --> 00:08:25.000
world better for our grandkids.

145
00:08:25.000 --> 00:08:29.410
It's about avoiding catastrophe for our grandkids. Um, and, and so that's why,

146
00:08:29.440 --> 00:08:34.270
you know, I, it is finally rising to the surface within the democratic party.

147
00:08:34.271 --> 00:08:37.900
I mean, it's been ignored or downplayed for too long.

148
00:08:37.901 --> 00:08:39.490
And certainly in the national campaigns,

149
00:08:39.491 --> 00:08:43.570
it was never perceived to be important enough or winning enough an issue to get

150
00:08:43.571 --> 00:08:48.520
a lot of attention. Now we see largely driven by the youth movement,

151
00:08:49.330 --> 00:08:53.590
um, insistence that, yeah, it's time. It is absolutely time.

152
00:08:53.591 --> 00:08:57.750
It's 30 years past that we get very aggressive about this. And,

153
00:08:57.810 --> 00:09:01.110
and so I don't know what happens now with,

154
00:09:01.320 --> 00:09:02.940
with COVID with George Floyd.

155
00:09:02.941 --> 00:09:06.000
Obviously there are other issues dominating the news right now,

156
00:09:06.510 --> 00:09:08.670
but I really hope we,

157
00:09:08.680 --> 00:09:12.630
we hang on to this issue as a critical one for the election and, and,

158
00:09:12.700 --> 00:09:16.290
and don't stop there because this is going to continue to require lots of

159
00:09:16.291 --> 00:09:18.300
pressure to make sure that we make the changes we need.

160
00:09:18.650 --> 00:09:20.750
<v 1>Yeah. I don't think it's going to go away, I think,</v>

161
00:09:20.751 --> 00:09:23.780
but other issues do come to the forefront. But what you said,

162
00:09:23.781 --> 00:09:27.560
I think is really interesting is that it gives you comfort to agree with the

163
00:09:27.561 --> 00:09:29.420
other people that are in your party, in your group.

164
00:09:29.750 --> 00:09:33.140
And that's something that is exacerbated by social media and manipulated by

165
00:09:33.141 --> 00:09:33.890
social media.

166
00:09:33.890 --> 00:09:37.910
And it's one of the weirder things about it is that a corporation could

167
00:09:38.330 --> 00:09:40.160
legally, um,

168
00:09:40.820 --> 00:09:45.680
create hundreds if not thousands of fake pages and

169
00:09:45.681 --> 00:09:47.270
then use those to make, like,

170
00:09:47.450 --> 00:09:51.530
I'm sure you're aware of the internet research agency from Russia that had an

171
00:09:51.531 --> 00:09:55.880
impact on the 2016 elections and, um, Renee de, uh,

172
00:09:55.940 --> 00:09:59.060
did some pretty fascinating work on that,

173
00:09:59.061 --> 00:10:03.620
where she did a deep dive into how these accounts,

174
00:10:03.621 --> 00:10:05.030
whether it's Facebook or Instagram,

175
00:10:05.031 --> 00:10:08.390
or what have you have been manipulated and how they, how they use them,

176
00:10:08.780 --> 00:10:13.310
where in one point they had a pro Texas group

177
00:10:13.520 --> 00:10:18.470
meet up at the exact same time as a pro Muslim group on

178
00:10:18.471 --> 00:10:21.770
the exact same block. Like they manipulated it.

179
00:10:21.830 --> 00:10:24.080
Like there was no one child's play exactly.

180
00:10:24.081 --> 00:10:28.100
It was like they were moving pieces on a chess board and they literally set up

181
00:10:28.460 --> 00:10:32.600
altercations. And you would imagine that, I mean,

182
00:10:32.601 --> 00:10:35.840
I don't know what these fossil fuel companies or,

183
00:10:35.850 --> 00:10:37.970
or any kind of company that's involved in, any,

184
00:10:38.240 --> 00:10:42.440
anything that would be considered sketchy environmentally w I don't know,

185
00:10:42.470 --> 00:10:46.790
I don't know how many manipulating sites they run or manipulative

186
00:10:47.240 --> 00:10:48.500
social media accounts they run,

187
00:10:48.740 --> 00:10:53.390
but I would imagine that's got to be part of the game plan because online

188
00:10:53.391 --> 00:10:58.100
discourse it's so easy to throw monkey wrenches into the

189
00:10:58.101 --> 00:11:01.160
gears is throw easy to throw sand into the gas tank.

190
00:11:01.161 --> 00:11:05.630
It's so easy to sort of monkey with the numbers and change the

191
00:11:05.631 --> 00:11:10.610
ideas that are being discussed and change the narratives that it's a, it's a,

192
00:11:10.640 --> 00:11:11.330
it's a term.

193
00:11:11.330 --> 00:11:15.830
It's just a way that you can sort of shift the public's

194
00:11:15.950 --> 00:11:17.600
interests and opinions on things.

195
00:11:17.900 --> 00:11:20.990
<v 2>Yeah. I mean, if you're willing to lie and manipulate, then you actually,</v>

196
00:11:21.020 --> 00:11:23.000
you have a, obviously a huge advantage,

197
00:11:23.001 --> 00:11:27.860
but there's also just the basic human tendency that when we talk to people

198
00:11:27.861 --> 00:11:31.580
we already agree with, we tend to then become stronger in our opinions.

199
00:11:31.581 --> 00:11:36.470
And so we, we, we get polarized basically, and that's even before social media.

200
00:11:36.471 --> 00:11:40.920
So then you sort of weaponize that polarization, that tendency, and,

201
00:11:41.090 --> 00:11:44.030
and you've got an algorithm that says, well, if you like that video,

202
00:11:44.031 --> 00:11:47.420
how about this video? And suddenly people are getting, you know,

203
00:11:47.570 --> 00:11:52.220
totally radicalized, you know, on, on climate change or on other issues. And so,

204
00:11:52.221 --> 00:11:54.940
yeah, I mean, it, it is a huge problem.

205
00:11:54.941 --> 00:11:59.650
How do we overcome the social divisions, the social distrust?

206
00:11:59.651 --> 00:12:03.670
How do we overcome the denial? Um, and you know,

207
00:12:03.700 --> 00:12:08.680
I think if, if the patterns in the book come to the fore, we will,

208
00:12:08.681 --> 00:12:11.260
society will find ways to build trust. Again,

209
00:12:11.261 --> 00:12:14.740
it'll probably have a lot to do with maintaining longterm accountability and not

210
00:12:14.741 --> 00:12:19.150
just a flash reaction to what you hear, but it could very well take decades.

211
00:12:19.180 --> 00:12:22.510
And we will have a lot of damage done in the meantime.

212
00:12:23.250 --> 00:12:27.870
<v 1>Or if there's going to be a time where there are laws against</v>

213
00:12:27.871 --> 00:12:32.490
social media, manipulation like that, because right now they're not. And it's,

214
00:12:32.491 --> 00:12:36.180
yeah, it seems like there has to be, because if you,

215
00:12:36.560 --> 00:12:39.720
I can't imagine I'm not naive enough to imagine that what's happening with the

216
00:12:39.750 --> 00:12:43.290
internet research agency in Russia. It's not happening here. It has to be,

217
00:12:43.700 --> 00:12:45.540
and they understand the effectiveness of it. It's been,

218
00:12:45.690 --> 00:12:49.350
well-documented the idea that, that corporations are going to step back and go,

219
00:12:49.351 --> 00:12:51.960
well, that's not our business. That's not what we do. I mean,

220
00:12:51.961 --> 00:12:54.060
that's an incredibly effective tool.

221
00:12:54.150 --> 00:12:58.350
And if you were going to use it to manipulate opinions on whether it's climate

222
00:12:58.351 --> 00:13:00.840
change or, you know, anything, you know,

223
00:13:01.050 --> 00:13:03.960
pharmaceutical drug overdoses, like whatever,

224
00:13:04.020 --> 00:13:06.330
whatever it is that you want to manipulate people with,

225
00:13:06.780 --> 00:13:09.150
I would imagine that that's a gigantic issue,

226
00:13:09.151 --> 00:13:13.050
but it's not something that really gets discussed in terms of,

227
00:13:13.380 --> 00:13:16.380
in terms of passing legislation to prevent that stuff.

228
00:13:17.250 --> 00:13:17.891
<v 2>Yeah. And,</v>

229
00:13:17.891 --> 00:13:22.700
and hopefully it gets more and more discussed because it is very scary. I mean,

230
00:13:22.980 --> 00:13:24.210
it turns out we,

231
00:13:24.630 --> 00:13:28.830
humans are easily manipulated and we're easily manipulated even before social

232
00:13:28.831 --> 00:13:29.190
media.

233
00:13:29.190 --> 00:13:34.190
But now there is this incredibly sophisticated engine to drive us apart to

234
00:13:34.210 --> 00:13:39.150
drive us in the direction that those best at manipulating us want us

235
00:13:39.151 --> 00:13:39.960
to go.

236
00:13:39.960 --> 00:13:42.660
<v 1>And it's addictive, which is even crazier. Yeah.</v>

237
00:13:42.900 --> 00:13:45.630
It's a completely addictive mechanism.

238
00:13:46.380 --> 00:13:50.220
Like people are lost in their phones and lost in their computers,

239
00:13:50.221 --> 00:13:52.890
like when they're checking their social media stuff.

240
00:13:52.891 --> 00:13:57.540
And that's one of the more interesting things about the social media algorithms

241
00:13:57.780 --> 00:14:02.610
that it's been determined that when people are upset about

242
00:14:02.611 --> 00:14:05.490
things and when they're angry about things, they post more.

243
00:14:05.640 --> 00:14:09.660
So it's more valuable. So the algorithms favor people being upset.

244
00:14:09.661 --> 00:14:12.030
So they'll send you if you, if you, uh,

245
00:14:12.060 --> 00:14:15.180
find abortion a hot topic or environmental issues,

246
00:14:15.510 --> 00:14:18.570
they'll start sending you those. That's, what's going to show up in your feed.

247
00:14:18.960 --> 00:14:22.740
You're going to get more of the cause is what you engage in. And, uh, it's,

248
00:14:22.741 --> 00:14:23.880
what's fascinating is it's,

249
00:14:24.000 --> 00:14:28.140
it's not even really malicious in that.

250
00:14:28.141 --> 00:14:32.010
It's just pragmatic because I have a friend who did an experiment, my friend,

251
00:14:32.040 --> 00:14:32.940
Ari, uh,

252
00:14:32.941 --> 00:14:37.080
wanted to find out what would happen if he just looked up puppies.

253
00:14:37.920 --> 00:14:41.820
So he just looked up puppies on YouTube and looked up puppies everywhere.

254
00:14:41.970 --> 00:14:43.950
And his feed was overwhelmed by puppies.

255
00:14:44.400 --> 00:14:49.050
So it's not like this vicious plot to only feed you

256
00:14:49.051 --> 00:14:51.090
things that you hate, just human.

257
00:14:51.770 --> 00:14:56.360
We tend to look at things that us off. It was just really kind of crazy.

258
00:14:56.650 --> 00:14:56.980
And so.

259
00:14:56.980 --> 00:15:01.870
<v 2>Now we have a very sophisticated machine to drive us in the direction of getting</v>

260
00:15:01.871 --> 00:15:02.704
more off.

261
00:15:02.950 --> 00:15:07.810
<v 1>And that sophisticated machine is clearly using the same sort of</v>

262
00:15:07.811 --> 00:15:11.920
deceptive deceptive tactics to try to diminish their

263
00:15:11.921 --> 00:15:14.320
responsibility for what they're doing. Yeah.

264
00:15:14.410 --> 00:15:16.450
<v 2>Exactly. And you know,</v>

265
00:15:16.640 --> 00:15:19.720
when one of the things that makes these tactics I think works so well is that

266
00:15:19.721 --> 00:15:22.660
they really are based in human nature. I mean,

267
00:15:22.661 --> 00:15:26.470
I think that if you are an executive, uh, you know, your,

268
00:15:26.471 --> 00:15:31.360
your instinct is that you are doing fine and your instinct is that the other

269
00:15:31.361 --> 00:15:35.530
side is wrong in that. And that psychological reflex then, you know,

270
00:15:35.531 --> 00:15:37.780
becomes a foundation for a corporate strategy.

271
00:15:38.260 --> 00:15:41.800
And then that corporate strategy becomes the basis of kind of its own new

272
00:15:41.801 --> 00:15:46.100
industry of, of public relations folks and advertising people in lawyers and,

273
00:15:46.101 --> 00:15:48.160
and think tanks who will promote that.

274
00:15:48.700 --> 00:15:51.340
And then that becomes an ideology.

275
00:15:51.730 --> 00:15:54.940
And that's certainly what we saw the progression for climate change and I,

276
00:15:54.970 --> 00:15:59.380
and I think, or climate denial, uh, and that's a dangerous trend.

277
00:16:07.510 --> 00:16:07.510
<v 4>[Inaudible].</v>

