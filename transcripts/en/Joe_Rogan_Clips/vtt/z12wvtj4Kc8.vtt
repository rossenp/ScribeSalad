WEBVTT

1
00:00:00.990 --> 00:00:02.730
<v 0>The Joe Rogan experience.</v>

2
00:00:02.820 --> 00:00:07.650
<v 2>Do you think that the algorithms are designed to do this,</v>

3
00:00:07.680 --> 00:00:12.300
or do you think that it's just a function of human nature that we tend

4
00:00:12.301 --> 00:00:16.950
to gravitate towards things that outrage us and then

5
00:00:17.280 --> 00:00:21.930
huddle up together in echo chambers that this is just a natural tribal

6
00:00:21.931 --> 00:00:22.800
behavior.

7
00:00:23.250 --> 00:00:28.200
And then what the algorithms do is essentially just highlight

8
00:00:28.260 --> 00:00:29.520
what we're really interested in.

9
00:00:30.180 --> 00:00:33.030
<v 1>They magnified in a feedback loop, right.</v>

10
00:00:33.090 --> 00:00:37.680
So you're right to say that humans do have these traits.

11
00:00:38.460 --> 00:00:40.380
Um, you know, and I, I haven't designed the algorithms,

12
00:00:40.381 --> 00:00:43.650
but I've also talked to people who have, and you know, a lot of them,

13
00:00:43.680 --> 00:00:46.350
they don't even understand how they work at a certain point.

14
00:00:46.351 --> 00:00:48.840
Like they're off to the races. Have you seen the social dilemma?

15
00:00:48.990 --> 00:00:51.540
<v 0>Yeah. Yeah. W what'd you think about that? It's great.</v>

16
00:00:51.750 --> 00:00:54.990
<v 1>It's great. Yeah. Yeah. Um, to.</v>

17
00:00:56.250 --> 00:00:58.200
<v 0>Uh, the conclusions that they draw. Oh.</v>

18
00:00:58.260 --> 00:00:59.640
<v 1>Certainly. I mean, I, yeah,</v>

19
00:00:59.670 --> 00:01:03.330
I made a film about eight years ago called terms and conditions may apply,

20
00:01:03.990 --> 00:01:08.670
you know, and that, that came out right before the Edward Snowden revelations,

21
00:01:08.970 --> 00:01:12.480
you know, and when it came out, peop the initial response was like, oh,

22
00:01:12.481 --> 00:01:14.460
this is maybe conspiratorial.

23
00:01:14.760 --> 00:01:19.650
Surely the government doesn't have this much insight into our

24
00:01:19.860 --> 00:01:23.670
behavior and, and, you know, an access to, um,

25
00:01:24.000 --> 00:01:26.730
to our devices and our personal information.

26
00:01:26.731 --> 00:01:29.100
And then this known revelations came out and then it was like, oh, well,

27
00:01:29.101 --> 00:01:33.900
maybe the series didn't go far enough. Um, you know,

28
00:01:33.901 --> 00:01:38.850
and I, uh, and, and back then we had talked about the idea of how,

29
00:01:38.851 --> 00:01:43.260
how is technology influencing us? How is it changing us, manipulating us?

30
00:01:43.261 --> 00:01:46.980
And it didn't feel like that was the biggest story at the time.

31
00:01:47.520 --> 00:01:52.110
And this question of privacy and how our rights are being eroded through

32
00:01:52.111 --> 00:01:55.790
these agreements that nobody ever really reads. And you could find all,

33
00:01:55.791 --> 00:01:58.350
all kinds of, uh, juicy tidbits, uh,

34
00:01:58.410 --> 00:02:01.380
hidden in there in terms of what the companies were actually doing and kind of

35
00:02:01.381 --> 00:02:06.120
revealing this unholy collusion between the government and big tech,

36
00:02:07.170 --> 00:02:12.060
um, you know, but, uh, it, at the time people would often say, well,

37
00:02:12.061 --> 00:02:14.430
what's the cost, you know, what's the big deal.

38
00:02:14.431 --> 00:02:16.710
If they're mining my personal data to serve me with ads.

39
00:02:17.010 --> 00:02:21.810
And I'd say that the environment we find ourselves in now is the

40
00:02:21.811 --> 00:02:22.644
cost.

41
00:02:23.070 --> 00:02:27.600
<v 0>Do you do anything to personally protect your data? Do you like use duck,</v>

42
00:02:27.601 --> 00:02:32.400
duck go for searches and things along those lines, you've brave as a browser.

43
00:02:32.401 --> 00:02:33.330
Like, do you do that stuff?

44
00:02:33.450 --> 00:02:37.890
<v 1>All that. Yeah. Yeah. You signal VPNs, VPNs. I mean, I,</v>

45
00:02:37.891 --> 00:02:39.840
I do my best, you know,

46
00:02:39.990 --> 00:02:44.460
if a government actor really wants to get at ya, they're going to be able to,

47
00:02:44.910 --> 00:02:49.560
I mean, you saw the NSO ClickList spyware story, right? I mean,

48
00:02:49.770 --> 00:02:50.460
if,

49
00:02:50.460 --> 00:02:55.410
if there's a zero day that allows you to get access to

50
00:02:55.411 --> 00:02:57.630
a microphone and everything that someone's doing on their phone,

51
00:02:57.631 --> 00:03:01.030
without them even having to click on link, you know, it's game over it,

52
00:03:01.390 --> 00:03:04.390
of course governments will abuse that.

53
00:03:05.380 --> 00:03:07.480
<v 0>And probably are right now. Probably.</v>

54
00:03:07.600 --> 00:03:09.400
<v 1>Are right now. Yeah. We're sort of we're phones at.</v>

55
00:03:11.680 --> 00:03:12.910
<v 0>Yeah. It's um,</v>

56
00:03:13.060 --> 00:03:17.380
it's interesting because you would think that there'd be a market for a

57
00:03:17.381 --> 00:03:22.330
platform that becomes Bulletproof and there have bins, um,

58
00:03:22.390 --> 00:03:27.100
you know, Linux-based, uh, cell phone operating system phones that they sell.

59
00:03:27.101 --> 00:03:29.620
Like they buy a phone, you get a Google phone,

60
00:03:29.621 --> 00:03:33.250
they de Google it and put different software on it and stuff.

61
00:03:33.251 --> 00:03:36.220
But I'm not sure if that's like,

62
00:03:36.610 --> 00:03:40.900
if you're deluding yourself into believing that you're actually protected with

63
00:03:40.901 --> 00:03:43.020
that stuff where you're actually are protected. Right.

64
00:03:43.021 --> 00:03:44.800
I would think they could work around all those things,

65
00:03:45.070 --> 00:03:48.940
especially something that's, I mean, it's essentially like open source, right?

66
00:03:48.970 --> 00:03:53.080
Like, like if it's a Linux-based operating system,

67
00:03:53.500 --> 00:03:55.030
there's some super geniuses out there.

68
00:03:55.031 --> 00:03:57.520
I'm sure they're going to be able to hack into that.

69
00:03:59.020 --> 00:04:02.140
<v 1>Yeah. I mean, I think with stuff like signal, you're just,</v>

70
00:04:02.170 --> 00:04:04.720
you're just protecting yourself as best you can, right?

71
00:04:04.721 --> 00:04:07.180
You use something that's, that's end to end encrypted.

72
00:04:07.420 --> 00:04:10.060
You're doing better than 99% of people who are out there.

73
00:04:10.061 --> 00:04:13.930
You're making somebody really have to work to get access to your tier two your

74
00:04:13.931 --> 00:04:17.570
stuff. And if you're using a VPN and you're using duck, duck go, then you're,

75
00:04:17.571 --> 00:04:20.140
you're minimizing your digital footprint. And you're not,

76
00:04:20.141 --> 00:04:24.160
as you're not worth as much of these companies, they're not able to, to,

77
00:04:25.840 --> 00:04:28.930
uh, manipulate you, I guess, in the same way through these algos.

78
00:04:28.960 --> 00:04:30.670
But let me ask you this. I mean, would you,

79
00:04:30.700 --> 00:04:33.280
what would you do about the algorithm problem? Right,

80
00:04:33.580 --> 00:04:37.450
because on the one hand algorithms are necessary for something like a search

81
00:04:37.451 --> 00:04:40.630
engine. On the other hand, um, you know, they are,

82
00:04:40.660 --> 00:04:44.650
they drive the most sensational content things like Q Anon, um,

83
00:04:45.190 --> 00:04:46.180
and, uh,

84
00:04:46.240 --> 00:04:50.680
and I think have largely facilitated the situation we find ourselves in. Now.

85
00:04:51.040 --> 00:04:54.490
<v 0>Watch the entire episode for free only on Spotify.</v>

