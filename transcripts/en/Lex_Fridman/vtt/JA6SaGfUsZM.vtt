WEBVTT

1
00:00:03.000 --> 00:00:03.960
<v 0>In case it's not clear,</v>

2
00:00:04.020 --> 00:00:07.140
teleoperation means you're controlling the truck remotely.

3
00:00:07.141 --> 00:00:10.680
Like it's a video game. So, uh,

4
00:00:11.490 --> 00:00:13.470
you gotten the chance to witness it does actually work.

5
00:00:14.460 --> 00:00:16.350
<v 1>Yeah. I mean, so it's, uh.</v>

6
00:00:16.470 --> 00:00:17.730
<v 0>What are the pros and cons.</v>

7
00:00:18.330 --> 00:00:20.310
<v 1>Problems with, with doing research like this,</v>

8
00:00:20.370 --> 00:00:24.680
with all these w with all these Silicon valley folks, the NDAs. Right.

9
00:00:25.760 --> 00:00:26.610
So I don't, you know,

10
00:00:26.611 --> 00:00:30.600
I don't know what I'm able to say about sort of watching it, but obviously the,

11
00:00:30.840 --> 00:00:33.180
their public statements about sort of what the challenges are. Right.

12
00:00:33.181 --> 00:00:33.970
And it's about the,

13
00:00:33.970 --> 00:00:38.430
the latency and the ability to sort of in real time.

14
00:00:38.820 --> 00:00:42.820
<v 0>There's challenges that, let me say one thing. Uh, so I'm talking to the,</v>

15
00:00:42.821 --> 00:00:47.370
the w w you know, I've talked to the Waymo CTO, I'm in conversations with them.

16
00:00:47.430 --> 00:00:52.320
I'm talking to the, the head of trucking Boris, uh, soften in next month.

17
00:00:52.321 --> 00:00:55.770
Actually, I'm a huge fan of his, because he was, uh,

18
00:00:55.790 --> 00:00:56.970
I think the founder of [inaudible], which is a toy robotics company. Uh,

19
00:00:56.970 --> 00:00:56.970
 so I love cute. I love human robot interaction, and he created one of the most,

20
00:00:56.970 --> 00:00:56.970
uh,  effective and beautiful toy robots. Um,

21
00:00:56.970 --> 00:00:56.970
 anyway, I keep complaining to them on email privately that, uh,

22
00:00:56.970 --> 00:00:56.970
 there's way too much marketing in these conversations and not enough showing off the, both the challenge and the beauty of the engineering efforts. And that seems to be the case for a lot of these Silicon valley tech companies. They, they put up this

23
00:00:56.970 --> 00:00:57.803
, uh, you're talking about NDAs.

24
00:01:33.330 --> 00:01:36.720
They, they they've, I, for some reason, rightfully wrongfully,

25
00:01:37.290 --> 00:01:40.140
because there's been so much hype and so much money being made,

26
00:01:40.770 --> 00:01:43.230
they don't see the, um,

27
00:01:44.190 --> 00:01:49.020
the upside in being transparent and educating the public about

28
00:01:49.021 --> 00:01:52.950
how difficult the problem is. It's much more effective for them to say,

29
00:01:53.430 --> 00:01:55.560
we'll have everything solved. This will change everything.

30
00:01:55.561 --> 00:01:57.120
This will change society. As we know it,

31
00:01:57.121 --> 00:02:00.510
and just kind of wave their hands as opposed to exploring together,

32
00:02:00.750 --> 00:02:03.360
like these different scenarios, what are the pros and cons?

33
00:02:03.570 --> 00:02:06.330
Why is it really difficult? You know, what are the,

34
00:02:06.331 --> 00:02:09.600
what are the gray areas of what works? And doesn't, uh,

35
00:02:09.601 --> 00:02:11.880
what's the role of the human in this picture of the,

36
00:02:11.881 --> 00:02:16.230
both the sort of the operators and the other humans on the road, all of that,

37
00:02:16.231 --> 00:02:18.900
which are fascinating, human problems,

38
00:02:19.230 --> 00:02:22.710
fascinating engineering problems that I wish we could have a conversation about

39
00:02:23.040 --> 00:02:26.640
as opposed to, uh, always feeling like it's just marketing talk,

40
00:02:26.820 --> 00:02:29.430
because a lot of what we're talking about now,

41
00:02:30.180 --> 00:02:33.780
even you with having private conversations under NDA,

42
00:02:34.380 --> 00:02:37.200
you still don't have the full picture of everything,

43
00:02:37.230 --> 00:02:38.700
of how difficult this problem is.

44
00:02:38.940 --> 00:02:43.740
One of the big questions I've had still have is how difficult is

45
00:02:43.741 --> 00:02:48.240
driving of disagree with, you know, Elon Musk and Jim Keller. On this point,

46
00:02:48.690 --> 00:02:53.310
I have a sense that driving is really difficult. You know,

47
00:02:53.311 --> 00:02:57.570
the task of driving just broadly, this is like philosophy talk, how,

48
00:02:57.930 --> 00:03:01.810
how much intelligence acquired to drive a car.

49
00:03:03.010 --> 00:03:07.330
So from a, like a Jim Keller was used to be the head of autopilot.

50
00:03:07.990 --> 00:03:10.690
The idea is that it's just a collision avoidance problem.

51
00:03:10.750 --> 00:03:14.500
It's like billiard balls. It's like, you have to convert the dry,

52
00:03:14.650 --> 00:03:17.620
if do some basic perception, the computer vision to convert,

53
00:03:18.160 --> 00:03:20.530
driving into a game of pool.

54
00:03:20.770 --> 00:03:23.950
And then you just have to get everything into a pocket. Yeah, to me,

55
00:03:24.170 --> 00:03:26.800
that seems to be some game theory at a dance, uh,

56
00:03:26.801 --> 00:03:29.380
combined with the fact that people's life is a stake.

57
00:03:29.500 --> 00:03:32.110
And then when people die at the hands of a robot,

58
00:03:32.140 --> 00:03:35.020
the reaction is going to be a much more complicated. So all of that,

59
00:03:35.140 --> 00:03:36.460
but that's still an open question.

60
00:03:36.610 --> 00:03:41.170
And the cool thing is all of these companies are struggling with this question

61
00:03:41.800 --> 00:03:42.700
of, of, uh,

62
00:03:42.730 --> 00:03:46.480
how difficult is it to solve this problem sufficiently such that we can build

63
00:03:46.481 --> 00:03:49.030
the business on top of it and have a product that's going to make a,

64
00:03:49.390 --> 00:03:54.220
a huge amount of money and compete with the manually driven, uh, vehicles.

65
00:03:54.430 --> 00:03:58.450
And so their teleoperation from Starsky is, is really interesting idea.

66
00:03:58.451 --> 00:04:01.150
How much can a, I mean, there's a,

67
00:04:01.210 --> 00:04:05.500
there's a few autonomous vehicle companies that tried to integrate teleoperation

68
00:04:05.590 --> 00:04:06.423
in the picture.

69
00:04:06.640 --> 00:04:10.840
Can we reduce some of the costs while still having

70
00:04:10.841 --> 00:04:15.250
reliability, like, um, uh,

71
00:04:15.280 --> 00:04:16.900
catch when the vehicle, uh,

72
00:04:16.930 --> 00:04:21.400
fails by having teleoperation it's an open question. Uh,

73
00:04:21.430 --> 00:04:25.360
so that's, that's for you scenario number two is to, uh,

74
00:04:25.390 --> 00:04:27.970
use teleoperation as part of the picture. Yeah.

75
00:04:28.210 --> 00:04:31.690
<v 1>Let me, let me follow up on that question of how hard driving is, because this,</v>

76
00:04:31.691 --> 00:04:35.500
this becomes a big question for researchers who are thinking about labor market

77
00:04:35.501 --> 00:04:36.161
impacts, right?

78
00:04:36.161 --> 00:04:40.780
Because we start from a perspective of what's hard or easy for

79
00:04:40.781 --> 00:04:44.170
humans, right. Um, and so, you know,

80
00:04:44.200 --> 00:04:47.610
if you were to look at truck driving prior to a, I mean,

81
00:04:47.611 --> 00:04:50.560
this has been a lot of thinking and debate in, um,

82
00:04:50.680 --> 00:04:55.240
in academic research circles around sort of how you estimate labor impacts,

83
00:04:55.241 --> 00:04:56.440
right. What these models look like.

84
00:04:56.860 --> 00:05:00.820
And a lot of it is about how automateable is a job object recognition,

85
00:05:01.000 --> 00:05:03.820
really easy for people, right. Really hard for computers.

86
00:05:04.090 --> 00:05:06.790
And so there's a whole bunch of things that, you know,

87
00:05:06.791 --> 00:05:10.360
truck drivers do that we see as, you know,

88
00:05:10.720 --> 00:05:15.310
super easy and as it would have been characterized 10 years ago, routine,

89
00:05:15.400 --> 00:05:18.870
and it's not for a computer, right. It's, it's,

90
00:05:18.910 --> 00:05:22.270
it turns out to be something that we do naturally that is, that is, you know,

91
00:05:22.300 --> 00:05:25.030
sort of cutting edge, right. Um, computer science.

92
00:05:25.570 --> 00:05:29.080
So on the tele operation question, I think this is, um,

93
00:05:30.130 --> 00:05:34.570
this is an more interesting one than, than people would like to sort of let on.

94
00:05:34.571 --> 00:05:39.190
I think publicly, um, there are going to be problems, right.

95
00:05:39.670 --> 00:05:39.820
Um,

96
00:05:39.820 --> 00:05:42.790
and this is one of the complexities of sort of putting these things out in the

97
00:05:42.791 --> 00:05:47.080
world. And if, if you see the real world of trucking, you realize, wow,

98
00:05:47.081 --> 00:05:50.140
it's rough. You know, they're dirt, lots, there's gravel,

99
00:05:50.380 --> 00:05:52.390
there's salt and ice and cold weather.

100
00:05:52.391 --> 00:05:55.360
And there's equipment that just gets left out in the middle of nowhere.

101
00:05:55.361 --> 00:05:58.820
And the brakes don't get maintained and somebody was supposed to serve as

102
00:05:58.821 --> 00:06:02.510
something and they didn't, you know, and so you imagine, okay,

103
00:06:02.511 --> 00:06:04.760
we've got this vehicle that can drive itself,

104
00:06:04.790 --> 00:06:08.750
which is going to require a whole lot of sensors to tell it that like the doors

105
00:06:08.751 --> 00:06:11.240
are still closed and the is still hooked up.

106
00:06:11.270 --> 00:06:15.110
And each of the tires has adequate pressure and, you know, any number of,

107
00:06:15.140 --> 00:06:15.381
you know,

108
00:06:15.381 --> 00:06:18.710
probably hundreds of sensors that are going to be sort of relaying information.

109
00:06:19.550 --> 00:06:21.320
And one of them, you know,

110
00:06:21.321 --> 00:06:25.730
after 500,000 miles or whatever goes out now, you know,

111
00:06:25.731 --> 00:06:30.110
do we have some fleet of technicians sort of continually cruising the highways

112
00:06:30.111 --> 00:06:32.480
and sort of servicing these things as they do,

113
00:06:32.481 --> 00:06:35.090
what pulled themselves off to the side of the road and say,

114
00:06:35.091 --> 00:06:37.760
I've got a sensor fault I'm pulling over, you know,

115
00:06:37.761 --> 00:06:40.970
or maybe there's some level of like critical safety, critical faults or, or,

116
00:06:40.980 --> 00:06:44.960
or whatever. Um, it might be. So, you know,

117
00:06:44.990 --> 00:06:49.850
that suggests that there might be a role for tele operation

118
00:06:49.880 --> 00:06:54.240
even with self-driving. And when I push people on it, in the,

119
00:06:54.380 --> 00:06:58.130
in the conversations, they all are like, yeah, we kind of have that on the,

120
00:06:58.131 --> 00:07:02.270
like bottom of the list, figure out how to rescue truck. You know,

121
00:07:02.271 --> 00:07:06.260
it gets looked on the to-do list, right after solving the self-driving,

122
00:07:06.290 --> 00:07:10.250
you know, question is like, yeah, what do we do with the problems? Right.

123
00:07:10.251 --> 00:07:13.610
I mean, now we could, we can imagine like, all right, we have some, you know,

124
00:07:13.640 --> 00:07:15.920
protocol that the truck is not, you know,

125
00:07:16.100 --> 00:07:21.050
realizes the system says not safe for operation pull to the side. Good.

126
00:07:21.051 --> 00:07:24.410
You have it crash, but now you got to trucks, ran it on the side of the road.

127
00:07:24.650 --> 00:07:28.340
You're going to send out somebody to like calibrate things and check out what's

128
00:07:28.341 --> 00:07:32.480
going on. Or that sounds like expensive labor. It sounds like downtime.

129
00:07:32.481 --> 00:07:35.600
It sounds like the kind of things that shippers don't like to happen to their

130
00:07:35.601 --> 00:07:38.420
freight, you know, in a, in a just-in-time world.

131
00:07:38.630 --> 00:07:41.780
And so wouldn't it be great if you could just sort of, you know,

132
00:07:41.930 --> 00:07:45.590
loop your way into the controls of that truck and say, all right,

133
00:07:45.591 --> 00:07:48.260
we've got a sensor out, says me that says that the tire's bad,

134
00:07:48.261 --> 00:07:50.900
but I can see visually from the camera looks fine.

135
00:07:50.960 --> 00:07:53.390
I'm going to drive it to our next Depot, you know,

136
00:07:53.391 --> 00:07:56.210
maybe the next Ryder or Penske location. Right.

137
00:07:56.330 --> 00:07:59.870
Sort of all these service locations around and have a technician technician take

138
00:07:59.871 --> 00:08:04.550
a look at it. So tele operation often gets this, you know, um,

139
00:08:04.670 --> 00:08:08.840
so dismissive, um, you know, commentary from,

140
00:08:08.870 --> 00:08:13.040
from other folks working on other other scenarios. But I think it's,

141
00:08:13.100 --> 00:08:17.450
it's potentially more relevant than, than, than we, we hear publicly.

142
00:08:17.840 --> 00:08:22.820
<v 0>A hard problem. And, uh, you know,</v>

143
00:08:22.850 --> 00:08:24.530
for me, uh,

144
00:08:24.531 --> 00:08:27.440
I've gotten a chance to interact with people that take on hard problems and

145
00:08:27.441 --> 00:08:32.000
solve them. And they're rare what Tesla has done with their data engine.

146
00:08:33.260 --> 00:08:37.940
So I thought autonomous driving cannot be solved without collecting a

147
00:08:37.941 --> 00:08:40.700
huge amount of data and organizing well, not just collecting,

148
00:08:40.701 --> 00:08:45.380
but organizing it and exactly what Tesla is doing now is what I

149
00:08:45.381 --> 00:08:48.200
thought it would be like. I couldn't see car companies doing that,

150
00:08:48.290 --> 00:08:52.100
including Tesla. And now that they're doing that, it's like, oh, okay.

151
00:08:52.310 --> 00:08:56.100
So it's possible to take on this huge effort seriously. To me,

152
00:08:56.101 --> 00:08:59.010
teleoperation is another huge effort like that.

153
00:08:59.550 --> 00:09:04.290
It's like taking seriously. What happens when it fails?

154
00:09:04.620 --> 00:09:09.390
What's the, in the case of Waymo for, um, for the consumer, like ride sharing,

155
00:09:09.690 --> 00:09:11.100
what's the customer experience.

156
00:09:11.101 --> 00:09:15.240
Like there's a bunch of videos online now where people are like the car

157
00:09:15.570 --> 00:09:19.260
fails and it pulls off to the side and you call a customer service.

158
00:09:19.260 --> 00:09:22.590
And you're basically sitting there for a long time and there's confusion.

159
00:09:22.800 --> 00:09:25.590
And then there's a rescue that comes and they start to drive. I mean,

160
00:09:25.591 --> 00:09:30.180
just the whole experience is a mess that has a ripple effect to how you

161
00:09:30.181 --> 00:09:32.790
trust in the, in the entirety of the experience,

162
00:09:33.060 --> 00:09:37.530
but like actually taking on the problem of that failure case and

163
00:09:37.590 --> 00:09:41.700
revolutionizing that experience both for trucking and for ride sharing.

164
00:09:42.060 --> 00:09:46.530
That's an amazing opportunity there because that feels like it would change

165
00:09:46.620 --> 00:09:51.270
everything. If you can reliably know when the failures happen, which they will,

166
00:09:51.480 --> 00:09:55.590
you have a clear plan that doesn't significantly affect the efficiency of the

167
00:09:55.591 --> 00:09:58.740
whole process. That that could be the game changer.

168
00:09:59.400 --> 00:10:01.860
And if teleoperation is part of that, it could be logistic.

169
00:10:01.861 --> 00:10:06.480
You're saying it could be teleoperation or it could be like a fleet of rescuers

170
00:10:06.660 --> 00:10:09.060
that can come in, which is a similar idea,

171
00:10:09.090 --> 00:10:12.420
but teleoperation obviously that allows you to,

172
00:10:12.460 --> 00:10:15.030
to just have a network of monitors,

173
00:10:15.120 --> 00:10:20.100
of people monitoring this giant fleet of trucks and taking over when needed.

174
00:10:20.430 --> 00:10:25.350
And it's a beautiful vision of the future where there's millions of robots

175
00:10:25.830 --> 00:10:29.820
and only thousands of humans monitoring those millions of robots.

176
00:10:30.390 --> 00:10:31.320
That seems like,

177
00:10:32.760 --> 00:10:37.350
that seems like a perfect dance of allowing humans to do what they do best and

178
00:10:37.351 --> 00:10:39.060
allowing robots to do what they do best.

179
00:10:39.600 --> 00:10:44.190
<v 1>Yeah. Yeah. So, I mean, I think there are, um, and we just applied for an NSF.</v>

180
00:10:44.191 --> 00:10:46.260
We didn't get anybody's watching,

181
00:10:47.760 --> 00:10:51.810
but with some folks from Wisconsin who do teleoperation right. And, and,

182
00:10:51.811 --> 00:10:54.990
you know, some of this is used for like rovers and, you know, I mean really,

183
00:10:55.260 --> 00:10:57.810
you know, high stakes, difficult problems,

184
00:10:58.110 --> 00:11:01.890
but one of the things we wanted to study where these mines in these Rio Tinto

185
00:11:01.891 --> 00:11:03.120
mines in Australia,

186
00:11:03.121 --> 00:11:07.860
where they remotely pilot the trucks and there there's some

187
00:11:07.861 --> 00:11:11.610
autonomy, I guess, and then, but it's overseen by, um, uh,

188
00:11:11.820 --> 00:11:14.700
a remote operator and they, you know, it's, uh, it's, uh,

189
00:11:14.760 --> 00:11:18.510
it's near Perth in it's, it's quite remote. And, um,

190
00:11:19.440 --> 00:11:23.130
they retrained the truck drivers to be the remote operators, right. Um,

191
00:11:23.460 --> 00:11:26.910
there's autonomy and in the port of Rotterdam and places like that,

192
00:11:26.940 --> 00:11:30.570
where there are jobs there. And so there, I think, you know,

193
00:11:30.630 --> 00:11:31.950
and maybe we'll get to this later, but you know,

194
00:11:31.951 --> 00:11:35.910
there's a real policy question about sort of who's going to lose and what we do

195
00:11:35.911 --> 00:11:39.240
about it. And, you know, whether or not there are opportunities there that,

196
00:11:39.270 --> 00:11:42.480
you know, maybe we need to put our thumb on the scale a little bit too,

197
00:11:42.670 --> 00:11:44.250
to make sure that, you know,

198
00:11:44.251 --> 00:11:48.540
there there's some give back to the community that's, that's taking the hit,

199
00:11:48.570 --> 00:11:52.770
you know? Um, so for instance, if there were tele operation centers, you know,

200
00:11:52.771 --> 00:11:56.890
maybe they go in these communities that we disproportionately source truck

201
00:11:56.891 --> 00:11:59.650
drivers from today now. I mean, what does that mean?

202
00:11:59.651 --> 00:12:02.740
It may not be the cheapest place to do it if they don't have great connectivity

203
00:12:02.830 --> 00:12:06.490
and it may not be where the upper level managers want to be at, you know,

204
00:12:06.491 --> 00:12:09.880
places like that, you know, issues like that. Right. So, um,

205
00:12:09.940 --> 00:12:12.520
I do think it's an interesting question, you know,

206
00:12:12.521 --> 00:12:16.030
both from sort of a practical, uh,

207
00:12:16.150 --> 00:12:18.430
scenario situation of how it's gonna work,

208
00:12:18.670 --> 00:12:21.220
but also from a policy perspective.

209
00:12:21.520 --> 00:12:25.600
<v 0>So there's platoons there's teleoperation, and this is, uh,</v>

210
00:12:25.630 --> 00:12:28.570
taking care of some of the highway driving that we're talking about.

211
00:12:28.840 --> 00:12:31.990
Is there other ideas, like, um,

212
00:12:32.650 --> 00:12:35.830
is there other ideas scenarios that you have for autonomous?

213
00:12:36.490 --> 00:12:40.930
<v 1>Yeah, so I mean the most obvious one actually is, is just, you know, uh,</v>

214
00:12:40.960 --> 00:12:45.250
facility to facility, right. This sort of, you know, um, it can't go everywhere,

215
00:12:45.280 --> 00:12:50.260
but a lot of logistics facilities are very close to interstates and they're,

216
00:12:50.261 --> 00:12:54.010
and they're on big commercial roads without, you know,

217
00:12:54.220 --> 00:12:56.080
bikes and parked cars and all that stuff.

218
00:12:56.560 --> 00:13:01.210
And some of the jobs that I think are really first on the chopping block are

219
00:13:01.211 --> 00:13:04.840
these LTL that less than truckload what's called line haul, right?

220
00:13:04.841 --> 00:13:08.320
So these are the drivers who go from terminal to terminal with those full

221
00:13:08.321 --> 00:13:09.910
trailers. Um,

222
00:13:10.000 --> 00:13:14.710
and those facilities are often located strategically to avoid congestion. Right.

223
00:13:15.010 --> 00:13:17.770
Um, and to be in big, you know, industrial facilities.

224
00:13:17.771 --> 00:13:21.100
So you could imagine that being, you know,

225
00:13:21.160 --> 00:13:25.930
the first place you see a Waymo self-driving, you know, truck rollout might be,

226
00:13:26.080 --> 00:13:27.040
you know, um,

227
00:13:27.100 --> 00:13:31.900
sort of direct facility to facility for ups or FedEx or

228
00:13:31.960 --> 00:13:33.490
less than truckload care. And then.

229
00:13:33.580 --> 00:13:37.210
<v 0>Yeah, there is fully driverless. So potentially not even a driver in the truck,</v>

230
00:13:37.780 --> 00:13:42.100
it's just going from facility to facility empty zero occupancy.

231
00:13:42.520 --> 00:13:45.610
<v 1>Yeah. And those, because that labor is expensive, uh, you know,</v>

232
00:13:45.611 --> 00:13:47.170
they don't keep those drivers out overnight.

233
00:13:47.171 --> 00:13:50.110
Those drivers do do a run back and forth typically, um,

234
00:13:50.130 --> 00:13:53.290
or in a team go back and forth in one day.

235
00:13:54.610 --> 00:13:58.240
<v 0>So from the people you've spoken with so far, what's your sense?</v>

236
00:13:58.270 --> 00:14:03.100
How far are we away from which scenarios closest and how far away are

237
00:14:03.101 --> 00:14:07.990
we from that scenario of autonomy being a big part

238
00:14:08.020 --> 00:14:08.890
of our trucking.

239
00:14:09.040 --> 00:14:13.210
<v 1>The most folks are focused on another scenario,</v>

240
00:14:13.300 --> 00:14:18.010
which is the exit exit, right. Um, which looks like that urban truck ports, um,

241
00:14:18.310 --> 00:14:20.380
thing that I laid out earlier, you know,

242
00:14:20.440 --> 00:14:24.520
so you have a human driven truck that comes out to, um, a drop lot.

243
00:14:24.580 --> 00:14:28.810
It meets up with an autonomous truck, the, that, that truck then, you know,

244
00:14:28.811 --> 00:14:33.220
drives it on the interstate to another lot. And then a human driver, um,

245
00:14:34.180 --> 00:14:38.650
you know, picks it up there, a couple variations maybe on that.

246
00:14:39.220 --> 00:14:43.930
Um, so let me just run it through the last two scenarios. Sure.

247
00:14:45.400 --> 00:14:49.270
The other thing you could do right, is to say, all right,

248
00:14:49.300 --> 00:14:51.380
I've got a truck that can drive itself. Um,

249
00:14:51.560 --> 00:14:55.460
and I refer to this one as autopilot, but, um, you know,

250
00:14:55.461 --> 00:14:57.560
you have a human drive it out to the interstate,

251
00:14:57.561 --> 00:15:00.680
but rather than have that transaction where,

252
00:15:00.740 --> 00:15:03.560
where the human driven truck detaches the trailer,

253
00:15:03.561 --> 00:15:06.860
and it gets coupled up to a self-driving truck, they just,

254
00:15:07.040 --> 00:15:11.480
that human driver just hops on the interstate with that truck and goes and back

255
00:15:11.600 --> 00:15:15.150
and goes off duty while the truck drives itself. And,

256
00:15:15.151 --> 00:15:19.190
and so you have a self-driving truck that's not driverless. Right. Um, and.

257
00:15:20.480 --> 00:15:23.120
<v 0>Because Tesla uses the term autopilot and so the airplanes,</v>

258
00:15:23.121 --> 00:15:25.010
and so everybody uses the word autopilot,

259
00:15:25.340 --> 00:15:29.750
we're referring to essentially full autonomy, but because it's exit exit,

260
00:15:29.870 --> 00:15:32.270
the truck driver is onboard the truck,

261
00:15:32.300 --> 00:15:33.920
but they're sleeping in the back or whatever.

262
00:15:34.430 --> 00:15:38.750
<v 1>Yeah. And this, this gets to the really weedy policy questions, right?</v>

263
00:15:38.751 --> 00:15:41.810
So basically for the department of transportation,

264
00:15:41.811 --> 00:15:44.000
for you to be off duty for safety reasons,

265
00:15:44.001 --> 00:15:46.430
you have to be completely relieved of all responsibility.

266
00:15:47.000 --> 00:15:49.580
So that truck has to not, you know,

267
00:15:49.610 --> 00:15:54.590
encounter a construction site or inclement weather or whatever it

268
00:15:54.591 --> 00:15:58.460
might be and, and call to you and say, Hey, you know, or, I mean,

269
00:15:58.461 --> 00:16:01.310
obviously right. We're imagining connected vehicles as well. Right?

270
00:16:01.311 --> 00:16:05.150
So you're in a self-driving truck, you're in the back and trucks,

271
00:16:05.151 --> 00:16:09.560
20 miles ahead experienced some problem, right. Um,

272
00:16:09.590 --> 00:16:12.140
that may require teleoperation or whatever it is. Right.

273
00:16:12.290 --> 00:16:16.430
And it signals to your truck, Hey, you know, tell the driver 20 miles ahead.

274
00:16:16.490 --> 00:16:20.030
He's, he's got to hop in the seat. That would mean that they're on duty.

275
00:16:20.031 --> 00:16:21.920
According to the way that the current rules are written,

276
00:16:22.040 --> 00:16:24.950
they have some responsibility. And, and part of that is, you know,

277
00:16:25.010 --> 00:16:27.360
we didn't need, we needed them get rest. Right. Um, they,

278
00:16:27.361 --> 00:16:29.930
they need to have an uninterrupted sleep.

