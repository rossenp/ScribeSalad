WEBVTT

1
00:00:02.930 --> 00:00:06.860
<v 0>What I'm also fascinated by is metrics that are different than engagement.</v>

2
00:00:07.600 --> 00:00:09.620
So the other thing, from an alien perspective,

3
00:00:10.370 --> 00:00:15.260
what social networks are doing is they, um, they,

4
00:00:15.440 --> 00:00:19.420
in the short term, bring out different aspects of each human being.

5
00:00:19.480 --> 00:00:22.300
So first let me say that

6
00:00:24.630 --> 00:00:29.610
an algorithm or a social network for each individual can bring out

7
00:00:30.190 --> 00:00:32.290
the best of that person or the worst of that person,

8
00:00:32.870 --> 00:00:37.170
or there's a bunch of different parts to us. Parts. We're proud of that we,

9
00:00:37.520 --> 00:00:41.810
that we are and parts we're not so proud of when we look at the big picture of

10
00:00:41.811 --> 00:00:44.360
our lives. When we look back 30 you days from now,

11
00:00:44.620 --> 00:00:46.520
am I proud that I said those things or not?

12
00:00:46.880 --> 00:00:48.680
&lt;laugh&gt; am I proud that I felt those things?

13
00:00:48.740 --> 00:00:53.600
Am I proud that I experienced or read those things or thought about

14
00:00:53.601 --> 00:00:56.880
those things just in that kind of self reflective kind of way.

15
00:00:57.660 --> 00:00:59.160
And so coupled with that,

16
00:00:59.720 --> 00:01:02.440
I wonder if it's possible to have different metrics that are not just about

17
00:01:02.441 --> 00:01:03.274
engagement,

18
00:01:03.460 --> 00:01:07.670
but are about long term happiness

19
00:01:08.490 --> 00:01:11.070
growth of a human being, where they look back and say,

20
00:01:11.830 --> 00:01:15.550
I am a better human being for having spent 100 hours on that app.

21
00:01:16.490 --> 00:01:20.790
And that feels like it's actually strongly correlated with engagement

22
00:01:21.410 --> 00:01:25.180
in the long term, in the short term, it may not be, but long term.

23
00:01:25.450 --> 00:01:29.500
It's like the same kind of thing where you really fall in love with a product

24
00:01:30.260 --> 00:01:31.500
mm-hmm, &lt;affirmative&gt; you fall in love with an iPhone.

25
00:01:31.501 --> 00:01:32.540
You fall in love with a car.

26
00:01:33.200 --> 00:01:37.660
That's what makes you fall in love is like really being proud

27
00:01:39.180 --> 00:01:41.020
and just in a self reflective way,

28
00:01:41.021 --> 00:01:44.260
understanding that you're better human being for having used the thing.

29
00:01:44.440 --> 00:01:47.970
And that's like where the that's what great relationships are made from.

30
00:01:48.320 --> 00:01:53.210
It's not just like you're hot and we like being together or

31
00:01:53.211 --> 00:01:53.800
something like that.

32
00:01:53.800 --> 00:01:56.450
It's more like I'm a better human being because I'm with you.

33
00:01:57.150 --> 00:02:01.290
And that feels like a metric that could be optimized for by the algorithms.

34
00:02:02.250 --> 00:02:06.040
Um, but you know, anytime I kind of talk about this with anybody,

35
00:02:06.510 --> 00:02:07.960
they seem to say, yeah, yeah, okay.

36
00:02:07.980 --> 00:02:12.880
That's going to get outcompeted immediately by the engagement, if it's a driven,

37
00:02:12.881 --> 00:02:17.440
especially I, I just don't think so. I don't, I mean, um,

38
00:02:18.360 --> 00:02:19.720
a lot of it's just implementation.

39
00:02:20.360 --> 00:02:25.280
<v 1>I'll say a couple things. One is to pull back the curtain on daily meetings</v>

40
00:02:26.710 --> 00:02:28.350
inside of these large social media companies,

41
00:02:30.030 --> 00:02:34.470
a lot of what management or, or at least the people that are, are tweaking.

42
00:02:34.471 --> 00:02:36.750
These algorithms spend their time on our trade offs.

43
00:02:37.410 --> 00:02:40.470
And there's these things called value functions, which are like, okay,

44
00:02:41.490 --> 00:02:44.830
we can predict the probability that you'll click on this thing,

45
00:02:44.930 --> 00:02:47.140
or the probability that you'll share it,

46
00:02:47.720 --> 00:02:50.580
or the probability that you will leave a comment on it,

47
00:02:50.640 --> 00:02:55.540
or the probability mm you'll. Dwell on it, individual actions. Right.

48
00:02:56.840 --> 00:03:00.720
And you've got this neural network that basically has a bunch of heads at the

49
00:03:00.721 --> 00:03:04.960
end and all of them are between zero and one and great. They all have values,

50
00:03:04.970 --> 00:03:08.400
right. Or, or, or they all have probabilities. Um,

51
00:03:09.260 --> 00:03:11.960
and then in these meetings, what they will do is say, well, uh,

52
00:03:12.740 --> 00:03:17.680
how much do we value a comment versus a click versus a share versus

53
00:03:18.040 --> 00:03:21.270
a, and that maybe even some downstream thing, right. Um,

54
00:03:21.271 --> 00:03:23.470
that has nothing to do with the item there,

55
00:03:23.570 --> 00:03:26.270
but like driving follows or something.

56
00:03:27.130 --> 00:03:29.910
And what typically happens is they will say, well,

57
00:03:29.911 --> 00:03:32.430
what are our goals for this quarter, at the company? Oh,

58
00:03:32.431 --> 00:03:35.030
we wanna drive sharing up. Okay, well, let's, uh,

59
00:03:35.031 --> 00:03:38.950
turn down these metrics and turn up these metrics and, and the,

60
00:03:39.260 --> 00:03:42.620
they blend them right into a single scaler with which they're trying to

61
00:03:42.820 --> 00:03:43.653
optimize.

62
00:03:44.780 --> 00:03:48.830
That is really hard because invariably you think you're solving for,

63
00:03:48.990 --> 00:03:51.550
I don't know something called meaningful interactions, right.

64
00:03:52.350 --> 00:03:53.950
And this was the big Facebook pivot. And I,

65
00:03:54.030 --> 00:03:57.710
I don't actually have any internal knowledge, like I wasn't in those meetings,

66
00:03:58.410 --> 00:03:59.470
but at least from,

67
00:03:59.970 --> 00:04:04.940
from what we've seen over the last month or so it seems by actually trying to,

68
00:04:04.941 --> 00:04:07.180
trying to optimize for meaningful interactions.

69
00:04:07.520 --> 00:04:11.300
It had all these side effects of optimizing for these other things. Um,

70
00:04:11.520 --> 00:04:13.980
and I don't claim to fully understand them.

71
00:04:13.981 --> 00:04:17.300
But what I will say is that trade offs abound.

72
00:04:18.240 --> 00:04:20.810
And as much as you'd like to solve for one thing,

73
00:04:21.190 --> 00:04:23.130
if you have a network of over a billion people,

74
00:04:24.070 --> 00:04:27.330
you're gonna have unintended consequences either way. And it gets really hard.

75
00:04:28.350 --> 00:04:31.570
So what you're describing is effectively a value model that says like,

76
00:04:31.830 --> 00:04:35.050
can we capture, this is the thing that I spent a lot of time thinking about.

77
00:04:35.051 --> 00:04:37.650
Like, can you capture utility

78
00:04:39.270 --> 00:04:42.440
in a way that like actually measure someone's happiness?

79
00:04:42.800 --> 00:04:46.160
Mm-hmm &lt;affirmative&gt; that, isn't just a, um, what do they call it?

80
00:04:46.161 --> 00:04:47.760
A surrogate problem where you say,

81
00:04:47.761 --> 00:04:52.280
well kind of think like the more you use the product, the happier you are,

82
00:04:52.390 --> 00:04:55.320
that was always the argument at Facebook, by the way, it was like, well,

83
00:04:55.860 --> 00:04:58.360
people use it more, so they must be more happy. Yeah.

84
00:04:58.570 --> 00:05:01.870
Turns out they're like a lot of things you use more that make you less happy in

85
00:05:01.871 --> 00:05:04.270
the world, not talking about Facebook, just, you know,

86
00:05:04.271 --> 00:05:08.270
let's think about whether it's gambling or whatever like that you can do more

87
00:05:08.271 --> 00:05:10.270
of, but doesn't necessarily make you happier.

88
00:05:10.271 --> 00:05:12.230
So the idea that time equals happiness.

89
00:05:12.300 --> 00:05:16.510
Obviously you can't map utility and time together easily. Mm-hmm &lt;affirmative&gt;,

90
00:05:16.511 --> 00:05:19.950
there are a lot of edge cases. So when you look around the world and you say,

91
00:05:19.951 --> 00:05:23.100
well, what are all the ways we can model utility? There's like one of the,

92
00:05:23.640 --> 00:05:25.660
please, if you know, someone's smart doing this,

93
00:05:25.661 --> 00:05:29.460
introduce me because I'm fascinated by it. And it seems really tough,

94
00:05:30.080 --> 00:05:33.980
but the idea that reinforcement learning, like everyone interesting.

95
00:05:34.180 --> 00:05:36.260
I know in machine learning, like I,

96
00:05:36.340 --> 00:05:39.340
I was really interested in recommend systems and supervised learning and,

97
00:05:40.760 --> 00:05:42.410
and the more I dug into it, I was like, oh,

98
00:05:42.520 --> 00:05:46.170
literally everyone smart is working on reinforcement learning. &lt;laugh&gt; like,

99
00:05:46.171 --> 00:05:49.730
literally everyone, you just made people at open AI and deep mind. Very happy.

100
00:05:49.830 --> 00:05:54.330
Yes. But I mean, but what's interesting is like, it's one thing to train a game.

101
00:05:54.870 --> 00:05:57.320
And like, I mean, that paper, that,

102
00:05:57.321 --> 00:06:01.800
where they just took Atari and they used a ComNet to basically just like train

103
00:06:01.801 --> 00:06:04.560
simple actions, mind blowing. Right. Mm-hmm,

104
00:06:04.640 --> 00:06:07.000
&lt;affirmative&gt; absolutely mind blowing, but it's a game. Great.

105
00:06:08.340 --> 00:06:13.240
So now what if you're constructing a feed for a person, right?

106
00:06:13.870 --> 00:06:18.470
Like how can you can construct that feed in such a way

107
00:06:18.980 --> 00:06:22.390
that optimizes for a diversity of experience, a, uh,

108
00:06:22.910 --> 00:06:27.750
a long term happiness, right. But that reward function,

109
00:06:27.850 --> 00:06:31.270
it turns out in, in reinforcement learning again, as I've learned,

110
00:06:31.860 --> 00:06:35.030
like reward design is really hard. Mm-hmm &lt;affirmative&gt; and

111
00:06:36.550 --> 00:06:37.301
I don't like,

112
00:06:37.301 --> 00:06:41.100
how do you design a scaler reward for someone's happiness over time? I mean,

113
00:06:41.101 --> 00:06:43.780
do you have to measure dopamine levels? &lt;laugh&gt; like, do you have to, well.

114
00:06:43.780 --> 00:06:45.340
<v 0>You have to have a lot of,</v>

115
00:06:46.140 --> 00:06:49.460
a lot more signals from the human being currently.

116
00:06:49.620 --> 00:06:53.500
It feels like there's not enough signals coming from the human being users of,

117
00:06:53.740 --> 00:06:55.220
uh, of this algorithm.

118
00:06:55.440 --> 00:06:59.090
So for reinforcement learning to work well needs to have a lot more.

119
00:06:59.090 --> 00:07:01.410
<v 1>Data needs to have a lot of data.</v>

120
00:07:01.411 --> 00:07:03.930
And that actually is a challenge for anyone who wants to start something,

121
00:07:03.931 --> 00:07:06.890
which is you don't have a lot of data. So how do you compete?

122
00:07:06.990 --> 00:07:11.010
But I do think back to your original point, rethinking the algorithm,

123
00:07:11.100 --> 00:07:14.610
rethinking reward functions, rethinking utility.

124
00:07:16.190 --> 00:07:20.800
That's fascinating. That's cool. And I think that's an, an open opportunity for,

125
00:07:21.600 --> 00:07:22.880
for a company that figures it out.

