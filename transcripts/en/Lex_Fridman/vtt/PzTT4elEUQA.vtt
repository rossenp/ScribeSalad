WEBVTT

1
00:00:03.010 --> 00:00:06.820
<v 0>When you look the field that I really love is artificial intelligence,</v>

2
00:00:07.080 --> 00:00:09.100
and there's not many projects.

3
00:00:09.170 --> 00:00:13.980
There's not many little flames of hope that have been carried out

4
00:00:14.320 --> 00:00:18.980
for many years for decades and psych represents one of them. And, uh,

5
00:00:19.860 --> 00:00:23.970
I mean that in itself is just a really inspiring thing.

6
00:00:24.590 --> 00:00:25.930
So I'm, I'm,

7
00:00:25.950 --> 00:00:29.130
I'm deeply grateful that you would be carrying that flame for so many years.

8
00:00:29.190 --> 00:00:32.210
And I think that's an inspiration to young people that said,

9
00:00:32.211 --> 00:00:36.130
you said life is finite. And we talked about mortality is a feature of AGI.

10
00:00:37.070 --> 00:00:40.050
Do you think about your own mortality? Are you afraid of death?

11
00:00:41.130 --> 00:00:45.760
<v 1>Um, sure. I'd be crazy if I weren't and, um,</v>

12
00:00:45.860 --> 00:00:50.840
as I get older, I'm now, um, over 70. So as I get older, um,

13
00:00:51.270 --> 00:00:52.720
it's more on my mind,

14
00:00:52.721 --> 00:00:57.480
especially as acquaintances and friends and especially, um,

15
00:00:57.481 --> 00:01:00.360
mentors, um, one by one are dying.

16
00:01:00.540 --> 00:01:05.470
So I can't avoid thinking about mortality. And I think that,

17
00:01:06.270 --> 00:01:06.810
um, the,

18
00:01:06.810 --> 00:01:11.310
the good news from the point of you and the rest of the world is that that adds

19
00:01:11.460 --> 00:01:16.150
impetus to, uh, my need to succeed in a small number of years in the future.

20
00:01:16.630 --> 00:01:18.950
&lt;laugh&gt; because I'm not, you have a deadline. Exactly.

21
00:01:19.090 --> 00:01:23.150
I'm not gonna have another 37 years to continue working on this.

22
00:01:23.170 --> 00:01:28.060
So we really do want to make an impact in the world. Um,

23
00:01:28.100 --> 00:01:32.060
commercially, physically, meta, physically, um,

24
00:01:32.160 --> 00:01:36.740
in the next small number of years, 2, 3, 5 years, not 2, 3,

25
00:01:36.740 --> 00:01:37.700
5 decades anymore.

26
00:01:38.160 --> 00:01:41.860
<v 2>And so this is really driving me toward, uh, this,</v>

27
00:01:42.170 --> 00:01:46.730
this sort of commercialization and in increasing increasingly

28
00:01:46.900 --> 00:01:51.250
widespread application of psych. Whereas before, um,

29
00:01:51.450 --> 00:01:54.210
I felt that I could just sort of sit back, roll my eyes,

30
00:01:54.520 --> 00:01:58.410
wait till the world caught up. And now I don't feel that way anyway, anymore.

31
00:01:58.490 --> 00:02:03.450
I feel like I need to put in some effort to make the world aware of what

32
00:02:03.451 --> 00:02:05.490
we have and what it can do. And,

33
00:02:05.800 --> 00:02:07.680
and the good news from your point of view is that that's,

34
00:02:07.820 --> 00:02:09.680
that's why I'm sitting here in, you could be.

35
00:02:09.680 --> 00:02:13.040
<v 0>More productive. &lt;laugh&gt;, uh, I love it.</v>

36
00:02:13.380 --> 00:02:17.600
And if I can help in any way, I would love to from a, from a, you know,

37
00:02:17.601 --> 00:02:21.160
from a programmer perspective, I, I love, uh,

38
00:02:21.161 --> 00:02:24.040
especially these days just contributing in small and big ways.

39
00:02:24.500 --> 00:02:28.910
So if there's any open source thing from an MIT side and the research, I,

40
00:02:29.030 --> 00:02:33.430
I would love to help, but when, you know, bigger than psych, like I said,

41
00:02:33.431 --> 00:02:36.550
it's that little flame that you're carrying of artificial intelligence,

42
00:02:36.610 --> 00:02:41.510
the big dream, um, is there, what do you hope your legacy is?

43
00:02:43.320 --> 00:02:44.600
<v 1>Hmm, that's a good question</v>

44
00:02:46.190 --> 00:02:50.230
that people will think of me as one of the

45
00:02:50.670 --> 00:02:53.110
pioneers or inventors of

46
00:02:55.170 --> 00:02:59.960
the AI that is ubiquitous and that they take for granted. Um,

47
00:03:00.020 --> 00:03:04.920
and so on much, much the way that today we look back on the,

48
00:03:05.400 --> 00:03:10.040
the pioneers of electricity or the pioneers of, um,

49
00:03:10.041 --> 00:03:14.800
similar types of, uh, technologies and so on as, um, you know,

50
00:03:14.801 --> 00:03:18.120
it's hard to imagine what life would be like if, uh,

51
00:03:18.121 --> 00:03:22.390
these people hadn't done what they, um, what they did. So that,

52
00:03:22.391 --> 00:03:26.670
that's one thing that I'd like to be remembered as another. Is that.

53
00:03:26.930 --> 00:03:31.110
<v 0>So the creator, one of the originators of this gigantic</v>

54
00:03:32.220 --> 00:03:37.030
knowledge store and acquisition system that is

55
00:03:37.050 --> 00:03:41.900
likely to be at the center of whatever this future AI thing

56
00:03:41.901 --> 00:03:43.060
will look like. Yes, exactly.

57
00:03:43.600 --> 00:03:46.140
<v 1>And I'd also like to be remembered as someone who</v>

58
00:03:47.800 --> 00:03:52.340
wasn't afraid to spend several decades

59
00:03:53.280 --> 00:03:57.900
on a project in a time when, um, all,

60
00:03:57.970 --> 00:04:01.810
when almost all of, of the other forces,

61
00:04:02.640 --> 00:04:07.530
institutional forces and commercial forces are

62
00:04:08.090 --> 00:04:10.410
incenting people to go for short term.

63
00:04:10.680 --> 00:04:11.513
<v 0>Rewards.</v>

64
00:04:11.630 --> 00:04:16.490
And a lot of people gave up a lot of people that dreamt the same dream as

65
00:04:16.510 --> 00:04:19.850
you gave up. Yes. And you didn't. Yes.

66
00:04:22.200 --> 00:04:26.440
I mean, uh, Doug it's, it's truly an honor. This was a long time coming. I, I,

67
00:04:26.640 --> 00:04:30.920
um, a lot of people bring up your work, uh,

68
00:04:30.960 --> 00:04:32.960
specifically and more broadly,

69
00:04:33.100 --> 00:04:36.760
philosophically of this is the dream of artificial intelligence.

70
00:04:37.070 --> 00:04:38.720
This is likely a part of the future.

71
00:04:39.680 --> 00:04:42.430
We're so sort of focused on machine learning applications,

72
00:04:42.450 --> 00:04:43.710
all that kind of stuff today.

73
00:04:43.730 --> 00:04:47.550
But it seems like the ideas that like carries forward, uh,

74
00:04:47.690 --> 00:04:51.070
is something that would be at the center of this problem.

75
00:04:51.470 --> 00:04:54.830
They we're all trying to solve, which is the, the problem of intelligence,

76
00:04:55.500 --> 00:04:59.590
emotional and, and, uh, otherwise. So thank you so much.

77
00:05:00.020 --> 00:05:01.420
It's such a hu huge you, John,

78
00:05:01.450 --> 00:05:04.740
that you would talk to me and spend your valuable time with me today.

79
00:05:04.741 --> 00:05:05.574
Thanks for talking.

80
00:05:05.640 --> 00:05:06.980
<v 1>Thanks. It's been great.</v>

