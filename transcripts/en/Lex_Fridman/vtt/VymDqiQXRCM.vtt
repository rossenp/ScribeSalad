WEBVTT

1
00:00:03.120 --> 00:00:05.730
<v 0>What do you think about the whistleblower Francis?</v>

2
00:00:06.180 --> 00:00:09.240
How going and recently coming out and saying that Facebook is aware of

3
00:00:09.241 --> 00:00:14.100
Instagram's harmful effect on teenage girls, um,

4
00:00:14.160 --> 00:00:17.460
as per their own internal research studies and the matter,

5
00:00:17.520 --> 00:00:22.440
what do you think about this baby of yours, Instagram being under fire now,

6
00:00:22.470 --> 00:00:26.100
as we've been talking about under the leadership of Facebook.

7
00:00:28.440 --> 00:00:29.970
<v 1>You know, I often question,</v>

8
00:00:31.350 --> 00:00:36.270
where does the blame lie is the blame at the people

9
00:00:36.990 --> 00:00:40.920
that originated the network me, right. Uh,

10
00:00:40.950 --> 00:00:45.210
is the blame at like the decision to combine the network with

11
00:00:46.020 --> 00:00:49.770
another network, with a certain set of values, um,

12
00:00:50.010 --> 00:00:54.900
is the blame at how it gets run after I left? Like,

13
00:00:55.060 --> 00:00:58.890
is, is it the driver or is it the car, right? Um,

14
00:01:00.030 --> 00:01:03.000
is it that someone enabled these devices in the first place,

15
00:01:03.360 --> 00:01:05.090
if you go to an extreme, right?

16
00:01:06.000 --> 00:01:10.320
<v 0>Or is it the users themselves just human nature? Is it,</v>

17
00:01:10.370 --> 00:01:11.820
is it just the way of human nature?

18
00:01:11.940 --> 00:01:12.241
<v 1>Sure.</v>

19
00:01:12.241 --> 00:01:15.810
And like the idea that we're going to find a mutually exclusive answer here is

20
00:01:15.811 --> 00:01:19.260
crazy. There's not one place, it's a combination of a lot of these things.

21
00:01:19.920 --> 00:01:22.500
And then the question is like, is it true at all? Right? Like,

22
00:01:22.770 --> 00:01:26.010
I'm not actually saying that's not true or that it's true,

23
00:01:26.520 --> 00:01:29.100
but there's always more nuance here.

24
00:01:29.730 --> 00:01:34.620
Do I believe that social media, um, has an effect on young people? Well,

25
00:01:34.621 --> 00:01:37.080
it's God or they use it a lot and I bet you,

26
00:01:37.200 --> 00:01:39.930
there are a lot of positive effects and I bet you, there are negative effects,

27
00:01:39.931 --> 00:01:44.910
just like any technology and where I've come to in my thinking on this is that

28
00:01:44.970 --> 00:01:49.650
I think any technology has negative side effects. The question is as a leader,

29
00:01:49.651 --> 00:01:50.700
what do you do about them?

30
00:01:50.760 --> 00:01:53.070
And are you actively working on them or do you just like,

31
00:01:53.100 --> 00:01:56.790
not really believe in them. If you're a leader that sits there and says, well,

32
00:01:56.791 --> 00:01:59.580
we're going to put an enormous amount of resources against this.

33
00:02:00.450 --> 00:02:04.350
We're going to acknowledge when there are true criticisms where we're going to

34
00:02:04.351 --> 00:02:07.860
be vulnerable and that we're not perfect and we're going to go fix them.

35
00:02:07.861 --> 00:02:10.050
And we're going to be held accountable along the way.

36
00:02:11.400 --> 00:02:14.850
I think that people generally really respect that. Um,

37
00:02:15.720 --> 00:02:17.610
but I think that where Facebook,

38
00:02:17.611 --> 00:02:20.550
I think has had issues in the past is where they say things like,

39
00:02:21.060 --> 00:02:24.360
I can't remember what mark said about misinformation during the election.

40
00:02:24.361 --> 00:02:26.130
There was that like famous quote where he's like,

41
00:02:26.850 --> 00:02:29.850
it's pretty crazy to think that Facebook had anything to do with this selection.

42
00:02:29.851 --> 00:02:31.290
Like that was something like that quote.

43
00:02:31.370 --> 00:02:36.300
And I don't remember what stage he was on. And, but Ooh, that did not age well.

44
00:02:36.360 --> 00:02:40.050
Right? Like you have to be willing to say, well,

45
00:02:40.051 --> 00:02:43.890
maybe there's there's, there's something there. And wow.

46
00:02:43.891 --> 00:02:47.070
Like I want to go look into it and truly believe it in your gut.

47
00:02:47.071 --> 00:02:51.210
But if people look at you and how you act and what you say and don't believe you

48
00:02:51.211 --> 00:02:52.050
truly feel that way.

49
00:02:52.870 --> 00:02:54.040
<v 0>It's not just the words you say,</v>

50
00:02:54.041 --> 00:02:58.240
but how you say them and that people believe they actually feel the pain of

51
00:02:58.241 --> 00:02:59.740
having caused any. So.

52
00:02:59.740 --> 00:03:00.940
<v 1>To me, it's, um,</v>

53
00:03:01.120 --> 00:03:06.040
it's much more about your actions and your posture post event than it

54
00:03:06.041 --> 00:03:10.030
is about debugging. The why? Cause I don't know, is it like,

55
00:03:10.031 --> 00:03:12.940
I don't know this research, it was written well after I left. Right? Like,

56
00:03:13.870 --> 00:03:18.220
is it the algorithm? Is it the explore page? Is it the people you might know,

57
00:03:18.221 --> 00:03:22.060
unit connecting you to, you know, ideas that are dangerous?

58
00:03:22.061 --> 00:03:24.760
Like I really don't know. Yeah. Um,

59
00:03:25.480 --> 00:03:28.180
so we'd have to have a much deeper, I think,

60
00:03:28.181 --> 00:03:30.370
dive to understand where the blame lies.

61
00:03:30.880 --> 00:03:34.240
<v 0>Well, it's very unpleasant to me to consider now. I don't know if this is true,</v>

62
00:03:34.241 --> 00:03:38.230
but to consider the very fact that there might be some

63
00:03:38.231 --> 00:03:42.190
complicated games being played here. For example, you know,

64
00:03:42.300 --> 00:03:47.140
as somebody I really love psychology and I love it enough to know

65
00:03:47.141 --> 00:03:49.840
that the field is pretty broken in the following way.

66
00:03:49.841 --> 00:03:53.800
It's very difficult to study human beings. Well at scale,

67
00:03:54.130 --> 00:03:57.310
because the questions you ask affect the results you can,

68
00:03:57.311 --> 00:03:59.020
you can basically get any results you want.

69
00:03:59.350 --> 00:04:03.490
And so you have an internal Facebook study that asks some question of which we

70
00:04:03.491 --> 00:04:06.400
don't know the full details and there's some kind of analysis,

71
00:04:06.730 --> 00:04:10.960
but that's just the one little tiny slice into, uh, some much bigger picture.

72
00:04:11.230 --> 00:04:14.860
And so you can have, um, thousands of employees at Facebook,

73
00:04:15.070 --> 00:04:18.280
one of them comes out and picks whatever narrative,

74
00:04:18.490 --> 00:04:20.590
knowing that they become famous.

75
00:04:20.600 --> 00:04:24.340
Couple of the other really uncomfortable thing I see in the world,

76
00:04:24.580 --> 00:04:27.460
which is journalists seem to understand,

77
00:04:27.461 --> 00:04:31.540
they get a lot of clickbait attention from saying something negative about

78
00:04:31.541 --> 00:04:35.170
social networks, certain companies like they even get some,

79
00:04:35.950 --> 00:04:40.320
some clickbait stuff about Tesla or about it,

80
00:04:40.321 --> 00:04:44.950
especially when it's like when there's a public famous CEO type of person.

81
00:04:45.220 --> 00:04:48.280
If they get a lot of use on the negative, not the positive,

82
00:04:48.670 --> 00:04:51.910
the positive they'll get immunity actually goes to the thing you were saying

83
00:04:51.911 --> 00:04:56.320
before. If there's a hot, sexy, new product, that's great to look forward to.

84
00:04:56.321 --> 00:05:00.730
They get positive on that, but absent a product it's nice to have,

85
00:05:01.780 --> 00:05:04.060
uh, like the seal messing up with some kind of way.

86
00:05:04.450 --> 00:05:09.430
And so couple that with the whistleblower and with the, uh,

87
00:05:09.790 --> 00:05:13.510
this whole dynamic of journalism and so on, you know, uh,

88
00:05:13.511 --> 00:05:17.860
with social dilemma being really popular documentary, it's like, all right.

89
00:05:18.790 --> 00:05:23.500
My concern is there's deep flaws in human nature here in terms of

90
00:05:23.501 --> 00:05:25.210
things we need to deal with,

91
00:05:25.480 --> 00:05:29.650
like the nature of hate of bullying, all those kinds of things.

92
00:05:30.490 --> 00:05:35.260
And then there's people who are trying to use that potentially to become

93
00:05:35.261 --> 00:05:38.190
famous and make money off of, uh,

94
00:05:38.620 --> 00:05:42.040
off of blaming others for causing more of the problem as opposed to helping

95
00:05:42.041 --> 00:05:45.940
solve the problem. So I don't know what to think. I'm not saying this is like,

96
00:05:45.970 --> 00:05:47.920
I'm just uncomfortable with, I guess,

97
00:05:48.040 --> 00:05:51.850
not knowing what to think about any of this, because a bunch of folks,

98
00:05:51.851 --> 00:05:55.660
I know that work at Facebook, uh, on the machine learning side. So Yan Lacoon,

99
00:05:55.690 --> 00:05:55.961
I mean,

100
00:05:55.961 --> 00:06:00.530
they they're quite upset by what's happening because there's a lot of really

101
00:06:00.531 --> 00:06:04.880
brilliant, good people inside Facebook they're trying to do good.

102
00:06:04.881 --> 00:06:08.450
And so like all of this press prescient is one of them and he has an amazing

103
00:06:08.451 --> 00:06:09.920
team of machine learning researchers.

104
00:06:09.921 --> 00:06:14.900
Like he's really upset with the fact that people don't seem to understand that

105
00:06:14.901 --> 00:06:15.980
this, this is not,

106
00:06:15.981 --> 00:06:20.000
the portrayal does not represent the full nature of efforts is going on on

107
00:06:20.001 --> 00:06:22.610
Facebook. So I don't know what to think about that. Well, okay.

108
00:06:22.670 --> 00:06:23.503
<v 1>You just,</v>

109
00:06:23.630 --> 00:06:28.550
I think very helpfully explain the nuances of the situation of why

110
00:06:28.551 --> 00:06:32.310
it's so hard to understand, but a couple things. One is, um,

111
00:06:35.150 --> 00:06:39.410
I think I have been surprised at the

112
00:06:39.620 --> 00:06:44.540
scale with which some product

113
00:06:44.541 --> 00:06:49.100
manager can do an enormous amount of harm to a very,

114
00:06:49.101 --> 00:06:53.760
very large company by releasing a trove of documents. Like I,

115
00:06:53.761 --> 00:06:56.990
I think I read a couple of them when they got published and I haven't even spent

116
00:06:56.991 --> 00:06:59.000
any time going deep part of it's like,

117
00:06:59.120 --> 00:07:02.720
I don't really feel like reliving a previous life, but, um,

118
00:07:03.890 --> 00:07:04.723
wow.

119
00:07:04.970 --> 00:07:09.680
Like talk about challenging the idea of open culture and like what that does

120
00:07:09.681 --> 00:07:14.000
to Facebook internally, if Facebook was built, like, I remember, um,

121
00:07:14.990 --> 00:07:19.850
like my office, uh, we had this like no visitors rule around my office.

122
00:07:20.030 --> 00:07:24.200
Cause we always had like confidential stuff up on the walls and never was super

123
00:07:24.201 --> 00:07:25.040
angry. Cause they were like,

124
00:07:25.041 --> 00:07:28.910
that goes against our culture of transparency and like marks and the fish cube

125
00:07:28.911 --> 00:07:32.060
or whatever they call it, the aquarium, I think they called it, um,

126
00:07:32.270 --> 00:07:36.260
where like literally anyone could see what he was doing at any point. And um,

127
00:07:36.890 --> 00:07:37.760
and I don't know. I mean,

128
00:07:37.761 --> 00:07:42.650
other companies like apple have been quiet slash locked down snapshots the same

129
00:07:42.651 --> 00:07:44.930
way for a reason. And

130
00:07:46.670 --> 00:07:49.700
I don't know what this does to transparency on the inside of startups.

131
00:07:49.701 --> 00:07:54.620
That value that I think that it's, it's a Seminole moment and you can say, well,

132
00:07:54.621 --> 00:07:57.410
you should have nothing to hide. Right. But to your point,

133
00:07:57.411 --> 00:08:01.700
you can pick out documents that show anything. Right. Um,

134
00:08:01.820 --> 00:08:02.421
but I don't know.

135
00:08:02.421 --> 00:08:06.560
So what happens to transparency inside of startups and the culture that

136
00:08:07.430 --> 00:08:08.550
we'll have that,

137
00:08:08.551 --> 00:08:11.870
that startups or companies in the future will grow like the startup of the

138
00:08:11.871 --> 00:08:13.400
future that becomes the next Facebook,

139
00:08:13.401 --> 00:08:16.460
what will it be locked down and what does that do? Right.

140
00:08:16.760 --> 00:08:19.100
So that's part one part two.

141
00:08:20.480 --> 00:08:24.650
Like I don't think that you could design a more like

142
00:08:25.160 --> 00:08:30.050
a well orchestrated handful of events from the

143
00:08:30.051 --> 00:08:33.650
like 16 minutes to, um,

144
00:08:33.890 --> 00:08:37.010
releasing the documents in the way that they were released at the right time.

145
00:08:37.640 --> 00:08:40.670
Uh, that takes a lot of planning and partnership.

146
00:08:40.700 --> 00:08:45.470
And it seems like she has a partner at some firm, right. That probably, uh,

147
00:08:45.500 --> 00:08:50.480
helped a lot with this. But man, on a, as at a personal level, if you're her,

148
00:08:50.481 --> 00:08:54.500
you'd have to really believe in what you are doing

149
00:08:55.080 --> 00:08:59.340
really believe in it because you are personally putting your on the line. Right.

150
00:08:59.341 --> 00:09:04.170
Like you've got a very large company that doesn't

151
00:09:04.200 --> 00:09:09.120
like enemies. Right. Um, it takes a lot of guts.

152
00:09:10.260 --> 00:09:13.980
Uh, and I don't love these conspiracy theories about like, oh,

153
00:09:13.981 --> 00:09:16.710
she's being financed from some person or people. Like,

154
00:09:16.711 --> 00:09:20.780
I don't love them because that's like the easy thing to say. I think the, the,

155
00:09:20.781 --> 00:09:24.960
the outcomes raiser here is like someone thought they were doing something wrong

156
00:09:25.530 --> 00:09:29.320
and was like very, very courageous. And,

157
00:09:29.880 --> 00:09:32.250
and I don't know if courageous is the word, but like,

158
00:09:32.460 --> 00:09:37.170
so without getting into like, is she a martyr? Is she courageous as sheet?

159
00:09:37.171 --> 00:09:39.360
Right. Like, let's put that aside for a second. Yeah.

160
00:09:40.050 --> 00:09:44.670
Then there are the documents themselves. They say what they say to your point.

161
00:09:45.090 --> 00:09:48.720
A lot of the things that like people have been worried about already in the

162
00:09:48.721 --> 00:09:53.010
documents are, or they're already been said externally. And, um,

163
00:09:53.670 --> 00:09:55.710
I don't know. I'm just like, I'm,

164
00:09:55.830 --> 00:09:58.560
I'm thankful that I am focused on new things with my life.

165
00:10:00.000 --> 00:10:00.990
<v 0>Well, let me just say,</v>

166
00:10:00.991 --> 00:10:05.100
I just think it's a really hard problem that probably Facebook and Twitter are

167
00:10:05.101 --> 00:10:08.490
trying to solve. I'm actually just fascinated by how hard this.

168
00:10:09.510 --> 00:10:14.430
<v 1>There are fundamental issues at Facebook in tone and in an approach of</v>

169
00:10:14.431 --> 00:10:18.570
how product gets built in the objective functions. And, um,

170
00:10:19.080 --> 00:10:23.880
and since people, uh, organizations are not people, so Yon and fair,

171
00:10:23.881 --> 00:10:24.181
right?

172
00:10:24.181 --> 00:10:27.540
Like there are a lot of really great people who like literally just want to push

173
00:10:27.570 --> 00:10:29.010
reinforcement learning forward.

174
00:10:29.400 --> 00:10:33.600
They literally just want to teach a robot to touch, feel lift, right?

175
00:10:33.601 --> 00:10:37.560
Like they're not thinking about political misinformation, right? Yeah.

176
00:10:38.070 --> 00:10:38.680
But there's a,

177
00:10:38.680 --> 00:10:43.530
there's a strong connection between what funds that research and an enormously

178
00:10:43.531 --> 00:10:48.270
profitable machine, uh, that has trade-offs and,

179
00:10:48.840 --> 00:10:52.410
uh, one cannot one cannot separate the two.

180
00:10:52.411 --> 00:10:55.080
You are not completely separate from the system.

181
00:10:55.590 --> 00:10:57.480
So I agree.

182
00:10:57.481 --> 00:11:01.260
It can feel really frustrating to feel if you're internally internal there,

183
00:11:01.410 --> 00:11:03.780
that you're working on something completely unrelated.

184
00:11:03.810 --> 00:11:06.840
And you feel like your group's good. I can understand that.

185
00:11:07.290 --> 00:11:09.360
<v 0>There are some responsibilities still. You have to acknowledge.</v>

186
00:11:09.390 --> 00:11:10.650
It's like the Ray Daleo thing.

187
00:11:10.651 --> 00:11:13.740
You have to look in the mirror and see if there's problems and you have to fix

188
00:11:13.741 --> 00:11:14.790
those problems. Yeah.

