WEBVTT

1
00:00:03.090 --> 00:00:06.210
<v 0>I think it's called the grace call processor. Mm-hmm &lt;affirmative&gt;, uh,</v>

2
00:00:06.410 --> 00:00:08.730
introduced last year. It's, uh, you know,

3
00:00:08.731 --> 00:00:11.010
there's a bunch of measures of performance. We're talking about horses.

4
00:00:11.970 --> 00:00:16.090
Mm-hmm &lt;affirmative&gt; it seems to outperform 368 trillion operations per second,

5
00:00:16.091 --> 00:00:17.770
seems to outperform in videos,

6
00:00:17.930 --> 00:00:20.370
Tesla T four system mm-hmm &lt;affirmative&gt; so these are just numbers.

7
00:00:21.010 --> 00:00:24.120
Mm-hmm &lt;affirmative&gt; what do they actually mean? And real world perform? Like,

8
00:00:24.121 --> 00:00:28.640
what are the metrics for you that you're chasing in, in your horse race?

9
00:00:28.641 --> 00:00:29.680
Like what do you care about?

10
00:00:30.030 --> 00:00:34.240
<v 1>Well, first the, so the, the native language of, you know,</v>

11
00:00:34.241 --> 00:00:38.760
people who write AI network programs is PI torch. Now PI torch TensorFlow.

12
00:00:38.761 --> 00:00:40.480
There's a couple others, deep.

13
00:00:40.680 --> 00:00:42.360
<v 0>PI torch is one over TensorFlow, which is.</v>

14
00:00:42.360 --> 00:00:44.710
<v 1>Just a, I'm not an expert. Okay. I,</v>

15
00:00:44.790 --> 00:00:47.870
I know many people who have switched from TensorFlow to pie torch. Yeah.

16
00:00:47.930 --> 00:00:50.830
And there's technical reasons for it. And I use both.

17
00:00:51.020 --> 00:00:51.950
<v 0>Both are still awesome.</v>

18
00:00:52.140 --> 00:00:52.830
<v 1>Both are still.</v>

19
00:00:52.830 --> 00:00:55.670
<v 0>Awesome. But the deepest love is for pie torch.</v>

20
00:00:55.670 --> 00:00:58.630
<v 1>Currently. Yeah. There there's more love for that. And that, that may change.</v>

21
00:00:58.890 --> 00:01:01.910
So the first thing is when they write their programs,

22
00:01:02.930 --> 00:01:06.460
can the hardware execute it pretty much as it was written mm-hmm &lt;affirmative&gt;

23
00:01:06.670 --> 00:01:08.980
right. So by torch turns into a graph,

24
00:01:09.600 --> 00:01:11.660
we have a graph compiler that makes that graph.

25
00:01:11.810 --> 00:01:15.140
Then it fractions the graph down. So if you have big matrix multiply,

26
00:01:15.141 --> 00:01:17.900
we turn it into right size chunks that are run on the processing elements.

27
00:01:18.400 --> 00:01:20.540
It hooks all the graph up. It lays out all the data.

28
00:01:21.210 --> 00:01:24.300
There's a couple mid-level representations of it.

29
00:01:24.301 --> 00:01:28.410
That also simulative so that if you are writing the code,

30
00:01:28.470 --> 00:01:32.170
you can see how it's gonna go through the machine, which is pretty cool.

31
00:01:32.171 --> 00:01:35.930
And then, then at the bottom, it schedules kernels like math data,

32
00:01:35.931 --> 00:01:38.970
manipulation data movement, kernels, which do this stuff.

33
00:01:39.150 --> 00:01:43.610
So we don't have to run, write a little program to do matrix multiplier.

34
00:01:43.810 --> 00:01:47.320
Cause we have a big matrix multiplier. Like there's no CDI program for that,

35
00:01:48.620 --> 00:01:52.890
but that there is scheduling for that. Right? So the, the,

36
00:01:52.950 --> 00:01:56.450
one of the goals is if you write a piece of pie, torch code,

37
00:01:56.451 --> 00:01:58.410
that looks pretty reasonable, you should be able to compile it,

38
00:01:58.610 --> 00:02:01.210
running it on the hardware without having to tweak it and,

39
00:02:01.850 --> 00:02:03.930
and do all kinds of crazy things to get performance.

40
00:02:04.200 --> 00:02:07.200
<v 0>There's not a lot of intermediate steps, right? It's running directly.</v>

41
00:02:07.220 --> 00:02:10.880
<v 1>As written like on a GPU. If you write a large matrix, multiplied naively,</v>

42
00:02:10.881 --> 00:02:15.200
you'll get five to 10% of the peak performance of the GPU, right?

43
00:02:15.201 --> 00:02:17.880
And then there's a bunch, there's a bunch of people published papers on this.

44
00:02:17.881 --> 00:02:20.280
And I read them about what steps do you have to do?

45
00:02:20.300 --> 00:02:24.600
And it goes from pretty reasonable, well transpose one of the matrices.

46
00:02:24.700 --> 00:02:27.470
So you do RO or not call 'em ordered, you know,

47
00:02:28.000 --> 00:02:32.990
block it so that you can put a block of the matrix on different SMS, you know,

48
00:02:32.991 --> 00:02:37.470
groups of threads. But some of it gets into little, little details.

49
00:02:37.471 --> 00:02:40.750
Like you have to schedule it just so, so you don't have registered conflicts.

50
00:02:41.250 --> 00:02:44.270
So the, the, the, the, they call 'em cutin ninjas

51
00:02:46.270 --> 00:02:48.620
S I love it to get to the optimal point.

52
00:02:48.621 --> 00:02:52.060
You either write a pre use a prewritten library,

53
00:02:52.270 --> 00:02:54.060
which is a good strategy for some things,

54
00:02:54.320 --> 00:02:58.700
or you have to be an expert in micro architecture to program it, right?

55
00:02:58.760 --> 00:03:01.840
So optimiz step is way more complicated with the GP. So our,

56
00:03:02.240 --> 00:03:05.600
our goal is if you write pie torch, that's good pie torch. You can do it.

57
00:03:05.940 --> 00:03:09.440
Now there's, as the networks are evolving, you know,

58
00:03:09.441 --> 00:03:12.120
they've changed from convolutional to matrix, multiply.

59
00:03:12.580 --> 00:03:14.360
People are talking about conditional graphs.

60
00:03:14.361 --> 00:03:17.160
They're talking about very large matrices. They're talking about Sparky.

61
00:03:17.710 --> 00:03:21.630
They're talking out problems that scale across many, many chips.

62
00:03:22.530 --> 00:03:26.710
So the, the native, you know, data item is a,

63
00:03:26.910 --> 00:03:30.470
is a packet like, so you send a packet to a processor, it gets processed.

64
00:03:30.810 --> 00:03:31.710
It does a bunch of work.

65
00:03:31.711 --> 00:03:34.310
And then it may send packets to other processors and they,

66
00:03:34.370 --> 00:03:38.670
and they execute and like a data flow graph kind of methodology. Got it.

67
00:03:39.040 --> 00:03:41.780
We have a big network on chip and then 16,

68
00:03:42.220 --> 00:03:45.380
next second chip has 16 ethernet ports to hook lots of them together.

69
00:03:45.840 --> 00:03:48.380
And it's the same graph com compiler across multiple chips.

70
00:03:48.680 --> 00:03:51.340
So that's where the scale comes in. So it's built to scale naturally.

71
00:03:51.600 --> 00:03:54.380
Now my experience with scaling is, as you scale,

72
00:03:54.400 --> 00:03:58.180
you run into lots of interesting problems. Yes. So scaling is a mountain,

73
00:03:58.340 --> 00:04:01.290
the climb. Yeah. So the hardware is built to do this.

74
00:04:01.310 --> 00:04:05.650
And then we're in the process of, is there a software part to this with,

75
00:04:06.050 --> 00:04:10.570
with ethernet and all that? Well, the, you know, the protocol at the bottom,

76
00:04:10.830 --> 00:04:13.210
you know, we send, you know, it's an ethernet fi,

77
00:04:13.950 --> 00:04:17.610
but the protocol basically says, send the packet from here to there.

78
00:04:17.611 --> 00:04:21.360
It's all point to point mm-hmm &lt;affirmative&gt; the header bit says which processor

79
00:04:21.430 --> 00:04:25.560
send it to. And we basically take a packet off our, on chip network,

80
00:04:25.860 --> 00:04:28.680
put an ether Hader on it, send it to the other end,

81
00:04:29.640 --> 00:04:32.040
S strip the header off and send it the local thing. It's pretty straightforward.

82
00:04:32.410 --> 00:04:34.480
Human human interaction is pretty straightforward too,

83
00:04:34.481 --> 00:04:37.880
but we can get a million of us. We could, we do some crazy stuff together. Yeah.

84
00:04:38.080 --> 00:04:38.360
Can be fun.

