WEBVTT

1
00:00:03.010 --> 00:00:07.110
<v 0>Do you think, uh, for the, for the neocortex in general, do you,</v>

2
00:00:07.310 --> 00:00:10.790
do you think there's a lot of innovation to be done on the machine side?

3
00:00:11.370 --> 00:00:14.630
You know, you use the computer as a metaphor quite a bit.

4
00:00:14.690 --> 00:00:17.990
Is there different types of computer that would help us build? I mean,

5
00:00:17.991 --> 00:00:18.350
what are the.

6
00:00:18.350 --> 00:00:21.350
<v 1>Intelligence physical manifestations of intelligent machines? Yeah. Or is it,</v>

7
00:00:21.630 --> 00:00:25.020
oh, oh no, it's gonna be totally crazy. Uh,

8
00:00:25.520 --> 00:00:29.220
we have no idea how this is gonna look out yet. Uh, you can already see this,

9
00:00:30.140 --> 00:00:34.580
um, today we, of course remodeled these things on traditional computers and now,

10
00:00:34.960 --> 00:00:39.260
now GPS are really popular with, with, uh, you know, neural networks and so on.

11
00:00:39.940 --> 00:00:43.420
Um, but there are companies coming up with fundamentally new physical,

12
00:00:43.500 --> 00:00:46.850
so substrates, um, that are just really cool.

13
00:00:47.250 --> 00:00:48.850
Mm-hmm &lt;affirmative&gt; I don't think they're gonna work or not. Um,

14
00:00:49.630 --> 00:00:53.610
but I think there'll be decades of innovation here. Yeah, totally.

15
00:00:53.950 --> 00:00:56.570
<v 0>Do you think the final thing will be messy?</v>

16
00:00:56.720 --> 00:01:01.130
Like our biology is messy or do you think, um, it's it's the,

17
00:01:01.200 --> 00:01:06.200
it's the old bird versus airplane question. Do you think we could just, um,

18
00:01:07.290 --> 00:01:10.360
build airplanes? Yeah. &lt;laugh&gt; that,

19
00:01:10.920 --> 00:01:15.040
that fly way better than birds in the same way we can build, uh, uh,

20
00:01:16.360 --> 00:01:17.100
electrical new.

21
00:01:17.100 --> 00:01:20.640
<v 1>Cortex. Yeah. You know, can I, can I, can I refund the bird thing a bit?</v>

22
00:01:20.880 --> 00:01:23.680
Cuz I think it's interesting people really misunderstand this,

23
00:01:24.140 --> 00:01:26.150
the rip for others. Um,

24
00:01:27.410 --> 00:01:30.030
the problem they were trying to solve was controlled flight.

25
00:01:30.170 --> 00:01:33.270
How to turn an airplane, not how to propel an airplane.

26
00:01:33.620 --> 00:01:35.910
They weren't worried about that. Interesting. Yeah, they already had,

27
00:01:36.010 --> 00:01:39.510
at that time there was already wing shapes, which they had from studying birds.

28
00:01:39.640 --> 00:01:41.470
There was already gliders that carry people.

29
00:01:41.890 --> 00:01:44.550
The problem was if you put a rudder on the back of a glider and you're turn it,

30
00:01:44.650 --> 00:01:49.020
the plane falls out of the sky. So the problem was how do you control flight?

31
00:01:49.680 --> 00:01:53.580
And they studied birds and they actually had birds in captivity.

32
00:01:53.581 --> 00:01:54.900
They watched birds in wind tunnels.

33
00:01:54.970 --> 00:01:58.620
They observed in the wild and they discovered the secret was the birds twist

34
00:01:58.621 --> 00:02:01.700
their wings when they turned. And so that's what they did on the rip,

35
00:02:01.820 --> 00:02:04.620
by the flyer. They had these sticks that you would twist the wing. And that was,

36
00:02:04.890 --> 00:02:07.170
that was their innovation, not the propellor.

37
00:02:07.670 --> 00:02:10.130
And today airplanes still twist their wings.

38
00:02:10.150 --> 00:02:13.850
We don't twist the entire w we just twist the tail end of it. The, the,

39
00:02:14.210 --> 00:02:15.610
the flaps, which is the same thing.

40
00:02:15.770 --> 00:02:18.730
So today's airplanes fly on the same principles as birds,

41
00:02:18.731 --> 00:02:20.930
which is observed by so everyone get that analogy wrong,

42
00:02:21.550 --> 00:02:23.130
but let's step back from that. Right?

43
00:02:23.520 --> 00:02:26.760
Once you understand the principle as a flight,

44
00:02:26.860 --> 00:02:29.640
you can choose how to implement 'em. Yeah, no,

45
00:02:29.641 --> 00:02:32.760
one's gonna use bones and feathers and muscles. Um,

46
00:02:32.980 --> 00:02:36.720
but they do have wings and, uh, we don't FLA 'em we have propellers.

47
00:02:36.721 --> 00:02:38.640
So when we have the principles of,

48
00:02:39.520 --> 00:02:42.560
of computation that goes onto modeling the world in a brain mm-hmm &lt;affirmative&gt;

49
00:02:43.060 --> 00:02:44.320
we understand those principles, right?

50
00:02:44.321 --> 00:02:48.750
Clearly we have choices out how to end 'em and some of 'em be biological like,

51
00:02:48.751 --> 00:02:51.430
and some won't. And, um,

52
00:02:52.050 --> 00:02:54.430
but I do think there's gonna be a huge amount of innovation here.

53
00:02:55.150 --> 00:02:57.110
You think about the innovation. When in the computer, they had to invent the,

54
00:02:57.430 --> 00:03:02.400
the, the transistor invented the Silicon ship. They had to invent,

55
00:03:02.420 --> 00:03:05.360
you know, then just software. I mean, Gazi of things,

56
00:03:05.480 --> 00:03:08.960
they had to do memory systems, um, we're gonna do, it's gonna be similar.

57
00:03:09.510 --> 00:03:12.320
<v 0>Well, it's interesting that the deep learning, um,</v>

58
00:03:13.180 --> 00:03:17.160
the effectiveness of deep learning for specific tasks is driving a lot of

59
00:03:17.161 --> 00:03:21.470
innovation in the hardware we, which may have effects for, uh,

60
00:03:21.750 --> 00:03:25.830
actually allowing us to discover intelligence systems that operate very

61
00:03:25.831 --> 00:03:28.150
differently. Yeah. Or at least much bigger than deep learning. Yeah.

62
00:03:28.350 --> 00:03:28.610
Interesting.

63
00:03:28.610 --> 00:03:33.430
So ultimately it's good to have an application that's making our life better

64
00:03:33.650 --> 00:03:38.430
now because the, the, the capitalist process, if you can make money. Yeah.

65
00:03:38.910 --> 00:03:41.100
Yeah. That works. I mean, the other way, I mean,

66
00:03:41.780 --> 00:03:45.220
Neil degra Tyson writes about this is the other way we fund science of course,

67
00:03:45.420 --> 00:03:48.540
is through military. So like, yeah. Conquest.

68
00:03:48.680 --> 00:03:51.100
<v 1>So here, here's an interesting thing we're doing on this regard.</v>

69
00:03:51.600 --> 00:03:54.860
So we've decided we, we, we used to have a series, these biological principles,

70
00:03:55.140 --> 00:03:56.620
and we can see how to build these intelligent machines,

71
00:03:57.240 --> 00:03:59.460
but we've decided to apply some of,

72
00:03:59.530 --> 00:04:01.850
of these principles to today's machine learning techniques.

73
00:04:01.851 --> 00:04:04.770
Mm-hmm &lt;affirmative&gt;. So, uh, one of the, we didn't talk about this principle.

74
00:04:04.771 --> 00:04:07.330
One is, uh, a sparsity in the brain. Um,

75
00:04:07.520 --> 00:04:09.290
most of the neurons are inactive at any point in time,

76
00:04:09.430 --> 00:04:11.450
sparse and the connectivity to sparse and that's different,

77
00:04:11.570 --> 00:04:13.370
the deep learning networks. Um,

78
00:04:13.870 --> 00:04:17.570
so we've already shown that we can speed up existing, deep learning networks,

79
00:04:18.330 --> 00:04:22.560
uh, anywhere from 10 to a factor of our hundred, I mean, literally a hundred,

80
00:04:23.520 --> 00:04:27.160
um, and make it more robust at the same time. So this is commercially very,

81
00:04:27.161 --> 00:04:30.920
very valuable. Um, and so, you know,

82
00:04:31.060 --> 00:04:33.440
if we can prove this actually in, in the,

83
00:04:33.441 --> 00:04:35.720
the largest systems that are commercially applied today,

84
00:04:35.870 --> 00:04:39.280
there's a big commercial desire to do this. Well.

85
00:04:39.720 --> 00:04:44.510
Sparsity is something that doesn't run really well on existing

86
00:04:44.990 --> 00:04:49.110
hardware. It doesn't really run really well on, on, um, GPU, um,

87
00:04:49.690 --> 00:04:54.190
and on CPUs. And so that would be a way of sort of bringing more,

88
00:04:54.750 --> 00:04:57.150
a more brain principles into the existing system on a,

89
00:04:57.151 --> 00:04:58.670
on a commercially valuable basis.

90
00:04:59.100 --> 00:05:02.580
Another thing we can think we can do is we're gonna use the, these D rights, um,

91
00:05:02.760 --> 00:05:06.220
models of we, uh, I talked earlier about the,

92
00:05:06.620 --> 00:05:08.820
the prediction occurring inside in their own mm-hmm &lt;affirmative&gt; that,

93
00:05:08.821 --> 00:05:13.260
that basic property can be applied to existing their own networks and allow them

94
00:05:13.261 --> 00:05:15.740
to learn continuously which something they don't do today. Mm-hmm &lt;affirmative&gt;.

95
00:05:16.040 --> 00:05:19.020
<v 0>And so the, the DRI spikes that you were talking about. Yeah.</v>

96
00:05:19.020 --> 00:05:20.500
<v 1>Well, we wouldn't model the spikes,</v>

97
00:05:20.501 --> 00:05:24.370
but the idea that you have that neuro today's neural networks have this company

98
00:05:24.371 --> 00:05:28.250
called the point neuron, which is a very simple model of the neuron and, uh,

99
00:05:28.470 --> 00:05:31.370
by adding D rights to 'em at this one more level of complexity,

100
00:05:31.910 --> 00:05:36.330
that's in BI systems, you can solve problems in continuous learning, um,

101
00:05:36.870 --> 00:05:41.810
and rapid learning. So we're trying to take, we're trying to bring existing, uh,

102
00:05:42.360 --> 00:05:44.080
field and we'll see if we can do it.

103
00:05:44.081 --> 00:05:48.160
We're trying to bring the existing field of machine learning, uh, commercially,

104
00:05:48.161 --> 00:05:50.960
along with us, you brought up this idea of keeping, you know, paying for it.

105
00:05:51.320 --> 00:05:52.480
Mm-hmm, &lt;affirmative&gt; commercially, along with us,

106
00:05:52.481 --> 00:05:55.040
as we move towards the ultimate goal of a true AI system.

107
00:05:55.670 --> 00:05:59.520
<v 0>Even small innovations on your own networks are really, really exciting. Yeah.</v>

108
00:06:00.080 --> 00:06:04.360
Cause it seems like such a trivial model of the brain &lt;laugh&gt; and,

109
00:06:04.600 --> 00:06:09.120
and applying different insights that just even, like you said, continuous,

110
00:06:09.800 --> 00:06:11.640
uh, learning, or, uh,

111
00:06:12.180 --> 00:06:16.790
making it more asynchronous or maybe making more dynamic make,

112
00:06:17.610 --> 00:06:19.430
or like, uh, incentivizing.

113
00:06:20.230 --> 00:06:21.670
<v 1>Making it even just one more robust.</v>

114
00:06:21.950 --> 00:06:22.830
<v 0>Robust, robust, uh,</v>

115
00:06:22.970 --> 00:06:27.910
and making it somehow much better incentivizing Sparky,

116
00:06:28.590 --> 00:06:29.220
uh, somehow.

117
00:06:29.220 --> 00:06:30.470
<v 1>Yeah. Uh, well,</v>

118
00:06:30.471 --> 00:06:33.790
if you can make things a hundred times faster than there's plenty of in &lt;laugh&gt;

119
00:06:33.890 --> 00:06:37.460
that's true people, people are spending millions of dollars, you know,

120
00:06:37.461 --> 00:06:39.580
just training some of these networks. Now these, uh,

121
00:06:39.940 --> 00:06:40.860
these transforming networks.

