WEBVTT

1
00:00:03.030 --> 00:00:06.690
<v 0>The challenges this world presents will create.</v>

2
00:00:06.691 --> 00:00:09.000
Divisions will create chaos and so on.

3
00:00:09.001 --> 00:00:13.590
So I'm more focused on the way we deal as a society with that chaos,

4
00:00:13.620 --> 00:00:16.800
the way we talk to each other, that's huge. That's writing the platform.

5
00:00:16.801 --> 00:00:17.634
That's healthy for that.

6
00:00:17.660 --> 00:00:21.620
<v 1>Now, as a, as a comedian creator, whatever you want to call it,</v>

7
00:00:21.621 --> 00:00:24.650
people that put out content, uh,

8
00:00:24.680 --> 00:00:27.170
the gatekeepers are now algorithmic, right?

9
00:00:27.171 --> 00:00:31.910
So they are kind of almost AI already. So if you are a person that puts out,

10
00:00:31.940 --> 00:00:36.890
you know, YouTube videos, podcasts, uh, whatever you're doing, um,

11
00:00:36.920 --> 00:00:41.510
you are, it used to be a guy in the back of the room with a cigar saying,

12
00:00:41.511 --> 00:00:46.310
I like you, or get him out of here now. It's, it's an algorithm.

13
00:00:46.311 --> 00:00:49.040
You barely understand. Like I talk, I've talked to people at YouTube,

14
00:00:49.120 --> 00:00:52.580
but I don't know if they understand the algorithm. They don't, they don't.

15
00:00:52.760 --> 00:00:55.490
And that's fascinating. Yeah, it's fascinating. Cause I,

16
00:00:55.491 --> 00:00:57.800
I speak to people at YouTube and I go, Hey man, what's going on here?

17
00:00:58.040 --> 00:01:02.180
One of my episode, titles of my podcast was called knife fight in Malibu.

18
00:01:02.510 --> 00:01:06.830
It was about real estate. And, and it was because a realtor in Malibu,

19
00:01:06.860 --> 00:01:09.170
I was trying to get a summer rental, which I can't really afford,

20
00:01:09.171 --> 00:01:12.890
but I don't think that's a huge problem. I, I, you know, I follow my dreams.

21
00:01:13.460 --> 00:01:16.070
So I called a realtor and she said, listen, she goes,

22
00:01:16.071 --> 00:01:18.200
I don't know what the government's saying, but she goes,

23
00:01:18.320 --> 00:01:20.450
it's a real knife fight out here. You know, an old grizzled woman,

24
00:01:20.451 --> 00:01:24.350
real realtor can scan, seek out the mouth, driving a Porsche. You know,

25
00:01:24.620 --> 00:01:26.300
it's a real knife fight out here. You know,

26
00:01:26.301 --> 00:01:28.280
her entire life had become real estate.

27
00:01:28.490 --> 00:01:31.700
Her soul had been hollowed out or container, you know,

28
00:01:31.730 --> 00:01:34.910
no one's made her common years, but it's just, you just loves heated, fun,

29
00:01:35.840 --> 00:01:39.560
fuse fun. She's a demon from hell. And we need them truly.

30
00:01:39.770 --> 00:01:41.570
We're getting rid of them. It's not good.

31
00:01:42.140 --> 00:01:45.800
And she goes to a real knife and out here. So we put that in the episode title.

32
00:01:46.040 --> 00:01:46.581
And of course,

33
00:01:46.581 --> 00:01:50.030
I guess some algorithm thought we were showing like people stabbing each other

34
00:01:50.120 --> 00:01:54.710
in a Wendy's. And we, we got like demonetized, did we get the amount of dyes?

35
00:01:56.880 --> 00:01:59.630
We lost a lot of views because we were kicked out of whatever out,

36
00:01:59.631 --> 00:02:02.070
like we were just kicked out. Yeah. And then I was asking YouTube about it.

37
00:02:02.260 --> 00:02:03.590
They were kind of understanding. And,

38
00:02:03.680 --> 00:02:07.250
but even the people that worked there didn't truly seem to understand the

39
00:02:07.251 --> 00:02:09.740
algorithm. So can you explain to me how that works with,

40
00:02:09.770 --> 00:02:10.760
they barely know what's going.

41
00:02:10.760 --> 00:02:14.670
<v 0>On? No, they do not understand the full dynamics of the,</v>

42
00:02:14.671 --> 00:02:17.390
the monster or the amazing thing that they've created.

43
00:02:17.510 --> 00:02:21.470
It's the amount of content that's being created is larger than anyone

44
00:02:21.471 --> 00:02:24.440
understands, right? Like this is huge. They can't deal with it.

45
00:02:24.441 --> 00:02:28.760
The teams aren't large enough to deal with it. There's like special cases.

46
00:02:29.090 --> 00:02:31.550
So if you fall into the category, a special case,

47
00:02:31.551 --> 00:02:34.910
so we can maybe talk about that. Like a Donald Trump, where you like,

48
00:02:35.000 --> 00:02:38.030
actually have meetings about what to do with this particular account.

49
00:02:38.210 --> 00:02:40.580
But everything outside of that is all algorithms.

50
00:02:40.820 --> 00:02:44.660
They get reported by people and they get, uh,

51
00:02:44.690 --> 00:02:48.350
they give enough people to report a particular video or a particular tweet.

52
00:02:48.740 --> 00:02:52.250
Again, it rises up to where humans look over it.

53
00:02:52.520 --> 00:02:53.870
But the,

54
00:02:53.950 --> 00:02:58.610
the re the initial step of the reporting and the

55
00:02:59.290 --> 00:03:02.500
rising up to the human supervision is done by algorithm.

56
00:03:02.500 --> 00:03:04.060
And they don't understand the dynamics of that.

57
00:03:04.420 --> 00:03:06.190
Cause we're talking about billions of tweets.

58
00:03:06.460 --> 00:03:11.380
We're talking about hundreds of thousands of hours

59
00:03:11.410 --> 00:03:13.840
of video uploaded every day.

60
00:03:14.230 --> 00:03:18.160
Now the hilarity of it is that

61
00:03:18.610 --> 00:03:23.050
most of the YouTube algorithm is based on the title.

62
00:03:24.340 --> 00:03:25.173
That's crazy.

63
00:03:25.390 --> 00:03:29.020
And the description is a small contribution in terms of filtering in terms of

64
00:03:29.021 --> 00:03:33.070
the knife fight situation, right. And that's all they can do. They cannot,

65
00:03:33.100 --> 00:03:37.720
they don't have algorithms at all that are able to process the content of the

66
00:03:37.721 --> 00:03:38.440
video.

67
00:03:38.440 --> 00:03:43.090
So they try to also infer information based on if you're

68
00:03:43.091 --> 00:03:46.000
watching all of these Q and on videos or something like that,

69
00:03:46.300 --> 00:03:47.560
or flat earth videos.

70
00:03:48.250 --> 00:03:53.200
And you also watch are really excitedly watching the whole night fight in

71
00:03:53.201 --> 00:03:56.290
a Malibu video that says,

72
00:03:56.890 --> 00:04:01.470
that increases the chance that the knife fight, uh, is, uh, uh,

73
00:04:01.540 --> 00:04:04.930
a dangerous video for society or something like that. Interesting. Wow.

74
00:04:04.960 --> 00:04:06.070
Based on their contribution.

75
00:04:06.630 --> 00:04:08.040
<v 1>Watching something,</v>

76
00:04:08.280 --> 00:04:12.390
cause I watched Q Anon and flat earth videos to ridicule them. Right. That,

77
00:04:12.391 --> 00:04:15.660
you know what I mean? I watch these videos and I make fun of them on my show.

78
00:04:16.080 --> 00:04:20.010
But what's interesting is if I then go watch something else I'm increasing the

79
00:04:20.011 --> 00:04:23.400
likelihood that that video is going to get looked at as potentially subverted or

80
00:04:23.401 --> 00:04:23.910
dangerous.

81
00:04:23.910 --> 00:04:27.480
<v 0>Exactly. That's why. So they make decisions about who you are,</v>

82
00:04:28.200 --> 00:04:30.990
like who you are as a human being, as a watcher divisional user,

83
00:04:30.991 --> 00:04:33.690
based on the clusters of videos you're in.

84
00:04:33.900 --> 00:04:36.720
But those clusters are not manually determined. They're,

85
00:04:37.290 --> 00:04:39.390
they're automatically clustered has.

86
00:04:40.560 --> 00:04:45.120
<v 1>Uh, we have titles where they got upset about it. I don't even understand. Yeah.</v>

87
00:04:45.150 --> 00:04:48.420
Like we had a title that was so innocuous in my opinion,

88
00:04:48.810 --> 00:04:51.360
and the title of the episode was called bomb Disney world.

89
00:04:52.110 --> 00:04:57.000
And I was asking people to consider bombing Disney world

90
00:04:57.420 --> 00:05:02.100
and YouTube got angry at that. So you don't know why you can never.

91
00:05:02.940 --> 00:05:04.560
<v 0>I said, Disney world is the bomb.</v>

92
00:05:07.290 --> 00:05:10.380
<v 1>Rearranging. Probably man. I was saying, let's start. Yeah.</v>

93
00:05:10.530 --> 00:05:15.390
Thinking about their plans to do like not let's do it

94
00:05:15.391 --> 00:05:19.620
like, but let's, let's get in the mind. Let's change the conversation. Yeah.

95
00:05:19.800 --> 00:05:21.840
I think it's very interesting because as a comedian,

96
00:05:21.841 --> 00:05:23.970
you don't want to live in that world of worrying about algorithms.

97
00:05:23.971 --> 00:05:26.100
You don't want to worry about de platforming and shadow banning. I mean,

98
00:05:26.101 --> 00:05:29.070
all of these conversations that I've had with other comedians about shadow

99
00:05:29.071 --> 00:05:31.110
banning, I mean, it's hilarious. We all call each other.

100
00:05:31.111 --> 00:05:33.990
I think I'm being shadow banned. Are you being shadow bad?

101
00:05:34.230 --> 00:05:37.650
And nobody knew what that word was a month, I mean, a year ago,

102
00:05:37.651 --> 00:05:41.430
but everyone now is convinced in everything they do that isn't succeeding as

103
00:05:41.431 --> 00:05:42.264
being shadowed.

104
00:05:42.930 --> 00:05:47.760
So it's this new paranoia there's algorithmic paranoia.

105
00:05:47.850 --> 00:05:49.290
And now that we all kind of have,

106
00:05:49.291 --> 00:05:54.150
because there are genuine instances of people being taken

107
00:05:54.151 --> 00:05:57.410
out of it, algorithm, you know, rightly or wrongly for whatever,

108
00:05:57.430 --> 00:05:58.310
however you want to believe.

109
00:05:58.670 --> 00:06:01.790
But then there are also things that just don't perform as well for a myriad of

110
00:06:01.791 --> 00:06:06.110
reasons. And, and then we're all saying like, well, they're against me,

111
00:06:06.590 --> 00:06:10.460
they're shutting me down. And you don't know if that's true or not, you know,

