WEBVTT

1
00:00:02.970 --> 00:00:06.630
<v 0>You are now the CTO of</v>

2
00:00:07.100 --> 00:00:11.820
a 10 store. And two as of two months ago, they,

3
00:00:12.150 --> 00:00:16.530
uh, build hardware for deep learning. Um,

4
00:00:16.680 --> 00:00:18.990
how do you build scalable and efficient, deep learning?

5
00:00:19.020 --> 00:00:20.370
This is such a fascinating space.

6
00:00:20.780 --> 00:00:23.660
<v 1>So it's interesting. So, um, up until recently,</v>

7
00:00:23.661 --> 00:00:25.160
I thought there was two kinds of computers.

8
00:00:25.220 --> 00:00:29.060
There are serial computers that run like C programs and then there's parallel

9
00:00:29.061 --> 00:00:32.180
computers. So the way I think about it, as you know,

10
00:00:32.210 --> 00:00:34.640
parallel computers have given parallelism,

11
00:00:34.790 --> 00:00:38.930
like GPS are great because you have a million pixels and modern GPU is running a

12
00:00:38.931 --> 00:00:42.650
program on every pixel. They call it a shader program, right? So,

13
00:00:43.280 --> 00:00:46.790
or like finite element analysis, you've built something, you know,

14
00:00:46.791 --> 00:00:49.940
you make this into little tiny chunks, you give each chunk to a computer.

15
00:00:50.000 --> 00:00:52.460
So you're given all these trunks of parallels like that.

16
00:00:53.030 --> 00:00:54.170
But most of the programs,

17
00:00:54.260 --> 00:00:59.000
you write this linear narrative and you have to make a go fast to make a go

18
00:00:59.001 --> 00:01:01.520
fast. You predict all the branches, all the data fetches,

19
00:01:01.521 --> 00:01:06.290
and you run that more parallel, but that's found parallelism. Um,

20
00:01:07.100 --> 00:01:07.933
AI is,

21
00:01:08.780 --> 00:01:12.620
I'm still trying to decide how fundamental it is is it's a given parallel is in

22
00:01:12.621 --> 00:01:13.454
problem,

23
00:01:13.760 --> 00:01:18.650
but the way people described the neural networks and then how they write

24
00:01:18.651 --> 00:01:19.850
them and pie torch, it makes graph.

25
00:01:20.750 --> 00:01:24.380
<v 0>Yeah. That might be fundamentally different than the GPU kind of.</v>

26
00:01:24.500 --> 00:01:26.960
<v 1>Parallelism yet. It might be because the,</v>

27
00:01:27.380 --> 00:01:32.000
when you run the GPU program on all the pixels you're running, but you know,

28
00:01:32.001 --> 00:01:35.360
it depends, you know, this group of pixels say it's background blue.

29
00:01:35.361 --> 00:01:38.510
And that runs a really simple program. Just pixel is, you know,

30
00:01:38.511 --> 00:01:39.770
some patch of your face.

31
00:01:39.771 --> 00:01:43.340
So you have some really interesting shader program to give you the impression of

32
00:01:43.341 --> 00:01:46.730
translucency, but the pixels themselves don't talk to each other.

33
00:01:47.030 --> 00:01:49.250
There's no graph, right?

34
00:01:49.460 --> 00:01:53.540
So you do the image and then you do the next image, the next image,

35
00:01:54.140 --> 00:01:58.460
and you run 8 million pixels, 8 million programs every time.

36
00:01:58.461 --> 00:02:01.820
And modern GPU's have like 6,000 thread engines in them.

37
00:02:02.480 --> 00:02:04.940
So they got 8 million pixels.

38
00:02:04.970 --> 00:02:09.710
Each one runs a program on 10 or 20 pixels. And that's how,

39
00:02:10.220 --> 00:02:11.600
that's how they work, but there's no graph.

40
00:02:12.200 --> 00:02:17.180
<v 0>But you think graph might be a totally new way to think about</v>

41
00:02:17.181 --> 00:02:17.630
hardware.

42
00:02:17.630 --> 00:02:21.950
<v 1>So Roger could Dora and I have been having this conversation about giving versus</v>

43
00:02:22.220 --> 00:02:26.690
fountain parallelism, and then the kind of walk as we got more transistors,

44
00:02:26.691 --> 00:02:28.310
like computers, way back,

45
00:02:28.311 --> 00:02:31.910
when did stuff on scalar data than we did on vector data,

46
00:02:31.911 --> 00:02:36.230
famous vector machines. Now we're making computers that operate on matrices,

47
00:02:37.070 --> 00:02:41.390
right? And then the, the CA the category we, we said that was next with spatial.

48
00:02:41.750 --> 00:02:44.540
Like, imagine you have so much data that, you know,

49
00:02:44.541 --> 00:02:48.440
you want to do the compute on this data. And then when it's done, it says,

50
00:02:48.770 --> 00:02:51.620
send the result of this pile of data, run some software on that.

51
00:02:52.130 --> 00:02:56.780
And it's better to think about it spatially then to

52
00:02:56.781 --> 00:03:00.700
move all the data to a central processor, do all the work. So.

53
00:03:00.910 --> 00:03:04.870
<v 0>Especially, I mean, moving in the space of data as opposed to move in the data.</v>

54
00:03:04.960 --> 00:03:05.280
Yeah.

55
00:03:05.280 --> 00:03:07.830
<v 1>Yeah. You have a, give a petabyte data space,</v>

56
00:03:08.220 --> 00:03:10.920
spread across some huge array of computers.

57
00:03:11.490 --> 00:03:13.410
And when you do a computation somewhere,

58
00:03:13.411 --> 00:03:17.220
you send the result of that computation, or maybe a pointer to the next program,

59
00:03:17.280 --> 00:03:21.570
to some other piece of data and do it. But I think a better word might be graph.

60
00:03:21.660 --> 00:03:24.210
And all the AI neural networks are graphs.

61
00:03:24.540 --> 00:03:28.050
Do some computations on the result here, do another computation,

62
00:03:28.080 --> 00:03:32.460
do a data transformation, do emerging, do a pooling to another computation.

63
00:03:33.220 --> 00:03:37.380
<v 0>Is it possible to compress and say how we make this thing efficient,</v>

64
00:03:37.410 --> 00:03:39.600
this whole process efficient there's different.

65
00:03:40.380 --> 00:03:44.700
<v 1>First, uh, the fundamental elements in the graphs are things like metrics,</v>

66
00:03:44.701 --> 00:03:48.690
multiplies, convolutions data manipulations and data movements. Yeah.

67
00:03:49.020 --> 00:03:52.950
So GPS emulate those things with their little singles, you know,

68
00:03:53.070 --> 00:03:57.270
basically running a single-threaded program. Yeah. And then there's a, you know,

69
00:03:57.271 --> 00:03:58.051
an Nvidia calls at,

70
00:03:58.051 --> 00:04:00.960
or where they grew up a bunch of programs that are similar together.

71
00:04:01.470 --> 00:04:06.420
So for efficiency and instruction use, and then the higher level, you kind of,

72
00:04:06.840 --> 00:04:10.080
you take this graph and you say it as part of the graph is a matrix multiplier

73
00:04:10.190 --> 00:04:12.030
was runs on these three 30 views reds.

74
00:04:12.660 --> 00:04:17.370
But the model at the bottom was built for running

75
00:04:17.371 --> 00:04:20.060
programs on pixels, not executing graphs.

76
00:04:20.280 --> 00:04:21.960
<v 0>It's emulation, ultimately.</v>

77
00:04:22.290 --> 00:04:25.350
So is it possible to build something that natively runs great?

78
00:04:25.830 --> 00:04:29.820
<v 1>Yes. So, so it tends torrented. So yeah.</v>

79
00:04:29.880 --> 00:04:33.570
<v 0>Where are we on that? How, like, in the history of that effort,</v>

80
00:04:33.780 --> 00:04:35.070
are we in the early days? Yeah.

81
00:04:35.640 --> 00:04:39.900
<v 1>I think so 10 store it's started by a friend of mine LaBeija by Jack and I,</v>

82
00:04:40.090 --> 00:04:41.820
I was his first investor.

83
00:04:41.880 --> 00:04:46.350
So I've been kind of following him and talking to him about it for years.

84
00:04:46.470 --> 00:04:51.390
And in the fall when I was considering things to do, I decided,

85
00:04:51.391 --> 00:04:52.950
you know, the, the, uh, we, we,

86
00:04:53.390 --> 00:04:57.360
we held a conference last year with a friend organized it and, and we,

87
00:04:57.420 --> 00:05:02.310
we wanted to bring in thinkers and two of the people were Andre capacity and

88
00:05:02.311 --> 00:05:07.020
Chris Lattner and Andre gave this talk it's on YouTube called software

89
00:05:07.021 --> 00:05:09.030
2.0, which I think is great,

90
00:05:09.750 --> 00:05:12.960
which is where we went from programs, computers,

91
00:05:13.050 --> 00:05:16.980
where you write programs to data program computers, you know,

92
00:05:16.981 --> 00:05:21.750
like the futures of software as data programs, the networks.

93
00:05:22.230 --> 00:05:26.820
And I think that's true. And then, um, Chris has been work.

94
00:05:26.830 --> 00:05:29.490
He worked on LLVM the low level virtual machine,

95
00:05:29.491 --> 00:05:33.330
which became the intermediate representation for all compilers.

96
00:05:34.230 --> 00:05:36.510
And now he's working on another project called MLR,

97
00:05:36.511 --> 00:05:39.300
which is mid-level intermediate representation,

98
00:05:39.330 --> 00:05:43.740
which is essentially under the graph about how do you

99
00:05:43.741 --> 00:05:48.120
represent a kind of computation and then coordinate large numbers of potentially

100
00:05:48.121 --> 00:05:51.540
heterogeneous computers. And I would say,

101
00:05:51.570 --> 00:05:55.740
technically tends torrents, you know, two pillars of those,

102
00:05:56.250 --> 00:06:00.530
those, those two ideas software 2.0 and mid-level representation,

103
00:06:01.160 --> 00:06:03.860
but it's in service of executing graft programs.

104
00:06:04.730 --> 00:06:08.270
The hardware is designed to do that. So that's including the hardware piece.

105
00:06:09.350 --> 00:06:12.950
And then the other cool thing is for a relatively small amount of money,

106
00:06:12.951 --> 00:06:15.440
they did a test chip and two production chips.

107
00:06:16.220 --> 00:06:18.650
So it's like a super effective team. And,

108
00:06:19.510 --> 00:06:23.600
and unlike some AI startups where if you don't build the hardware to run the

109
00:06:23.601 --> 00:06:25.310
software that they really want to do,

110
00:06:25.760 --> 00:06:28.190
then you have to fix it by writing lots more software.

111
00:06:28.910 --> 00:06:32.840
So the hardware naturally does matrix multiply, convolution,

112
00:06:32.870 --> 00:06:37.700
the data manipulations and the data movement between processing

113
00:06:37.701 --> 00:06:40.280
elements that, that you can see in the graph,

114
00:06:41.660 --> 00:06:45.960
which I think is all pretty clever. And that's,

115
00:06:46.310 --> 00:06:47.630
that's what I'm working on now.

