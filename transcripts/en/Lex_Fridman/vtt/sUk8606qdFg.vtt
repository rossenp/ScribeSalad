WEBVTT

1
00:00:03.260 --> 00:00:07.100
<v 0>It's at least possible that, uh, the fear of death,</v>

2
00:00:07.520 --> 00:00:12.340
the terror of your mortality is the creative force that created

3
00:00:12.600 --> 00:00:16.860
all of the things around us at this human civilization. Yeah. Well, and,

4
00:00:17.060 --> 00:00:18.940
and I think about from an engineering perspective,

5
00:00:20.330 --> 00:00:22.820
this is where I lose all of my robotics.

6
00:00:23.110 --> 00:00:27.530
<v 2>Uh, colleagues is, I feel like if you want to create intelligence,</v>

7
00:00:27.710 --> 00:00:31.850
you have to also engineer in some kind of

8
00:00:32.870 --> 00:00:36.650
echoes of this kind of fear of, um, and not, you know,

9
00:00:36.651 --> 00:00:41.050
fear is such a complicated word, but kind of a, like a scarcity,

10
00:00:42.090 --> 00:00:46.760
uh, a scarcity of time of scarcity of resources that creates a kind of

11
00:00:46.870 --> 00:00:50.360
anxiety, like deadlines, get you to do stuff. Yeah.

12
00:00:50.660 --> 00:00:54.480
And there's something almost fundamental to that in terms of, uh,

13
00:00:55.250 --> 00:00:56.720
human experience. Yeah.

14
00:00:56.720 --> 00:00:57.680
<v 1>Well, that's an interesting thought.</v>

15
00:00:57.700 --> 00:01:01.760
So you're basically in order to create

16
00:01:03.040 --> 00:01:03.873
a kind of

17
00:01:05.590 --> 00:01:09.230
structure that mirrors what we call consciousness. Yes.

18
00:01:09.450 --> 00:01:14.070
You better have that structure confront the same kinds of

19
00:01:14.530 --> 00:01:16.790
issues and terrors that, that, that we.

20
00:01:16.790 --> 00:01:20.350
<v 0>Do self consciousness and suffering only makes sense in the context of death,</v>

21
00:01:20.650 --> 00:01:25.060
if you want to, I feel like if you want to fit into humans as society,

22
00:01:26.060 --> 00:01:29.060
if you, if you're a robot and if you want to fit into human society,

23
00:01:29.600 --> 00:01:34.460
you better have the same kind of existential dread the

24
00:01:34.461 --> 00:01:37.500
same kind of fear of mortality. Otherwise you're not gonna fit in. Right.

25
00:01:37.820 --> 00:01:38.500
&lt;laugh&gt;.

26
00:01:38.500 --> 00:01:39.333
<v 1>&lt;Laugh&gt;.</v>

27
00:01:41.100 --> 00:01:42.820
<v 0>That's good. It might be, it might be wild,</v>

28
00:01:43.080 --> 00:01:47.850
but it at least like we're to talking about all the theories that are at least

29
00:01:47.851 --> 00:01:50.330
worth consideration, I think that's a really powerful one.

30
00:01:50.830 --> 00:01:55.170
And definitely one has, uh, resonated with me and, um,

31
00:01:56.220 --> 00:01:58.210
definitely seems to capture something

32
00:02:00.800 --> 00:02:05.450
beautifully, like real about the human condition.

33
00:02:05.720 --> 00:02:06.880
&lt;affirmative&gt; and I,

34
00:02:07.040 --> 00:02:11.400
I wonder it's of course sucks to think that we need death

35
00:02:11.900 --> 00:02:16.080
to appreciate life. Um, but, uh,

36
00:02:16.500 --> 00:02:18.400
that's just may be the way it is. Well.

37
00:02:18.400 --> 00:02:21.440
<v 1>It's interesting if this robotic or artificially intelligent system</v>

38
00:02:23.110 --> 00:02:26.990
understands the world and understands the second law of the thermodynamics and

39
00:02:27.110 --> 00:02:27.943
entropy,

40
00:02:28.500 --> 00:02:32.990
even in artificial intelligence will realize that even if it's parts are really

41
00:02:33.050 --> 00:02:37.270
robust, ultimately it will disintegrate.

42
00:02:37.380 --> 00:02:40.790
<v 2>Yeah. I mean, so the time scales may be different, but in a way,</v>

43
00:02:40.791 --> 00:02:41.391
when you think about it,

44
00:02:41.391 --> 00:02:45.590
doesn't matter once you know that you are mortal in the sense that you are not

45
00:02:45.700 --> 00:02:47.900
eternal, &lt;affirmative&gt; the time scale hardly matters.

46
00:02:48.040 --> 00:02:50.340
Mm-hmm &lt;affirmative&gt; because it's,

47
00:02:50.341 --> 00:02:54.500
it's either the whole thing or not because on the scales of eternity,

48
00:02:54.920 --> 00:02:56.420
any finite duration, however,

49
00:02:56.590 --> 00:03:00.200
large is effectively zero mm-hmm &lt;affirmative&gt; on the scales of eternity.

50
00:03:00.700 --> 00:03:05.360
And so maybe it won't be so hard for an artificial system to feel that sense of

51
00:03:05.550 --> 00:03:06.383
mortality,

52
00:03:07.150 --> 00:03:11.640
because it will recognize the underlying physical laws and recognize its own

53
00:03:12.230 --> 00:03:13.063
fine.

54
00:03:13.700 --> 00:03:16.400
<v 0>And then it'll be us and robots drinking beers,</v>

55
00:03:16.401 --> 00:03:18.870
looking up at the stars and just the, uh,

56
00:03:21.130 --> 00:03:25.990
you know, &lt;laugh&gt;, uh, having a good laugh in awe of the whole thing.

57
00:03:26.310 --> 00:03:27.143
Yeah,

58
00:03:28.310 --> 00:03:32.350
I think that's a pretty good way to end it talking about the fear of death.

59
00:03:32.410 --> 00:03:35.990
We started talking about the meaning of life and ended on the fear of death.

60
00:03:36.310 --> 00:03:39.860
Brian, it's just an incredible conversation. My pleasure. Thank God. Really,

61
00:03:39.861 --> 00:03:43.020
really enjoyed it. It's been a long time coming. I'm a huge fan of your work,

62
00:03:43.100 --> 00:03:46.100
a huge fan of your writing. Thanks for talking to me, Brian. Thank you.

