WEBVTT

1
00:00:02.960 --> 00:00:07.410
<v 0>There's an interesting idea. I think Michael mal brought it up as a, as a test,</v>

2
00:00:07.411 --> 00:00:11.090
whether you're on the left or the right. Uh,

3
00:00:11.790 --> 00:00:15.730
the question he asks, which is, do you think some people are better than others?

4
00:00:17.990 --> 00:00:18.890
If you say yes,

5
00:00:19.150 --> 00:00:24.120
he claims you're on the right &lt;laugh&gt; if you start answering you start like

6
00:00:24.260 --> 00:00:27.800
saying a lot of things like, uh,

7
00:00:29.320 --> 00:00:30.153
uh, you're on the left.

8
00:00:30.340 --> 00:00:33.680
<v 2>So if, if you start explaining yourself well, communicating, yeah.</v>

9
00:00:33.681 --> 00:00:37.920
That's a good term for it. I was really, so I, in this,

10
00:00:38.180 --> 00:00:39.013
in this test,

11
00:00:39.120 --> 00:00:43.990
I suppose I would be on the left because I'm uncomfortable with the idea that

12
00:00:43.991 --> 00:00:47.430
some people are better than others as a basic feeling,

13
00:00:47.730 --> 00:00:50.390
as a starting point in the way you think about the world.

14
00:00:50.580 --> 00:00:54.910
Because as we're talking about, everybody's a hero of their own story.

15
00:00:56.020 --> 00:00:57.190
When you start to think,

16
00:00:57.191 --> 00:01:01.190
some people are better than others as a starting Axiom,

17
00:01:02.820 --> 00:01:07.260
it's like a slippery slope to where you think you're

18
00:01:08.830 --> 00:01:09.890
way better than others.

19
00:01:10.910 --> 00:01:12.370
<v 0>And then you start to like,</v>

20
00:01:12.520 --> 00:01:17.210
basically it's okay to take advantage of a large percent of the

21
00:01:17.220 --> 00:01:20.010
population for the greater good. Totally.

22
00:01:20.150 --> 00:01:24.410
And then you go into Stalin mode and Hitler mode where it's okay to

23
00:01:25.200 --> 00:01:29.280
a larger part out of the population for the greater good. So it's like,

24
00:01:29.310 --> 00:01:32.000
it's this very dangerous, slippery slope in my mind.

25
00:01:32.100 --> 00:01:34.920
<v 2>So I try to not, uh, yeah,</v>

26
00:01:34.921 --> 00:01:37.800
I was always uncomfortable with that kind of test or even that kind of thought

27
00:01:37.980 --> 00:01:40.800
and yes, the same applies and suppose in, um,

28
00:01:41.640 --> 00:01:44.560
in government and central banking is if you,

29
00:01:45.500 --> 00:01:50.270
some people are better than others applying your idea of what is good can have

30
00:01:50.520 --> 00:01:54.750
large scale detrimental, uh, effects. Of course. Yeah.

31
00:01:55.070 --> 00:01:59.470
<v 1>I I'm glad you didn't pose me the question &lt;laugh&gt;. I mean, I think it,</v>

32
00:01:59.870 --> 00:02:04.070
it maybe not the left right. Ax isn't, uh, the disjunction,

33
00:02:04.240 --> 00:02:09.140
isn't the way I would sort of put it, but, um, you know, to me,

34
00:02:09.141 --> 00:02:13.100
it's just, if you reason in a consequentialist way, you know,

35
00:02:13.170 --> 00:02:14.900
that lends itself to authoritarianism.

36
00:02:15.490 --> 00:02:16.300
<v 2>Yeah.</v>

37
00:02:16.300 --> 00:02:20.500
Where whereby you think you can shape society and only you can shape society in

38
00:02:20.501 --> 00:02:25.420
a positive direction, according to your, you know, specific objectives.

