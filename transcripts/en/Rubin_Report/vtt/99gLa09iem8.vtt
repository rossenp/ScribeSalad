WEBVTT

1
00:00:00.060 --> 00:00:04.320
<v 0>One of the hearings. I chaired, we had testimony from a guy, uh, named,</v>

2
00:00:04.340 --> 00:00:07.320
named Dr. Epstein. Oh yeah, I know about it. Um,

3
00:00:07.350 --> 00:00:10.920
and he's a psychologist used to be the editor of psychology today.

4
00:00:11.670 --> 00:00:14.250
He's an academic he's by the way, not a conservative,

5
00:00:14.310 --> 00:00:18.660
he's a liberal Democrat who voted for Hillary Clinton and openly supported

6
00:00:18.661 --> 00:00:19.730
Hillary Clinton. And,

7
00:00:19.731 --> 00:00:24.720
and he did empirical research on Google's manipulative

8
00:00:24.721 --> 00:00:29.550
search outcomes. So it's interesting psychology when you type in a search,

9
00:00:29.610 --> 00:00:32.970
the auto-correct, the auto-fill in, not the auto-correct,

10
00:00:32.971 --> 00:00:37.080
but what automatically populates makes a huge

11
00:00:37.110 --> 00:00:39.180
difference. Um,

12
00:00:39.630 --> 00:00:42.870
and the first few stories that come up make a huge difference.

13
00:00:42.871 --> 00:00:47.760
And there was a dramatic differential between when you typed in Hillary

14
00:00:47.761 --> 00:00:50.490
Clinton, it would, auto-populate good. All good things.

15
00:00:50.850 --> 00:00:53.430
When you typed in Donald Trump, it would auto-populate bad things.

16
00:00:53.431 --> 00:00:56.880
And the stories that would come up would be predominantly good stories for

17
00:00:57.240 --> 00:01:01.890
Hillary predominantly bad stories, uh, for Trump. And what was interesting is,

18
00:01:01.891 --> 00:01:06.420
is, is Dr. Epstein did the study and he concluded that in 2016,

19
00:01:06.421 --> 00:01:07.830
Google's, uh,

20
00:01:08.940 --> 00:01:12.720
deceptive search outcomes shifted

21
00:01:12.870 --> 00:01:17.730
2.4 million votes to Hillary Clinton.

22
00:01:18.150 --> 00:01:21.450
And he said this as a Hillary Clinton supporter, but he was, he was horrified,

23
00:01:21.451 --> 00:01:25.620
but he also projected, he said in 2015, he said,

24
00:01:25.621 --> 00:01:27.840
big tech is getting worse. It's getting more aggressive.

25
00:01:27.841 --> 00:01:30.600
They could move as many as 15 million votes

26
00:01:32.460 --> 00:01:36.330
to the Democrats in 2020. And so I think it's a huge threat.

27
00:01:46.190 --> 00:01:47.630
<v 2>We got the important stuff out of the way.</v>

28
00:01:47.690 --> 00:01:51.290
Let's talk about big tech because you've been right in the center of this thing.

29
00:01:51.291 --> 00:01:53.270
As you know, I've been fighting it from my,

30
00:01:53.300 --> 00:01:58.250
from my garage for the last five years. Uh, Trump did this executive action.

31
00:01:59.000 --> 00:02:02.840
Uh, two 30 is the, is the, what do you call it? It's not, it's not a bill.

32
00:02:02.841 --> 00:02:03.380
It's a.

33
00:02:03.380 --> 00:02:07.550
<v 0>So it's section, section 30 of a bill called the communications decency act,</v>

34
00:02:07.730 --> 00:02:07.910
right.

35
00:02:07.910 --> 00:02:12.320
<v 2>Basically what he did was stripped some protections from the big boys from</v>

36
00:02:12.321 --> 00:02:15.710
Twitter, YouTube, and Facebook. Now my personal preference.

37
00:02:15.711 --> 00:02:17.270
And I think we're pretty close on this.

38
00:02:17.630 --> 00:02:20.750
The libertarian side is you don't want the government running around sending

39
00:02:20.751 --> 00:02:23.360
regulators to these companies cause that's not going to do good,

40
00:02:23.630 --> 00:02:28.340
but stripping protection protections to me seemed like the

41
00:02:28.341 --> 00:02:32.540
right idea. Do you agree with the right idea? Is it enough? Is it not enough,

42
00:02:32.600 --> 00:02:33.433
et cetera?

43
00:02:33.500 --> 00:02:35.300
<v 0>I think it is the right idea. Um,</v>

44
00:02:35.330 --> 00:02:38.660
if it's something I'd been urging Trump to do for three years,

45
00:02:38.661 --> 00:02:43.640
so I'm glad that that the administration did it. Um, listen,

46
00:02:43.641 --> 00:02:47.840
I agree with your sensibility. Nobody wants government free speech police.

47
00:02:47.870 --> 00:02:51.170
I mean that, that would be a terrible outcome. Some people do, but

48
00:02:52.760 --> 00:02:56.690
nobody who's not insane. And that's a qualifier.

49
00:02:56.691 --> 00:02:59.350
We can talk about some more because there are a lot of people that fall that

50
00:02:59.351 --> 00:03:03.220
category, but government free speech police would be a terrible outcome.

51
00:03:03.790 --> 00:03:05.920
But what big tech is doing,

52
00:03:08.350 --> 00:03:11.350
it's deliberate. It's conscious, it's naked,

53
00:03:12.610 --> 00:03:14.470
it's abusive and it's dangerous.

54
00:03:14.471 --> 00:03:19.360
I think it's the single biggest threat to free speech and democracy

55
00:03:19.361 --> 00:03:21.400
we have in this country, uh,

56
00:03:22.660 --> 00:03:25.390
because big tech has become a monopoly,

57
00:03:26.230 --> 00:03:30.730
controlling the instruments of communication. And you know,

58
00:03:30.731 --> 00:03:34.090
I've chaired multiple hearings in the Senate on big tech censorship.

59
00:03:35.260 --> 00:03:37.660
And one of the hearings,

60
00:03:37.661 --> 00:03:40.360
we talked about a document that Google had prepared.

61
00:03:40.361 --> 00:03:41.500
It's called the good sensor.

62
00:03:41.980 --> 00:03:44.290
So they prepared it's PowerPoints about 50 page long.

63
00:03:45.190 --> 00:03:49.930
And it talks about how the old vision of the internet was the

64
00:03:49.931 --> 00:03:52.210
free speech, laissez Faire internet,

65
00:03:52.211 --> 00:03:53.740
where people could speak and say what they wanted.

66
00:03:54.370 --> 00:03:56.890
And then it talked about the new vision of the internet.

67
00:03:56.891 --> 00:04:01.000
And this is Google's own words is the European style

68
00:04:01.300 --> 00:04:03.460
censorship model. And by the way,

69
00:04:03.461 --> 00:04:07.300
the four companies that Google identified as implementing it were Google,

70
00:04:07.301 --> 00:04:08.560
YouTube, Facebook, and Twitter.

71
00:04:09.790 --> 00:04:14.200
This is a conscious decision and the dangers are enormous. So the question is,

72
00:04:14.201 --> 00:04:16.630
how do you fix it? If it is a problem,

73
00:04:16.631 --> 00:04:19.360
we can have a long discussion about whether it's a problem, although

74
00:04:21.160 --> 00:04:22.360
they're not hiding it anymore.

75
00:04:22.470 --> 00:04:27.300
<v 2>Yeah. I think most sane people at this point would agree that there even,</v>

76
00:04:27.301 --> 00:04:29.490
even people that disagree with us politically on this,

77
00:04:29.700 --> 00:04:33.870
I think most people realize that the extraordinary amount of power is a problem

78
00:04:33.871 --> 00:04:34.704
one way or no.

79
00:04:34.950 --> 00:04:38.910
<v 0>So I will. It's interesting. The political debate though,</v>

80
00:04:38.911 --> 00:04:42.870
one of the talking points at big tech in the left is they don't engage in

81
00:04:42.871 --> 00:04:46.560
censorship. And the reason they say that is they say, well,

82
00:04:46.561 --> 00:04:51.270
there are no objective data that prove we do. And it's,

83
00:04:51.460 --> 00:04:51.750
you know,

84
00:04:51.750 --> 00:04:55.680
there's the old aphorism of the PR of the guy who kills his parents and then

85
00:04:55.681 --> 00:04:59.100
pleads mercy on the court because he's an orphan. It's true.

86
00:04:59.101 --> 00:05:03.120
There are no objective data because Vic tech controls all the data and they,

87
00:05:03.210 --> 00:05:06.210
they there's zero transparency, zero accountability.

88
00:05:06.211 --> 00:05:10.110
So you can use anecdotes and I I've gone through lots and lots of specific

89
00:05:10.111 --> 00:05:14.010
anecdotes, but every time you ask big tech and I've done it in writing,

90
00:05:14.011 --> 00:05:18.630
I've done it in hearings, simple questions. Um, all right.

91
00:05:18.631 --> 00:05:20.520
In the 2018 election cycle,

92
00:05:20.910 --> 00:05:23.880
how many posts from Republican candidates for office?

93
00:05:23.881 --> 00:05:25.260
Did you block or shadow ban?

94
00:05:25.980 --> 00:05:28.200
How many posts from democratic candidates for office?

95
00:05:28.201 --> 00:05:32.010
Did you block or shadow ban their objective answers to that? I mean,

96
00:05:32.011 --> 00:05:36.990
there is a number and they know the number and they refuse to answer

97
00:05:36.991 --> 00:05:40.530
it and then say, there are no data. And so how do you fix it?

98
00:05:41.400 --> 00:05:45.240
I think one way to fix it is getting rid of the special immunity from liability

99
00:05:45.241 --> 00:05:48.600
that big tech has that Congress gave them. So that's two 30, right?

100
00:05:48.601 --> 00:05:52.620
That's two 30. And the reason two 30 was passed.

101
00:05:53.790 --> 00:05:58.730
Congress believed big tech would be a neutral public forum.

102
00:05:59.570 --> 00:06:00.403
In other words,

103
00:06:00.620 --> 00:06:05.420
it wasn't fair to Sue Facebook for a

104
00:06:05.421 --> 00:06:08.450
comment made by an individual comment or cause it wasn't,

105
00:06:08.451 --> 00:06:10.880
they weren't the speaker, it was someone else. And so that was,

106
00:06:11.090 --> 00:06:12.740
that was Congress's reasoning.

107
00:06:12.741 --> 00:06:16.520
So we're going to give big Texas immunity from liability because it's third

108
00:06:16.521 --> 00:06:20.480
party speakers and we want to see the internet grow well.

109
00:06:20.480 --> 00:06:22.730
What's happened is big tech changed their mind. And they said,

110
00:06:22.731 --> 00:06:27.200
we're not going to be, to use the language of the Google document,

111
00:06:27.530 --> 00:06:31.580
the laissez Faire free speech place anymore. We're going to sensor well,

112
00:06:31.581 --> 00:06:33.560
you know what, if they're going to silence views,

113
00:06:33.561 --> 00:06:38.420
they disagree and promote views. They agree with they don't deserve.

114
00:06:38.421 --> 00:06:40.580
I don't believe as a public policy matter,

115
00:06:40.610 --> 00:06:44.690
a special protection vote for liability. There's also the antitrust laws.

116
00:06:45.860 --> 00:06:49.370
Google is a monopoly by any measure,

117
00:06:49.371 --> 00:06:51.980
big tech is richer, stronger,

118
00:06:51.981 --> 00:06:56.420
more powerful than at and T was when it was broken up under the antitrust laws.

119
00:06:56.421 --> 00:07:01.100
They're bigger than, than us steel was, uh, a line from the godfather.

120
00:07:01.220 --> 00:07:04.970
We're bigger than USDA where they're bigger than USD, um,

121
00:07:06.170 --> 00:07:11.000
and that abuse of power. So I've also,

122
00:07:11.001 --> 00:07:12.410
you want to talk about the real,

123
00:07:13.820 --> 00:07:17.720
what the Trump administration did on section two 30 will be challenged.

124
00:07:17.721 --> 00:07:20.510
It's at the FCC. There'll be litigation.

125
00:07:21.110 --> 00:07:25.550
The real bite here is federal antitrust

126
00:07:26.060 --> 00:07:30.890
litigation, which, which I have urged the president to pursue diverged.

127
00:07:30.891 --> 00:07:33.730
The vice-president pursue have urged the attorney general to pursue I've urged

128
00:07:33.731 --> 00:07:36.170
the chairman of the federal trade commission to pursue urge the white house

129
00:07:36.200 --> 00:07:39.980
chief of staff to pursue I've urged the white house counsel to pursue, to,

130
00:07:40.220 --> 00:07:42.020
to force the transparency,

131
00:07:42.021 --> 00:07:46.550
to get the answers and to stop their, their, their naked bias.

132
00:07:46.690 --> 00:07:47.800
<v 2>Are you worried though,</v>

133
00:07:47.801 --> 00:07:52.000
that as we sit here right now in the middle of July only, you know, three,

134
00:07:52.001 --> 00:07:55.300
four months before an election that insert in a certain way that the ship has

135
00:07:55.301 --> 00:07:57.790
already sailed the damage that they've done.

136
00:07:58.150 --> 00:08:01.870
And even if you could get everything you wanted tomorrow pass tomorrow,

137
00:08:02.050 --> 00:08:05.800
by the time the processes and the systems are are implemented.

138
00:08:06.280 --> 00:08:07.240
Congratulations.

139
00:08:08.400 --> 00:08:11.710
<v 0>I am deeply worried about it. One,</v>

140
00:08:12.240 --> 00:08:16.560
one of the hearings I chaired, we had testimony from a guy, uh, named,

141
00:08:16.561 --> 00:08:19.510
named Dr. Epstein. Oh yeah, I know about it. Um,

142
00:08:19.540 --> 00:08:23.140
and he's a psychologist used to be the editor of psychology today.

143
00:08:23.860 --> 00:08:26.470
He's an academic he's by the way, not a conservative,

144
00:08:26.520 --> 00:08:30.850
he's a liberal Democrat who voted for Hillary Clinton and openly supported

145
00:08:30.851 --> 00:08:31.950
Hillary Clinton. And,

146
00:08:32.140 --> 00:08:36.910
and he did empirical research on Google's manipulative

147
00:08:36.911 --> 00:08:41.770
search outcomes. So it's interesting psychology when you type in a search,

148
00:08:41.800 --> 00:08:45.160
the autocorrect, the auto-fill in, not the auto-correct,

149
00:08:45.161 --> 00:08:49.300
but what automatically populates makes a huge

150
00:08:49.330 --> 00:08:51.370
difference. Um,

151
00:08:51.850 --> 00:08:55.060
and the first few stories that come up make a huge difference.

152
00:08:55.260 --> 00:08:59.940
There was a dramatic differential between when you typed in Hillary

153
00:08:59.941 --> 00:09:02.700
Clinton, it would, auto-populate good. All good things.

154
00:09:03.060 --> 00:09:05.610
When you typed in Donald Trump, it would auto-populate bad things.

155
00:09:05.611 --> 00:09:09.090
And the stories that would come up would be predominantly good stories for

156
00:09:09.091 --> 00:09:12.540
Hillary predominantly bad stories, uh, for Trump.

157
00:09:12.900 --> 00:09:14.490
And what was interesting is, is,

158
00:09:14.491 --> 00:09:19.200
is Dr. Epstein did the study and he concluded that in 2016, Google's,

159
00:09:20.040 --> 00:09:24.900
uh, deceptive search outcomes shifted

160
00:09:25.050 --> 00:09:29.910
2.4 million votes to Hillary Clinton.

161
00:09:30.330 --> 00:09:33.630
And he said this as a Hillary Clinton supporter, but he was, he was horrified,

162
00:09:33.631 --> 00:09:37.830
but he also projected, he said in 2015, he said,

163
00:09:37.831 --> 00:09:40.020
big tech is getting worse. It's getting more aggressive.

164
00:09:40.021 --> 00:09:42.780
They could move as many as 15 million votes

165
00:09:44.670 --> 00:09:48.360
to the Democrats in 2020. And so I think it's a huge threat.

166
00:09:48.740 --> 00:09:52.040
<v 3>If you're looking for more honest and thoughtful conversations about politics,</v>

167
00:09:52.041 --> 00:09:55.370
instead of nonstop yelling, checkout our politics playlist.

168
00:09:55.400 --> 00:09:58.310
And if you want to watch full interviews on a variety of topics,

169
00:09:58.311 --> 00:10:01.550
watch our full episode playlists, all right, over here,

170
00:10:01.850 --> 00:10:03.740
and to get notified of all future videos,

171
00:10:03.741 --> 00:10:06.620
be sure to subscribe and click the notification bell.

