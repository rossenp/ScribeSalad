WEBVTT

1
00:00:00.270 --> 00:00:04.290
<v 0>Dear fellow scholars. This is two minute papers with Dr. [inaudible] here.</v>

2
00:00:04.880 --> 00:00:09.140
<v 1>Early 2019. A learning based technique appeared that could perform common,</v>

3
00:00:09.141 --> 00:00:13.190
natural language processing operations. For instance, answering questions,

4
00:00:13.400 --> 00:00:17.990
completing text, reading, comprehension, summarization, and more.

5
00:00:18.290 --> 00:00:23.150
This method was developed by scientists at open AI and they call it GPT

6
00:00:23.151 --> 00:00:24.770
to a follow-up paper,

7
00:00:24.800 --> 00:00:29.360
introduced a more capable version of this technique called the GBT three and

8
00:00:29.510 --> 00:00:31.220
among many incredible examples.

9
00:00:31.370 --> 00:00:34.520
It could generate website layouts from a written description.

10
00:00:34.820 --> 00:00:39.650
The key idea in both cases was that we will provide it an incomplete piece of

11
00:00:39.651 --> 00:00:43.100
text and it would try to finish it. However,

12
00:00:43.190 --> 00:00:47.600
no one said that these neural networks have to only deal with text information

13
00:00:47.870 --> 00:00:48.703
and sure enough,

14
00:00:48.740 --> 00:00:53.570
in this work scientists at open AI introduced a new version of this method that

15
00:00:53.571 --> 00:00:56.810
tries to complete not taxed bad images.

16
00:00:57.230 --> 00:00:58.940
The problem statement is simple.

17
00:00:59.150 --> 00:01:03.890
We give it an incomplete image and we ask the AI to fill in the missing pixels.

18
00:01:04.370 --> 00:01:05.510
That is of course,

19
00:01:05.660 --> 00:01:10.070
an immensely difficult task because these images made the picked any part of the

20
00:01:10.071 --> 00:01:10.970
world around us.

21
00:01:11.390 --> 00:01:15.320
It would have to know a great deal about our world to be able to continue the

22
00:01:15.321 --> 00:01:19.010
images. So how well did it do let's have a look.

23
00:01:19.460 --> 00:01:22.910
This is undoubtedly a cat bat. Look,

24
00:01:23.390 --> 00:01:25.640
see that white part that is just starting.

25
00:01:26.120 --> 00:01:30.320
The interesting part has been cut out of the image. What could that be?

26
00:01:30.710 --> 00:01:35.660
A piece of paper or something else now let's leave the dirty work to the machine

27
00:01:35.810 --> 00:01:39.530
and ask it to finish it. Wow.

28
00:01:40.130 --> 00:01:45.110
The piece of paper, indeed, according to the AI and it even has taxed on it,

29
00:01:45.500 --> 00:01:49.490
but attacks has a heading section and a paragraph below it to

30
00:01:50.030 --> 00:01:53.540
truly excellent. You know, what is even more excellent,

31
00:01:53.870 --> 00:01:55.280
perhaps the best part.

32
00:01:55.610 --> 00:01:59.390
It also added the indirect illumination on the fur of the cat,

33
00:01:59.690 --> 00:02:04.100
meaning that it sees that a blue room surrounds it and therefore some amount of

34
00:02:04.101 --> 00:02:07.880
the color bleeds onto the fare of the cat making it blower.

35
00:02:08.360 --> 00:02:10.640
I am a light transport researcher by trade.

36
00:02:10.790 --> 00:02:14.060
So I spent the majority of my life calculating things like this.

37
00:02:14.270 --> 00:02:17.030
And I have to say that this looks quite good to me.

38
00:02:17.450 --> 00:02:21.740
Absolutely amazing attention to detail, but it had more ideas.

39
00:02:22.220 --> 00:02:26.240
What's this, the face of the cat has been finished quite well, in fact,

40
00:02:26.480 --> 00:02:30.890
but the rest, I am not so sure if you have an idea what this is supposed to be,

41
00:02:31.100 --> 00:02:34.790
please let me know in the comments and here go the rest of the results.

42
00:02:35.300 --> 00:02:36.320
All quite good.

43
00:02:36.890 --> 00:02:40.760
And the true real image has been concealed for the algorithm.

44
00:02:41.600 --> 00:02:45.770
This is the reference solution. Let's see the next one. Oh,

45
00:02:45.780 --> 00:02:49.430
my scientists at open AI pulled no punches here.

46
00:02:49.550 --> 00:02:53.600
This is also quite nasty. How many stripes should this continue with

47
00:02:55.340 --> 00:03:00.190
zero? Maybe in any case, this solution is not unreasonable.

48
00:03:00.550 --> 00:03:03.820
I appreciate the fact that it continued the shadows of the humans.

49
00:03:05.050 --> 00:03:08.590
Next one. Yes. More stripes. Great,

50
00:03:08.890 --> 00:03:13.090
but likely a few too many here are the remainder of the solutions

51
00:03:14.170 --> 00:03:16.210
and the true reference image. Again,

52
00:03:17.440 --> 00:03:20.260
let's have a look at this water droplet example too.

53
00:03:20.710 --> 00:03:25.150
We humans know that since we see the remnants of some ripples over there too,

54
00:03:25.300 --> 00:03:28.570
there must be a splash bat. Does the AI know?

55
00:03:30.880 --> 00:03:35.680
Oh yes. Yes, it does. Amazing. And the true image.

56
00:03:37.780 --> 00:03:39.820
Now what about these little creatures?

57
00:03:40.780 --> 00:03:45.010
The first continuation finishes them correctly and puts them on a twig.

58
00:03:45.550 --> 00:03:47.710
The second one involves a stone.

59
00:03:48.910 --> 00:03:53.560
The third is my favorite hold onto your papers. And look at this.

60
00:03:54.190 --> 00:03:58.120
They stand in the water and we can even see their mirror images.

61
00:03:58.690 --> 00:03:59.523
Wow.

62
00:04:00.310 --> 00:04:04.720
The fourth is a branch and finally the true reference image.

63
00:04:05.110 --> 00:04:07.690
This is one of its best works. I have seen so far.

64
00:04:08.800 --> 00:04:13.450
Here are some more results and note that these are not cherry-picked or in other

65
00:04:13.451 --> 00:04:17.650
words, there was no selection process for the results. Nothing was discarded.

66
00:04:18.040 --> 00:04:20.290
These came out from the AI. As you see them,

67
00:04:20.980 --> 00:04:23.800
there's a link to these and to the paper in the video description.

68
00:04:23.890 --> 00:04:26.500
So make sure to have a look and let me know in the comments,

69
00:04:26.530 --> 00:04:28.270
if you have found something interesting.

70
00:04:28.630 --> 00:04:32.890
So what about the size of the neural network for this technique? Well,

71
00:04:33.070 --> 00:04:36.880
it contains from one and a half to about 7 billion parameters.

72
00:04:37.210 --> 00:04:40.060
Let's have a look together and find out what that means.

73
00:04:40.570 --> 00:04:42.910
These are the results from the GPT two paper,

74
00:04:43.120 --> 00:04:47.050
the previous version of the text processor on a challenging reading

75
00:04:47.051 --> 00:04:50.410
comprehension test as a function of the number of parameters.

76
00:04:50.890 --> 00:04:53.590
As you see around one and a half billion parameters,

77
00:04:53.650 --> 00:04:57.820
which is roughly similar to GPT two, it learned a great deal,

78
00:04:58.000 --> 00:05:02.440
but its understanding was nowhere near the level of human comprehension.

79
00:05:02.770 --> 00:05:06.970
However, as they grew the network, something incredible happened.

80
00:05:08.140 --> 00:05:09.880
Non-trivial capabilities started to appear.

81
00:05:09.970 --> 00:05:13.270
As we approached a hundred billion parameters, look,

82
00:05:13.780 --> 00:05:18.640
it nearly matched the level of humans and all this was measured on a

83
00:05:18.641 --> 00:05:20.350
nasty reading comprehension test.

84
00:05:20.980 --> 00:05:25.660
So this image GPT has the number of parameters that is closer to GPT

85
00:05:26.080 --> 00:05:27.280
two than GPT three.

86
00:05:27.520 --> 00:05:32.350
So we can maybe speculate that the next version could be potentially another

87
00:05:32.351 --> 00:05:36.400
explosion in capabilities. I can't wait to have a look at that.

88
00:05:36.790 --> 00:05:38.200
What a time to be alive.

89
00:05:38.560 --> 00:05:42.370
This episode has been supported by weights and biases in this post.

90
00:05:42.400 --> 00:05:46.810
They show you how to use their system to visualize which part of the image your

91
00:05:46.811 --> 00:05:50.350
neural network looks at before. It concludes that it is a cat.

92
00:05:50.620 --> 00:05:53.620
You can even try an example in an interactive notebook,

93
00:05:53.650 --> 00:05:55.330
through the link in the video description,

94
00:05:55.720 --> 00:05:59.390
weights and biases provides tools to track your experiments in your deep

95
00:05:59.391 --> 00:06:03.980
learning projects. Their system is designed to save you a ton of time and money,

96
00:06:04.130 --> 00:06:07.310
and it is actively used in projects at prestigious labs,

97
00:06:07.400 --> 00:06:11.570
such as open AI Toyota research GitHub and more.

98
00:06:11.960 --> 00:06:16.220
And the best part is that if you have an open source academic or personal

99
00:06:16.221 --> 00:06:21.200
project, you can use their tools for free. It really is as good as it gets.

100
00:06:21.500 --> 00:06:22.790
Make sure to visit them through

101
00:06:25.250 --> 00:06:28.850
wmb.com/papers or click the link in the video description to start tracking your

102
00:06:28.851 --> 00:06:30.620
experiments in five minutes,

103
00:06:31.190 --> 00:06:35.210
our thanks to weights and biases for their longstanding support and for helping

104
00:06:35.211 --> 00:06:36.710
us make better videos for you.

105
00:06:37.130 --> 00:06:41.060
Thanks for watching and for your generous support. And I'll see you next time.

