WEBVTT

1
00:00:00.120 --> 00:00:04.190
<v 0>Dear fellow scholars. This is two minute papers with Dr. [inaudible]</v>

2
00:00:04.190 --> 00:00:04.190
. In the last few years, we have seen a bunch of new AI based techniques that were specialized in generating new and novel images. This is mainly done through 

3
00:00:04.190 --> 00:00:04.190
learning based

4
00:00:04.190 --> 00:00:04.190
 techniques. Typically a generative adversarial network, again in short, which is an architecture where a generator neural network creates new images and passes it to a discriminator network, which learns to distinguish real photos from these fake generated images. The two networks learn and improve together and generate better and better images over time. What do you see here is a set of results created with 

5
00:00:04.190 --> 00:00:04.190
a technique by the name psycho

6
00:00:04.190 --> 00:00:04.190
. Again, this could even translate daytime into nighttime images. Re-imagine

7
00:00:04.190 --> 00:00:04.190
 the picture of a horse as if it were a zebra and more, we can also use it for 

8
00:00:04.190 --> 00:00:04.190
star transfer the problem where we have to

9
00:00:04.190 --> 00:00:05.023
 input images, one for content and one for style.

10
00:01:02.040 --> 00:01:06.030
And as you see here, the output would be a nice mixture of the two.

11
00:01:06.690 --> 00:01:10.380
However, if we use cycle GaN for this kind of style transfer,

12
00:01:10.920 --> 00:01:12.330
we'll get something like this.

13
00:01:12.870 --> 00:01:17.160
The goal was to learn the style of a select set of famous illustrators of

14
00:01:17.161 --> 00:01:20.550
children's books by providing an input image with their work.

15
00:01:21.240 --> 00:01:26.160
So what do you think about the results while the style is indeed completely

16
00:01:26.161 --> 00:01:27.210
different from the source,

17
00:01:27.600 --> 00:01:32.070
but the algorithm seems a little too heavy handed and did not leave the content

18
00:01:32.071 --> 00:01:36.540
itself intact. Let's have a look at another result with a previous technique.

19
00:01:36.990 --> 00:01:40.200
Maybe this will do better. This is due again,

20
00:01:40.350 --> 00:01:44.820
which refers to a paper by the name and supervise dual learning for image to

21
00:01:44.821 --> 00:01:49.170
image translation. This uses two Gans to perform image translation,

22
00:01:49.560 --> 00:01:53.730
where one GaN learns to translate for instance, from day to night,

23
00:01:54.240 --> 00:01:58.050
while the other learns the opposite night, two day translation.

24
00:01:59.160 --> 00:02:02.580
This among other advantages makes things very efficient.

25
00:02:02.880 --> 00:02:07.770
But as you see here in these cases, it preserves the content of the image,

26
00:02:08.430 --> 00:02:12.840
but perhaps a little too much because the style itself does not appear too

27
00:02:12.841 --> 00:02:14.700
prominently in the output images.

28
00:02:15.480 --> 00:02:18.930
So cycle GaN is good at transferring style,

29
00:02:19.230 --> 00:02:23.670
but a little less so for content and do again is good at preserving the

30
00:02:23.671 --> 00:02:27.870
content, but sometimes adds too little of the style to the image.

31
00:02:28.530 --> 00:02:32.940
And now hold onto your papers because this new technique by the name Gunilla

32
00:02:33.120 --> 00:02:37.920
offers us these results, the content is intact checkmark,

33
00:02:38.280 --> 00:02:42.420
and the style goes through really well. Check Mark it,

34
00:02:42.421 --> 00:02:46.710
preserves the content and transfers the style at the same time.

35
00:02:47.310 --> 00:02:48.143
Excellent.

36
00:02:48.570 --> 00:02:52.470
One of the many key reasons as to why this happens is the usage of skip

37
00:02:52.471 --> 00:02:53.304
connections,

38
00:02:53.370 --> 00:02:58.080
which help preserve the content information as we travel deeper into the neural

39
00:02:58.081 --> 00:02:59.980
network. So finally,

40
00:03:00.130 --> 00:03:04.870
let's put our money where our mouth is and take a bunch of illustrators Marvel

41
00:03:04.930 --> 00:03:09.790
at their unique style and then apply it to photographs and see how

42
00:03:09.791 --> 00:03:13.020
the algorithm stacks up against other previous works.

43
00:03:15.630 --> 00:03:18.330
Wow. I love these beautiful results.

44
00:03:18.810 --> 00:03:23.130
These comparisons really show how good the new Gunilla technique is at

45
00:03:23.131 --> 00:03:28.110
preserving content and note that these are distinct artistic styles that are

46
00:03:28.111 --> 00:03:31.020
really difficult to reproduce. Even for humans.

47
00:03:31.500 --> 00:03:35.280
It is truly amazing that we can perform such a thing. Algorithmically,

48
00:03:35.880 --> 00:03:40.680
don't forget that the first star transfer paper appeared approximately three

49
00:03:40.740 --> 00:03:45.180
to three and a half years ago, and now we have come a long, long way.

50
00:03:45.630 --> 00:03:50.580
The pace of progress in machine learning research is truly stunning while we are

51
00:03:50.581 --> 00:03:54.840
looking at some more amazing results this time around only from Gunilla.

52
00:03:55.170 --> 00:03:59.910
I will note that the authors also made a user study with 48 people who

53
00:03:59.911 --> 00:04:02.160
favored this against yes techniques

54
00:04:06.360 --> 00:04:08.250
and perhaps leaving the best for last.

55
00:04:08.550 --> 00:04:12.060
It can even draw in the style of [inaudible]. I bet they're

56
00:04:12.060 --> 00:04:12.893
 a bunch of Miyazaki fans watching. So let me know in the comments, what you think about these results, what a time to be alive.

57
00:04:21.210 --> 00:04:25.050
And this episode has been supported by weights and biases in this post.

58
00:04:25.140 --> 00:04:29.910
They show you how to easily iterate on models by visualizing and comparing

59
00:04:29.911 --> 00:04:31.650
experiments. In real time.

60
00:04:32.070 --> 00:04:36.270
Also weights and biases provides tools to track your experiments in your deep

61
00:04:36.271 --> 00:04:40.950
learning project. Their system is designed to save you a ton of time and money,

62
00:04:41.250 --> 00:04:44.670
and it is actively used in projects at prestigious labs,

63
00:04:44.850 --> 00:04:48.960
such as open AI Toyota research GitHub and more.

64
00:04:49.440 --> 00:04:54.120
And the best part is that if you're an academic or have an open source project,

65
00:04:54.300 --> 00:04:58.710
you can use their tools for free. It really is as good as it gets,

66
00:04:59.010 --> 00:05:02.580
make sure to visit them through wnb.com/papers,

67
00:05:02.850 --> 00:05:05.220
or just click the link in the video description.

68
00:05:05.460 --> 00:05:09.930
And you can get a freedom or today are thanks to weights and biases for their

69
00:05:10.500 --> 00:05:13.740
longstanding support and for helping us make better videos for you.

70
00:05:14.160 --> 00:05:18.090
Thanks for watching and for your generous support. And I'll see you next time.

