WEBVTT

1
00:00:00.150 --> 00:00:01.350
<v 0>Dear fellow scholars.</v>

2
00:00:01.620 --> 00:00:04.280
This is two minute papers with Dr. [inaudible]. If you have been watching this

3
00:00:04.280 --> 00:00:05.113
 serious for awhile.

4
00:00:06.890 --> 00:00:11.840
<v 1>You know, very well that I love learning algorithms and fluid simulations,</v>

5
00:00:12.170 --> 00:00:17.030
but do you know what I like even better learning algorithms applied to

6
00:00:17.031 --> 00:00:21.170
fluid simulations. So I couldn't be happier with today's paper.

7
00:00:21.710 --> 00:00:23.990
We can create wondrous fluid simulations,

8
00:00:24.140 --> 00:00:28.550
like the ones you see here by studying the laws of fluid motion from physics

9
00:00:28.790 --> 00:00:32.150
and right, the computer program that contains these laws.

10
00:00:32.690 --> 00:00:36.830
As you see the amount of detail we can simulate with these programs is nothing

11
00:00:36.831 --> 00:00:40.700
short of amazing. However, I just mentioned neural networks.

12
00:00:40.970 --> 00:00:44.960
If we can write a simulator that runs the laws of physics to create these

13
00:00:44.961 --> 00:00:48.320
programs, why would we need learning based algorithms?

14
00:00:49.220 --> 00:00:53.630
The answer is in this paper that we have discussed about 300 episodes ago,

15
00:00:54.140 --> 00:00:56.330
the goal was to show a neural network,

16
00:00:56.390 --> 00:01:00.260
video footage of lots and lots of fluid and smoke simulations,

17
00:01:00.530 --> 00:01:05.330
and have it learn how the dynamics work to the point that it can continue

18
00:01:05.420 --> 00:01:09.500
and guess how the behavior of a smoke path would change over time.

19
00:01:09.890 --> 00:01:13.760
We stopped the video and it would learn how to continue it if you will.

20
00:01:14.570 --> 00:01:16.610
There's definitely is an interesting take.

21
00:01:16.760 --> 00:01:21.500
As normally we use neural networks to solve problems that are otherwise close to

22
00:01:21.501 --> 00:01:26.450
impossible to tackle. For instance, it is very hard if not impossible,

23
00:01:26.480 --> 00:01:30.500
to create a hand-crafted algorithm that detects cats reliably,

24
00:01:30.680 --> 00:01:34.520
because we cannot really write down the mathematical description of a cat.

25
00:01:35.060 --> 00:01:39.080
However, these days we can easily teach a neural network to do that.

26
00:01:39.560 --> 00:01:43.340
But this task here is fundamentally different. Here.

27
00:01:43.400 --> 00:01:47.930
The neural networks are applied to solve something that we already know how to

28
00:01:47.931 --> 00:01:52.760
solve. Especially given that if we use a neural network to perform this task,

29
00:01:52.910 --> 00:01:56.210
we have to train it, which is a long and arduous process.

30
00:01:56.630 --> 00:01:59.570
I hope to have convinced you that this is a bad, bad idea.

31
00:02:00.020 --> 00:02:04.250
Why would anyone better to do that? Does this make any sense?

32
00:02:05.120 --> 00:02:07.070
Well, it does make a lot of sense.

33
00:02:07.370 --> 00:02:11.930
And the reason for that is that this training step only has to be done once and

34
00:02:11.931 --> 00:02:16.490
afterwards querying the neural network that is predicting what happens next in

35
00:02:16.491 --> 00:02:19.040
the simulation runs almost immediately.

36
00:02:19.730 --> 00:02:24.290
This takes way less time than calculating all the forces and pressures in the

37
00:02:24.291 --> 00:02:27.380
simulation while retaining high quality results.

38
00:02:27.920 --> 00:02:32.690
So we suddenly went from thinking that an idea is useless to being

39
00:02:32.780 --> 00:02:37.370
amazing. What are the weaknesses of the approach? Generalization,

40
00:02:37.850 --> 00:02:39.350
you see these techniques,

41
00:02:39.500 --> 00:02:44.360
including a newer variant that you see here can give us detailed simulations in

42
00:02:44.361 --> 00:02:46.640
real time or close to real time.

43
00:02:47.000 --> 00:02:51.260
But if we present them with something that is far outside of the cases that they

44
00:02:51.261 --> 00:02:53.870
had seen in the training domain, they will fail.

45
00:02:54.470 --> 00:02:59.150
This does not happen with our handcrafted techniques only to AI based methods.

46
00:02:59.590 --> 00:03:01.600
So onwards to this new technique,

47
00:03:01.780 --> 00:03:06.010
and you will see in just a moment that a key differentiator here is that is

48
00:03:06.011 --> 00:03:10.150
generalization capabilities are just astounding. Look here.

49
00:03:10.600 --> 00:03:14.020
The predicted results match, the true simulation quite well.

50
00:03:16.390 --> 00:03:20.470
Let's look at it in slow motion tool so we can evaluate it a little better

51
00:03:22.840 --> 00:03:27.220
looking great, but we have talked about superior generalization.

52
00:03:27.280 --> 00:03:30.010
So what about that? Well,

53
00:03:30.070 --> 00:03:32.980
it can also handle sand and goop simulations.

54
00:03:33.100 --> 00:03:36.220
So that's a great step beyond just water and smoke.

55
00:03:45.660 --> 00:03:47.520
And now have a look at this one.

56
00:03:47.880 --> 00:03:50.460
This is a scene with the boxes it has been trained on,

57
00:03:52.380 --> 00:03:57.300
and now let's ask it to try to simulate the evolution of significantly

58
00:03:57.301 --> 00:04:02.100
different shapes. Wow.

59
00:04:02.490 --> 00:04:05.580
It not only does well with these previously unseen shapes,

60
00:04:05.820 --> 00:04:09.300
but it also handles their interactions really well,

61
00:04:10.470 --> 00:04:11.580
but there is more,

62
00:04:11.880 --> 00:04:16.020
we can also train it on a tiny domain with only a few particles.

63
00:04:16.260 --> 00:04:20.880
And then it is able to learn general concepts that we can reuse to

64
00:04:20.881 --> 00:04:24.840
simulate a much bigger domain and also with more particles.

65
00:04:25.380 --> 00:04:29.730
Fantastic. But there is even more,

66
00:04:30.180 --> 00:04:33.960
we can train it by showing how water behaves on these water ramps,

67
00:04:34.500 --> 00:04:39.270
and then let's remove the ramps and see if it understands what it has to

68
00:04:39.271 --> 00:04:42.480
do with all these particles. Yes, it does.

69
00:04:42.960 --> 00:04:47.460
Now let's give it something more difficult. I want more ramps.

70
00:04:48.000 --> 00:04:51.180
Yes. And now even more AMS.

71
00:04:56.160 --> 00:05:00.900
Yes. I love it. Let's see if it can do it with sand too.

72
00:05:01.560 --> 00:05:05.580
Here's the Ram for the training and let's try an hourglass now,

73
00:05:08.400 --> 00:05:12.240
absolute witchcraft, and we are even being paid to do this.

74
00:05:12.570 --> 00:05:13.950
I can hardly believe this.

75
00:05:14.490 --> 00:05:18.840
The reason why you see so many particles in many of these views is because if we

76
00:05:18.841 --> 00:05:19.950
look under the hood,

77
00:05:20.220 --> 00:05:25.080
we see that the paper proposes a really cool graph based method that represents

78
00:05:25.110 --> 00:05:29.850
the particles and they can pass messages to each other over these connections

79
00:05:29.851 --> 00:05:34.140
between them. This leads to a simple general and accurate model.

80
00:05:34.320 --> 00:05:37.740
That truly is a force to be reckoned with. Now,

81
00:05:37.800 --> 00:05:41.100
this is a great leap in your own network based physics simulations,

82
00:05:41.220 --> 00:05:43.320
but of course not, everything is perfect.

83
00:05:43.321 --> 00:05:47.610
Here is generalization capabilities have their limits. For instance,

84
00:05:47.700 --> 00:05:51.570
over longer timeframes solids may get incorrectly deformed.

85
00:05:53.610 --> 00:05:56.880
However, I will quietly note that during my college years,

86
00:05:57.020 --> 00:06:00.680
I was also studying the beautiful Navia Stokes equations.

87
00:06:00.830 --> 00:06:03.260
And even as a highly motivated student,

88
00:06:03.410 --> 00:06:07.940
it took several months to understand the theory and write my first flute

89
00:06:07.941 --> 00:06:08.750
simulator.

90
00:06:08.750 --> 00:06:11.930
You can check out the thesis and the source code in the video description,

91
00:06:11.960 --> 00:06:16.640
if you're interested and to see that these neural networks could learn something

92
00:06:16.670 --> 00:06:19.010
very similar in a matter of days,

93
00:06:19.400 --> 00:06:23.090
every time I think about this shivers ran down my spine.

94
00:06:23.690 --> 00:06:26.810
Absolutely amazing what a time to be alive.

95
00:06:27.320 --> 00:06:31.580
This episode has been supported by Lambda. If you're a researcher or a startup,

96
00:06:31.610 --> 00:06:34.730
looking for cheap GPU compute to run these algorithms,

97
00:06:34.910 --> 00:06:38.690
check out Lambda GPU cloud. I've talked about Lambdas,

98
00:06:38.720 --> 00:06:40.850
GPU workstations in our videos,

99
00:06:40.970 --> 00:06:45.260
and I'm happy to tell you that they are offering GPU cloud services as well.

100
00:06:45.620 --> 00:06:50.210
The Lambda GPU cloud can train image net to 93% accuracy

101
00:06:50.480 --> 00:06:52.520
for less than $19.

102
00:06:53.360 --> 00:06:57.080
Lambda's web based IDE lets you easily access your instance right in your

103
00:06:57.081 --> 00:06:57.914
browser.

104
00:06:57.950 --> 00:07:02.450
And finally hold onto your papers because the Lambda GPU cloud costs

105
00:07:02.600 --> 00:07:05.600
less than half of AWS and Azure.

106
00:07:05.960 --> 00:07:10.790
Make sure to go to Lambda labs.com/papers and sign up for

107
00:07:10.791 --> 00:07:13.100
one of their amazing GPU instances today.

108
00:07:13.460 --> 00:07:17.420
Thanks for watching and for your generous support. And I'll see you next time.

