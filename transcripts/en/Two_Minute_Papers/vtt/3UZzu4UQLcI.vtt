WEBVTT

1
00:00:00.210 --> 00:00:01.320
<v 0>Dear fellow scholars.</v>

2
00:00:01.560 --> 00:00:05.210
This is too many papers with Dr. [inaudible] neural network.

3
00:00:05.210 --> 00:00:08.270
<v 1>Based learning methods are capable of wondrous things.</v>

4
00:00:08.271 --> 00:00:10.820
These days they can do classification,

5
00:00:10.970 --> 00:00:14.930
which means that they can look at an image. And the output is a decision.

6
00:00:14.990 --> 00:00:19.820
Whether we see a dog or a cat or a sentence that describes an image

7
00:00:20.780 --> 00:00:23.990
in the case of deep minds, AI playing Atari games,

8
00:00:24.230 --> 00:00:26.510
the input is the video footage of the game.

9
00:00:26.810 --> 00:00:30.890
And the output is an action that decides what we do with our character.

10
00:00:30.920 --> 00:00:34.910
Next in open AI's amazing jukebox paper.

11
00:00:35.180 --> 00:00:36.590
The input was a star. I.

12
00:00:36.590 --> 00:00:40.100
<v 2>Love someone and music, genre and lyrics,</v>

13
00:00:40.370 --> 00:00:45.140
and the output was away form. Or in other words, a song we can listen to.

14
00:00:46.970 --> 00:00:51.410
<v 1>But a few hundred episodes ago, we covered a paper from 2015,</v>

15
00:00:51.620 --> 00:00:54.380
where scientists at deed mind as the question,

16
00:00:54.620 --> 00:00:57.380
what if we would get these neural networks to output,

17
00:00:57.560 --> 00:01:02.090
not sentences decisions, way forms or any of that sort?

18
00:01:02.540 --> 00:01:05.870
What if the output would be a computer program?

19
00:01:06.620 --> 00:01:08.990
Can we teach a neural network programming?

20
00:01:09.740 --> 00:01:14.420
I was convinced that the answer is no until I saw these results.

21
00:01:15.020 --> 00:01:16.730
So what is happening here?

22
00:01:17.210 --> 00:01:21.950
The input is a scratch pad where we are performing multi digit addition in front

23
00:01:21.951 --> 00:01:26.390
of the curious eyes of the neural network. And if it has looked for long enough,

24
00:01:26.570 --> 00:01:31.430
it was indeed able to reproduce a computer program that could eventually

25
00:01:31.640 --> 00:01:32.870
perform addition.

26
00:01:33.590 --> 00:01:38.360
It could also perform sorting and would even be able to rotate the images of

27
00:01:38.361 --> 00:01:40.340
these cars into a target position.

28
00:01:40.850 --> 00:01:43.340
It was called a neural programmer interpreter.

29
00:01:43.610 --> 00:01:46.520
And of course it was slow and a bit inefficient,

30
00:01:46.790 --> 00:01:51.500
but no matter because it could finally make something previously impossible

31
00:01:51.620 --> 00:01:54.350
possible. That is an amazing leap.

32
00:01:55.040 --> 00:01:59.300
So why are we talking about this work from 2015? Well,

33
00:01:59.360 --> 00:02:02.870
apart from the fact that there are many amazing works that are timeless,

34
00:02:03.170 --> 00:02:05.510
and this is one of them in this series,

35
00:02:05.570 --> 00:02:09.380
I always say two more papers down the line and it will be improved

36
00:02:09.410 --> 00:02:13.670
significantly. So here is the two minute papers, moment of truth.

37
00:02:14.090 --> 00:02:16.910
How has this area improved with this followup work?

38
00:02:17.510 --> 00:02:21.530
Let's have a look at this paper from scientists at Nvidia that implements a

39
00:02:21.531 --> 00:02:24.650
similar concept for computer games.

40
00:02:25.310 --> 00:02:28.820
So how is that even possible? Normally,

41
00:02:28.880 --> 00:02:33.200
if we wish to write a computer game, we first envisioned the game in our mind.

42
00:02:33.500 --> 00:02:35.750
Then we sit down and do the programming,

43
00:02:36.260 --> 00:02:38.930
but this new paper does this completely differently.

44
00:02:39.410 --> 00:02:43.490
Now hold onto your papers because this is a neural network based method.

45
00:02:43.700 --> 00:02:48.350
That first looks at someone playing the game and then it is able to

46
00:02:48.380 --> 00:02:51.470
implement the game so that it not only looks like it,

47
00:02:51.830 --> 00:02:55.310
but it also behaves the same way to our key process.

48
00:02:56.060 --> 00:02:58.490
You'll see it at work here. Yes,

49
00:02:58.520 --> 00:03:02.830
this means that we can even play with it and it learns the internal rules of the

50
00:03:02.831 --> 00:03:06.430
game and the graphics just by looking at some gameplay.

51
00:03:07.030 --> 00:03:11.410
Not that the key part here is that we are not doing any programming by hand.

52
00:03:11.740 --> 00:03:14.680
The entirety of the program is written by the AI.

53
00:03:15.250 --> 00:03:18.820
We don't need access to the source code or the internal workings of the game.

54
00:03:19.060 --> 00:03:22.150
As long as we can just look at it, it can learn the rules,

55
00:03:22.750 --> 00:03:25.150
everything truly behaves as expected.

56
00:03:25.300 --> 00:03:28.810
We can even pick up the capsule and eat the ghost as well.

57
00:03:29.530 --> 00:03:34.000
This already sounds like science fiction and we are not nearly done yet.

58
00:03:34.300 --> 00:03:38.920
There are additional goodies, it has memory and uses it consistently.

59
00:03:39.160 --> 00:03:42.430
In other words, things don't just happen. Arbitrarily.

60
00:03:42.820 --> 00:03:46.120
If we turned to a state of the game that we visited before,

61
00:03:46.330 --> 00:03:49.780
it will remember to present us with very similar information.

62
00:03:50.380 --> 00:03:55.120
It also has an understanding of foreground and background dynamic and

63
00:03:55.121 --> 00:03:59.590
static objects as well. So we can experiment with replacing these parts,

64
00:03:59.860 --> 00:04:02.110
thereby risking our games.

65
00:04:02.890 --> 00:04:05.590
It still needs quite a bit of data to perform all this.

66
00:04:05.800 --> 00:04:10.690
As it has looked at approximately 120 hours of footage of the game being

67
00:04:10.691 --> 00:04:12.130
played. However,

68
00:04:12.250 --> 00:04:16.420
now something is possible that was previously impossible.

69
00:04:16.780 --> 00:04:18.910
And of course, two more papers down the line.

70
00:04:19.090 --> 00:04:21.490
This will be improved significantly. I am sure.

71
00:04:22.060 --> 00:04:26.740
I think this work is going to be one of those important milestones that remind

72
00:04:26.741 --> 00:04:31.300
us that many of the things that we had handcrafted methods for will over

73
00:04:31.301 --> 00:04:34.120
time be replaced with these learning algorithms.

74
00:04:34.840 --> 00:04:38.350
They already know the physics of fluids, or in other words,

75
00:04:38.500 --> 00:04:43.330
they are already capable of looking at videos of these simulations and learn the

76
00:04:43.390 --> 00:04:44.830
underlying physical laws.

77
00:04:45.130 --> 00:04:49.450
And they can demonstrate having learned general knowledge of the rules by being

78
00:04:49.451 --> 00:04:51.580
able to continue these simulations.

79
00:04:51.910 --> 00:04:56.740
Even if we change the scene around quite a bit in light transport research,

80
00:04:56.830 --> 00:05:01.630
we also have decades of progress in simulating how rays of light interact with

81
00:05:01.631 --> 00:05:06.280
scenes. And we can create these beautiful images, parts of these algorithms.

82
00:05:06.340 --> 00:05:10.840
For instance, noise filtering are already taken over by AI based techniques.

83
00:05:11.140 --> 00:05:14.920
And I can't help, but feel that a bigger tidal wave is coming.

84
00:05:15.490 --> 00:05:20.140
This tidal wave will be an entirely AI driven technique that will write the

85
00:05:20.141 --> 00:05:24.820
code for the entirety of the system. Sure. The first ones will be limited.

86
00:05:25.000 --> 00:05:25.833
For instance,

87
00:05:25.870 --> 00:05:30.490
this is on your render from one of our papers that is limited to the scene and

88
00:05:30.491 --> 00:05:31.324
lighting set up.

89
00:05:31.420 --> 00:05:35.290
But you know the saying two more papers down the line and it will be an order of

90
00:05:35.291 --> 00:05:36.130
magnitude better.

91
00:05:36.850 --> 00:05:40.870
I can't wait to tell all about it to you with a video when this happens,

92
00:05:41.140 --> 00:05:45.340
make sure to subscribe and hit the bell icon to not miss any up works.

93
00:05:45.940 --> 00:05:50.020
Goodness, I love my job. What a time to be alive.

94
00:05:50.500 --> 00:05:54.580
This episode has been supported by weights and biases in this post.

95
00:05:54.700 --> 00:05:59.600
They show you how to visualize molecular structures using their system weights

96
00:05:59.601 --> 00:06:03.140
and biases provides tools to track your experiment in your deep learning

97
00:06:03.141 --> 00:06:07.520
project. Their system is designed to save you a ton of time and money,

98
00:06:07.640 --> 00:06:10.670
and it is actively used in projects as prestigious labs,

99
00:06:10.730 --> 00:06:14.720
such as open AI Toyota research GitHub and more.

100
00:06:15.080 --> 00:06:19.220
And the best part is that if you have an open source academic or personal

101
00:06:19.221 --> 00:06:24.140
project, you can use their tools for free. It really is as good as it gets.

102
00:06:24.470 --> 00:06:28.250
Make sure to visit them through wnb.com/papers,

103
00:06:28.520 --> 00:06:32.030
or just click the link in the video description to start tracking your

104
00:06:32.031 --> 00:06:33.680
experiments in five minutes,

105
00:06:34.130 --> 00:06:38.510
our thanks the weights and biases for the longstanding support and for helping

106
00:06:38.511 --> 00:06:39.980
us make better videos for you.

107
00:06:40.310 --> 00:06:44.270
Thanks for watching and for your generous support. And I'll see you next time.

