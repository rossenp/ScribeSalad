WEBVTT

1
00:00:00.180 --> 00:00:05.100
<v 0>Dear fellow scholars. This is terminate papers with Dr. Károly Zsolnai-Fehér. Today,</v>

2
00:00:05.310 --> 00:00:10.230
I will attempt to tell you a glorious tale about AI based music generation.

3
00:00:10.680 --> 00:00:14.880
You see there's no shortage of neural network based methods that can perform

4
00:00:15.000 --> 00:00:18.930
physics, simulations, style transfer, deep fakes,

5
00:00:19.260 --> 00:00:24.210
and a lot more applications where the training data is typically images or

6
00:00:24.420 --> 00:00:28.350
video. If the training data for a neural network is in pure text,

7
00:00:28.620 --> 00:00:33.060
it can learn about that. If the training data is way forms and music,

8
00:00:33.300 --> 00:00:38.070
it can learn that to wait. Really? Yes. In fact,

9
00:00:38.190 --> 00:00:42.570
let's look at two examples and then dive into today's amazing paper

10
00:00:43.080 --> 00:00:46.470
in this earlier. Work by the name. Look, listen, and learn.

11
00:00:46.890 --> 00:00:48.390
Two scientists at deep mind,

12
00:00:48.600 --> 00:00:53.580
set out to look at a large number of videos with sound you see here that

13
00:00:53.581 --> 00:00:57.510
there is a neural network for processing division and one for the audio

14
00:00:57.511 --> 00:01:01.500
information. That sounds great, but what are these heat maps?

15
00:01:01.950 --> 00:01:06.270
These were created by this learning algorithm and they show us which part of the

16
00:01:06.271 --> 00:01:10.950
image is responsible for the sounds that we hear in the video, the hotter,

17
00:01:10.951 --> 00:01:14.250
the color, the more sounds are expected from a given region.

18
00:01:14.700 --> 00:01:19.670
It was truly amazing that it didn't automatically look for humans and color

19
00:01:19.890 --> 00:01:21.060
red. In the heat map.

20
00:01:21.330 --> 00:01:25.050
There are cases where the humans are expected to be the source of the noise.

21
00:01:25.410 --> 00:01:28.950
For instance, in concerts, where in other cases,

22
00:01:29.070 --> 00:01:33.900
they don't emit any noise at all. It could successfully identify these cases.

23
00:01:34.500 --> 00:01:36.810
This still feels like science fiction to me.

24
00:01:37.230 --> 00:01:40.170
And we covered this paper in 2017,

25
00:01:40.530 --> 00:01:43.470
approximately 250 episodes ago.

26
00:01:43.920 --> 00:01:45.840
You will see that we have come a long,

27
00:01:45.841 --> 00:01:50.820
long way since we often say that these neural networks should try to embody

28
00:01:51.000 --> 00:01:54.990
general learning concepts. That's an excellent, and in this case,

29
00:01:55.170 --> 00:01:56.310
testable statement.

30
00:01:56.460 --> 00:02:00.570
So let's go ahead and have a look under the hood of these vision and audio

31
00:02:00.571 --> 00:02:05.280
processing neural networks. And yes, they are almost identical.

32
00:02:05.670 --> 00:02:09.630
Some parameters are not the same because they have been adapted to the length

33
00:02:09.780 --> 00:02:11.940
and dimensionality of the incoming data.

34
00:02:12.300 --> 00:02:16.170
But the key algorithm that we ran for the learning is the same

35
00:02:16.920 --> 00:02:18.660
later in 2018,

36
00:02:18.780 --> 00:02:23.700
DeepMind published a followup work that looks at performances on the piano

37
00:02:23.760 --> 00:02:27.420
from the masters of the past and learns to play in their style.

38
00:02:27.870 --> 00:02:32.220
A key differentiating factor here was that it did not do what most previous

39
00:02:32.221 --> 00:02:36.240
techniques do, which was looking at the score of the performance.

40
00:02:36.780 --> 00:02:41.580
These older techniques knew what to play, but not how to play these notes.

41
00:02:41.850 --> 00:02:45.060
And these are the nuances that truly make music come alive.

42
00:02:45.600 --> 00:02:50.040
This method learned from raw audio wave forms, and thus could capture much,

43
00:02:50.130 --> 00:02:54.660
much more of the artistic style, less listen to it. And in the meantime,

44
00:02:54.780 --> 00:02:58.290
you can look at the composers. It has learned from, to produce these works.

45
00:03:19.720 --> 00:03:20.553
<v 1>[Inaudible].</v>

46
00:03:23.370 --> 00:03:24.030
<v 0>However,</v>

47
00:03:24.030 --> 00:03:29.010
in 2019 open AI recognized that text-based music synthesizers

48
00:03:29.250 --> 00:03:31.230
can not only look at a piece of score,

49
00:03:31.590 --> 00:03:36.210
but can also continue it thereby composing a new piece of music.

50
00:03:36.570 --> 00:03:37.403
And what's more,

51
00:03:37.590 --> 00:03:42.180
they could even create really cool blends between genres listen,

52
00:03:42.240 --> 00:03:46.680
as their AI starts out from the first six notes of a Chapin piece and

53
00:03:46.681 --> 00:03:51.210
transitions into a pop style with a bunch of different instruments entering a

54
00:03:51.211 --> 00:03:52.080
few seconds in.

55
00:04:04.680 --> 00:04:05.513
<v 1>[Inaudible].</v>

56
00:04:06.810 --> 00:04:11.070
<v 0>Very cool. The score based techniques are a little lacking in nuance,</v>

57
00:04:11.250 --> 00:04:14.250
but can do magical genre mixing and more,

58
00:04:15.300 --> 00:04:17.640
whereas the way farm-based based techniques are more limited,

59
00:04:17.910 --> 00:04:20.820
but can create much more sophisticated music.

60
00:04:21.510 --> 00:04:24.660
Are you thinking what I am thinking? Yes,

61
00:04:24.661 --> 00:04:28.740
you have guests ride hold onto your papers because in open AI's new work,

62
00:04:28.950 --> 00:04:33.720
they tried to fuse the two concepts together. Or in other words,

63
00:04:33.960 --> 00:04:38.310
take a genre and artist and even lyrics as an input.

64
00:04:38.610 --> 00:04:40.440
And it would create a song for us.

65
00:04:40.980 --> 00:04:43.710
Let's Marvel at a few curated samples together.

66
00:04:44.100 --> 00:04:48.030
The genre artists and lyrics informations will always be on the screen.

67
00:05:21.930 --> 00:05:22.763
<v 1>[Inaudible], you know.</v>

68
00:05:34.710 --> 00:05:37.080
<v 0>Wow, I am speechless.</v>

69
00:05:37.920 --> 00:05:42.300
Love the AIB is lyrics to this has the nuance of way form based techniques

70
00:05:42.480 --> 00:05:46.890
with the versatility of score based methods. Glorious.

71
00:05:47.250 --> 00:05:48.690
If you look in the video description,

72
00:05:48.720 --> 00:05:52.290
you will find a selection of uncurated music samples as well.

73
00:05:52.860 --> 00:05:57.560
It does what it does by compressing the raw audio wave form into a compact

74
00:05:57.561 --> 00:05:59.780
representation in this space.

75
00:05:59.960 --> 00:06:04.880
It is much easier to synthesize new patterns after which we can decompress it

76
00:06:04.910 --> 00:06:06.410
to get the output wave forms.

77
00:06:08.180 --> 00:06:12.350
It has also learned to group up and cluster a selection of artists,

78
00:06:12.560 --> 00:06:15.200
which reflect how the AI thinks about them.

79
00:06:15.890 --> 00:06:19.940
There is so much cool stuff in here that it would be worthy of a video of its

80
00:06:19.941 --> 00:06:24.890
own. Not that it currently takes nine hours to generate one minute of music.

81
00:06:25.130 --> 00:06:29.360
And the network was mainly trained on Western music and only speaks English.

82
00:06:29.510 --> 00:06:31.610
But, you know, as we always see around here,

83
00:06:31.970 --> 00:06:35.420
two more papers down the line and it will be improved significantly.

84
00:06:36.110 --> 00:06:40.070
I cannot wait to report on them, should any follow up works, appear.

85
00:06:40.280 --> 00:06:43.670
So make sure to subscribe and hit the bell icon to not miss it.

86
00:06:44.090 --> 00:06:45.470
What a time to be alive.

87
00:06:46.100 --> 00:06:50.360
What you see here is an instrumentation of this exact paper we have talked

88
00:06:50.361 --> 00:06:53.060
about, which was made by weights and biases.

89
00:06:53.360 --> 00:06:58.040
I think organizing these experiments really showcases the usability of their

90
00:06:58.041 --> 00:07:02.210
system weights and biases provides tools to track your experiments in your deep

91
00:07:02.211 --> 00:07:06.590
learning project. Their system is designed to save you a ton of time and money,

92
00:07:06.770 --> 00:07:09.860
and it is actively used in projects at prestigious labs,

93
00:07:10.010 --> 00:07:14.030
such as open AI Toyota research GitHub and more.

94
00:07:14.390 --> 00:07:19.160
And the best part is that if you're an academic or have an open source project,

95
00:07:19.310 --> 00:07:23.660
you can use their tools for free. It really is as good as it gets,

96
00:07:23.990 --> 00:07:27.500
make sure to visit them through wmb.com/papers,

97
00:07:27.710 --> 00:07:30.140
or just click the link in the video description.

98
00:07:30.380 --> 00:07:34.400
And you can get the freedom or today our thanks to weights and biases for their

99
00:07:34.970 --> 00:07:38.060
longstanding support and for helping us make better videos for you.

100
00:07:38.390 --> 00:07:42.290
Thanks for watching and for your generous support. And I'll see you next time.

