WEBVTT

1
00:00:00.090 --> 00:00:01.170
<v 0>Dear fellow scholars.</v>

2
00:00:01.440 --> 00:00:04.830
This is two minute papers with Dr. Carriage on NIFA here today.

3
00:00:04.980 --> 00:00:09.570
<v 1>I am going to try to tell you the glorious tale of AI based human phase</v>

4
00:00:09.571 --> 00:00:14.550
generation and showcase an absolutely unbelievable new paper in this

5
00:00:14.551 --> 00:00:16.440
area. Early in the series,

6
00:00:16.500 --> 00:00:21.330
we covered a stunning paper that showcased a system that could not only classify

7
00:00:21.331 --> 00:00:21.990
an image,

8
00:00:21.990 --> 00:00:26.400
but write a proper sentence on what is going on and could cover

9
00:00:26.580 --> 00:00:29.970
even highly non-trivial cases. You may be surprised,

10
00:00:30.000 --> 00:00:32.250
but this thing is not recent at all.

11
00:00:32.730 --> 00:00:36.840
This is four year old news insanity. Later.

12
00:00:36.930 --> 00:00:41.310
Researchers turned this whole problem around and performed something that was

13
00:00:41.311 --> 00:00:43.560
previously thought to be impossible.

14
00:00:43.980 --> 00:00:48.570
They started using these networks to generate photorealistic images from a

15
00:00:48.571 --> 00:00:49.770
written text description.

16
00:00:50.190 --> 00:00:54.720
We could create new bird species by specifying that it should have orange legs

17
00:00:54.900 --> 00:00:56.400
and a short yellow bell.

18
00:00:56.910 --> 00:01:01.170
Then researchers at Nvidia recognized and addressed two shortcomings.

19
00:01:01.650 --> 00:01:05.040
One was that the images were not that detailed. And too,

20
00:01:05.280 --> 00:01:07.170
even though we could input text,

21
00:01:07.320 --> 00:01:12.000
we couldn't exert too much artistic control over the results in cane

22
00:01:12.030 --> 00:01:13.590
style, again, to the rescue,

23
00:01:13.680 --> 00:01:18.270
which was then able to perform both of these difficult tasks. Her really well.

24
00:01:18.810 --> 00:01:21.900
Furthermore, there are some features that are highly localized.

25
00:01:22.110 --> 00:01:24.300
As we exert control over these images,

26
00:01:24.570 --> 00:01:29.100
you can see how this part of the teeth and eyes were pinned to a particular

27
00:01:29.101 --> 00:01:33.960
location and the algorithm just refuses to let it go sometimes to the

28
00:01:33.961 --> 00:01:38.520
detriment of its surroundings, a follow-up work titled style again,

29
00:01:38.970 --> 00:01:43.440
to addresses all of these problems in one goal. So star again,

30
00:01:43.770 --> 00:01:48.600
two was able to perform near perfect synthesis of human faces. And remember,

31
00:01:48.840 --> 00:01:53.400
none of these people that you see here, you really exist quite remarkable.

32
00:01:53.820 --> 00:01:57.750
So how can we improve this magnificent technique? Well,

33
00:01:57.810 --> 00:01:59.970
this nowhere can do so many things.

34
00:02:00.150 --> 00:02:03.870
I don't even know where to start first and most important.

35
00:02:04.020 --> 00:02:06.180
We now have much more intuitive,

36
00:02:06.240 --> 00:02:10.740
artistic control over the output images we can add or remove a beard,

37
00:02:11.190 --> 00:02:15.480
make the subject younger or older, change their hairstyle,

38
00:02:16.020 --> 00:02:19.350
make their hairline recede, put a smile on their face,

39
00:02:19.410 --> 00:02:23.820
or even make their nose point here. Absolute witchcraft.

40
00:02:24.300 --> 00:02:27.270
So why can we do all this with this new method?

41
00:02:27.780 --> 00:02:32.190
The key idea is that it is not using a generative adversarial network.

42
00:02:32.490 --> 00:02:34.650
Again in short, again,

43
00:02:34.651 --> 00:02:39.450
means two competing networks where one is trained to generate new images.

44
00:02:39.690 --> 00:02:44.310
And the other one is used to tell whether degenerated images are real or fake

45
00:02:44.940 --> 00:02:49.740
gas dominated this field for a long while because of their powerful generation

46
00:02:49.741 --> 00:02:52.530
capabilities. But on the other hand,

47
00:02:52.710 --> 00:02:57.480
they are quite difficult to train and we only have limited control over its

48
00:02:57.481 --> 00:03:01.210
output among other changes. This work, this assembles,

49
00:03:01.211 --> 00:03:06.130
the generator network into F and G and the discriminator network into

50
00:03:06.190 --> 00:03:11.050
E and D or in ad-words adds an encoder and decoder

51
00:03:11.051 --> 00:03:12.010
network here.

52
00:03:13.360 --> 00:03:18.190
Why the key idea is that the encoder compresses the image data down into a

53
00:03:18.200 --> 00:03:21.160
representation that we can edit more easily.

54
00:03:21.670 --> 00:03:25.390
This is the land of beards and smiles, or in other words,

55
00:03:25.630 --> 00:03:29.260
all of these intuitive features that we can edit exist here.

56
00:03:29.620 --> 00:03:30.580
And when we are done,

57
00:03:30.730 --> 00:03:35.200
we can decompress the output with a decoder network and produce these beautiful

58
00:03:35.201 --> 00:03:38.020
images. This is already incredible,

59
00:03:38.650 --> 00:03:42.880
but what else can we do with this new architecture? A lot more,

60
00:03:43.330 --> 00:03:48.040
for instance, to, if we add a source and destination subject, their course,

61
00:03:48.160 --> 00:03:52.840
middle or fine styles can also be mixed. What does that mean?

62
00:03:52.841 --> 00:03:53.674
Exactly?

63
00:03:53.770 --> 00:03:58.720
The course part means that high level attributes like pose hairstyle and face

64
00:03:58.721 --> 00:04:02.260
shape where resembled a sore subject. In other words,

65
00:04:02.500 --> 00:04:06.910
the child will remain a child and inherit some of the properties of the

66
00:04:06.911 --> 00:04:10.060
destination people. However,

67
00:04:10.150 --> 00:04:12.430
as we transitioned to the find from source part,

68
00:04:12.700 --> 00:04:17.380
the effect of the destination subject will be stronger and the source will only

69
00:04:17.381 --> 00:04:21.730
be used to change the color scheme and micro structure of this image.

70
00:04:22.240 --> 00:04:25.810
Interestingly, it also changes the background of the subject

71
00:04:27.580 --> 00:04:30.730
three. It can also perform image interpolation.

72
00:04:31.120 --> 00:04:34.090
This means that we have these four images as starting points,

73
00:04:34.450 --> 00:04:37.480
and it can compute intermediate images between them.

74
00:04:37.900 --> 00:04:42.400
You see here that as we slowly become bill gates, somewhere along the way,

75
00:04:42.520 --> 00:04:43.540
glasses appear.

76
00:04:44.410 --> 00:04:49.030
Now note at interpolated between images is not difficult in the slightest and

77
00:04:49.031 --> 00:04:51.100
has been possible for a long, long time.

78
00:04:51.670 --> 00:04:55.570
All we need to do is just compute average results between these images.

79
00:04:56.050 --> 00:05:00.250
So what makes a good interpolation process? Well,

80
00:05:00.340 --> 00:05:04.480
we are talking about good interpolation when each of the intermediate images

81
00:05:04.481 --> 00:05:07.210
make sense and can stand on their own.

82
00:05:07.510 --> 00:05:11.050
I think this technique does amazingly well at that. Uh,

83
00:05:11.051 --> 00:05:15.670
stop the process at different places you can see for yourself and let me know in

84
00:05:15.671 --> 00:05:17.440
the comments, if you agree or not.

85
00:05:18.040 --> 00:05:21.370
I also kindly thanked the authors for creating more footage,

86
00:05:21.430 --> 00:05:26.380
just for us to showcase in this series. That is a huge honor. Thank you so much.

87
00:05:26.800 --> 00:05:31.330
And note that star GaN to appeared around December of 2019,

88
00:05:31.660 --> 00:05:36.100
and now this paper by the name adversarial late on auto and quarters

89
00:05:36.350 --> 00:05:40.690
appeared only four months later, four months later.

90
00:05:41.170 --> 00:05:45.220
My goodness, this is so much progress in so little time.

91
00:05:45.550 --> 00:05:48.970
It truly makes my head spin. What a time to be alive.

92
00:05:49.660 --> 00:05:53.890
What you see here is an instrumentation of this exact paper we have talked

93
00:05:53.891 --> 00:05:56.740
about, which was made by weights and biases.

94
00:05:57.200 --> 00:06:01.970
I think organizing these experiments really showcases the usability of their

95
00:06:01.971 --> 00:06:06.350
system weights and biases provides tools to track your experiment in your deep

96
00:06:06.351 --> 00:06:10.970
learning project. Their system is designed to save you a ton of time and money,

97
00:06:11.090 --> 00:06:14.120
and it is actively used in projects as prestigious labs,

98
00:06:14.180 --> 00:06:18.170
such as open AI Toyota research GitHub and more.

99
00:06:18.560 --> 00:06:22.700
And the best part is that if you have an open source academic or personal

100
00:06:22.701 --> 00:06:27.590
project, you can use their tools for free. It really is as good as it gets,

101
00:06:27.950 --> 00:06:31.730
make sure to visit them through wnb.com/papers,

102
00:06:32.000 --> 00:06:35.510
or just click the link in the video description to start tracking your

103
00:06:35.511 --> 00:06:37.130
experiments in five minutes,

104
00:06:37.760 --> 00:06:42.140
our thanks to weights and biases for the longstanding support and for helping us

105
00:06:42.170 --> 00:06:45.800
make better videos for you. Thanks for watching and for your generous support.

106
00:06:45.980 --> 00:06:47.780
And I'll see you next time.

