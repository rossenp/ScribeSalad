WEBVTT

1
00:00:00.440 --> 00:00:01.290
<v 0>The fellow scholars,</v>

2
00:00:01.500 --> 00:00:04.920
this is two minute papers with Dr. Carriage on IFA here today.

3
00:00:05.130 --> 00:00:09.600
A variety of techniques exist that can take an image that contains humans and

4
00:00:09.601 --> 00:00:11.520
perform Bose estimation on it.

5
00:00:11.940 --> 00:00:16.140
This gives us these interesting skeletons that show us the current posture of

6
00:00:16.141 --> 00:00:18.060
the subjects shown in these images.

7
00:00:18.480 --> 00:00:22.680
Having this skeleton opens up the possibility for many cool applications,

8
00:00:22.800 --> 00:00:23.633
for instance,

9
00:00:23.640 --> 00:00:28.620
is great for fall detection and generally many kinds of activity recognition,

10
00:00:28.830 --> 00:00:32.160
analyzing athletic performance and much, much more,

11
00:00:32.610 --> 00:00:36.690
but that would require that we can do it not only for still images,

12
00:00:36.960 --> 00:00:40.950
but for animations. Can we, yes,

13
00:00:40.980 --> 00:00:42.000
we already can.

14
00:00:42.450 --> 00:00:46.380
This is a piece of footage from a previous episode that does exactly that.

15
00:00:46.950 --> 00:00:51.360
But what if we wish for more, let's think bigger, for instance,

16
00:00:51.510 --> 00:00:54.450
can we reconstruct not only the pose of the model,

17
00:00:54.720 --> 00:00:59.250
but the entire 3d geometry of the model itself, you know,

18
00:00:59.340 --> 00:01:03.060
including the body shape, face clothes and more,

19
00:01:03.570 --> 00:01:05.820
that sounds like science fiction, right?

20
00:01:06.330 --> 00:01:08.910
Or with today's powerful learning algorithms,

21
00:01:09.150 --> 00:01:13.800
maybe it is finally a possibility who really knows let's have a look

22
00:01:13.801 --> 00:01:18.300
together and evaluate it with three increasingly more difficult experiments.

23
00:01:18.720 --> 00:01:23.190
Let's start with experiment. Number one, steal images. Nice.

24
00:01:23.610 --> 00:01:25.320
I think if I know these people,

25
00:01:25.380 --> 00:01:30.180
I might have a shot at recognizing them solely from the 3d reconstruction

26
00:01:30.720 --> 00:01:34.110
and not only that, but I also see some detail in the clothes.

27
00:01:34.500 --> 00:01:37.470
A suit can be recognized and jeans have wrinkles.

28
00:01:38.010 --> 00:01:42.720
This new method uses a different geometry representation that enables higher

29
00:01:42.721 --> 00:01:46.860
resolution outputs, and it immediately shows checkmark.

30
00:01:47.400 --> 00:01:50.400
It is clearly working quite well on still images.

31
00:01:51.060 --> 00:01:54.510
And now hold onto your papers for experiment number two,

32
00:01:54.780 --> 00:01:58.680
because it can not only deal with still images on the front side only,

33
00:01:59.010 --> 00:02:02.580
but it can also reconstruct the backside of the person.

34
00:02:03.030 --> 00:02:07.710
Look my goodness, but hold on for a second,

35
00:02:08.100 --> 00:02:13.050
that part of the data is completely unobserved. We haven't seen the backside.

36
00:02:13.530 --> 00:02:16.890
So how is that even possible? Well,

37
00:02:16.980 --> 00:02:18.840
we have to shift our thinking a little,

38
00:02:19.230 --> 00:02:22.770
an intelligent person would be able to infer some of these details.

39
00:02:22.920 --> 00:02:27.090
For instance, we know that this is a suit or that these are boots.

40
00:02:27.360 --> 00:02:30.960
And we know roughly what the backside of these objects should look like.

41
00:02:31.470 --> 00:02:35.550
This new method leans on an earlier technique by the name image to image

42
00:02:35.551 --> 00:02:40.020
translation, to estimate this data. And it truly works like magic.

43
00:02:40.530 --> 00:02:41.760
If you take a closer look,

44
00:02:41.970 --> 00:02:45.210
you'll see that we have less detail in the backside than the front,

45
00:02:45.450 --> 00:02:48.810
but the fact that we can do this is truly a miracle,

46
00:02:49.530 --> 00:02:53.790
but we can go even further. I know it is not reasonable to ask,

47
00:02:54.030 --> 00:02:58.980
but what about video reconstruction? Let's have a look don't expect miracles,

48
00:02:59.290 --> 00:03:03.310
at least not yet. There's obviously still quite a bit of flickering left,

49
00:03:03.550 --> 00:03:06.220
but the preliminary results are quite encouraging.

50
00:03:06.730 --> 00:03:10.630
And I am fairly certain that two more papers down the line and these video

51
00:03:10.631 --> 00:03:14.830
results will be nearly as good as the ones for the still images.

52
00:03:15.460 --> 00:03:19.840
The key idea here is that the new method performs these reconstructions in a way

53
00:03:20.050 --> 00:03:22.510
that is consistent. Or in other words,

54
00:03:22.720 --> 00:03:25.270
if there is a small change in the input model,

55
00:03:25.390 --> 00:03:28.210
there will also be a small change in the output model.

56
00:03:28.570 --> 00:03:32.680
This is the property that opens up the possibility to extend this method to

57
00:03:32.681 --> 00:03:36.400
videos. So how does it compare to previous methods?

58
00:03:36.820 --> 00:03:41.110
All of these competing techniques are quite recent as they are from 2019.

59
00:03:41.500 --> 00:03:43.810
They appear to be missing a lot of detail.

60
00:03:44.080 --> 00:03:48.040
And I don't think we would have a chance at recognizing the targets subject from

61
00:03:48.041 --> 00:03:53.020
the reconstructions and now just a year and a half later.

62
00:03:53.200 --> 00:03:55.180
Look at that incredible progress.

63
00:03:55.510 --> 00:03:58.600
It truly feels like we are living in a science fiction world.

64
00:03:58.960 --> 00:04:00.340
What a time to be alive.

65
00:04:00.640 --> 00:04:04.420
This episode has been supported by weights and biases in this post.

66
00:04:04.490 --> 00:04:09.100
They show you how to use their system to log and visualize these colorful masks

67
00:04:09.160 --> 00:04:11.710
for semantic segmentation. What's more,

68
00:04:11.860 --> 00:04:16.000
you can even try an example in an interactive notebook through the link in the

69
00:04:16.001 --> 00:04:16.840
video description,

70
00:04:17.020 --> 00:04:20.710
weights and biases provides tools to track your experiments in your deep

71
00:04:20.711 --> 00:04:25.300
learning projects. Their system is designed to save you a ton of time and money,

72
00:04:25.450 --> 00:04:28.630
and it is actively used in projects at prestigious labs,

73
00:04:28.750 --> 00:04:32.920
such as open AI Toyota research GitHub and more.

74
00:04:33.310 --> 00:04:37.570
And the best part is that if you have an open source academic or personal

75
00:04:37.571 --> 00:04:42.550
project, you can use their tools for free. It really is as good as it gets,

76
00:04:42.850 --> 00:04:44.110
make sure to visit them through

77
00:04:46.570 --> 00:04:50.170
and.com/papers or click the link in the video description to start tracking your

78
00:04:50.171 --> 00:04:51.940
experiments in five minutes,

79
00:04:52.540 --> 00:04:56.530
our thanks to weights and biases for their longstanding support and for helping

80
00:04:56.531 --> 00:04:58.030
us make better videos for you.

81
00:04:58.450 --> 00:05:02.380
Thanks for watching and for your generous support. And I'll see you next time.

