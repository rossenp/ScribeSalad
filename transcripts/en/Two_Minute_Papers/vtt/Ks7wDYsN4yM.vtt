WEBVTT

1
00:00:00.300 --> 00:00:01.410
<v 0>Dear fellow scholars.</v>

2
00:00:01.590 --> 00:00:05.370
This is two minute papers with Károly Zsolnai-Fehér. In computer graphics,

3
00:00:05.400 --> 00:00:07.560
when we are talking about portrait relighting,

4
00:00:07.770 --> 00:00:12.390
we mean a technique that is able to look at an image and change the lighting.

5
00:00:12.570 --> 00:00:17.400
And maybe even the materials or geometry after this image has been taken.

6
00:00:17.820 --> 00:00:19.860
This is a very challenging endeavor.

7
00:00:20.220 --> 00:00:25.170
So can Euro networks put a dent into this problem and give us something new

8
00:00:25.410 --> 00:00:27.000
and better? You bet.

9
00:00:27.360 --> 00:00:31.800
The examples that you see here are done with this new work that uses a learning

10
00:00:32.070 --> 00:00:36.750
based technique and is able to change the lighting for human portraits and only

11
00:00:36.751 --> 00:00:40.320
requires one input image. You'll see,

12
00:00:40.380 --> 00:00:43.560
normally using methods in computer graphics to relight.

13
00:00:43.561 --> 00:00:48.000
These images would require trying to find out what the geometry of the face

14
00:00:48.210 --> 00:00:50.790
materials and lighting is from the image.

15
00:00:51.180 --> 00:00:54.660
And then we can change the lighting or other parameters.

16
00:00:55.110 --> 00:00:59.310
We run a light simulation program and hope that the estimations are good enough

17
00:00:59.370 --> 00:01:01.710
to make it realistic. However,

18
00:01:01.770 --> 00:01:05.130
if we wish to use neural networks to learn the concept of portraits with

19
00:01:05.131 --> 00:01:08.820
lighting, of course, we need quite a bit of training data.

20
00:01:09.330 --> 00:01:11.310
Since this is not trivially available.

21
00:01:11.490 --> 00:01:16.320
The paper contains a new dataset with over 25,000 portrait images

22
00:01:16.470 --> 00:01:18.810
that are really lit in five different ways.

23
00:01:19.230 --> 00:01:23.370
It also proposes a neural network structure that can learn this relighting

24
00:01:23.371 --> 00:01:24.840
operation efficiently.

25
00:01:25.230 --> 00:01:29.880
It is shaped a bit like an hourglass and contains an encoder and decoder

26
00:01:29.881 --> 00:01:34.410
parts. The quarter part takes an image as an input and estimates.

27
00:01:34.530 --> 00:01:39.390
What lighting could have been used to produce it while the decoder part is

28
00:01:39.391 --> 00:01:42.030
where we can play around with changing the lighting.

29
00:01:42.300 --> 00:01:46.500
And it will generate the appropriate image that this kind of lighting would

30
00:01:46.501 --> 00:01:47.334
produce.

31
00:01:47.670 --> 00:01:52.560
What you see here are skip connections that are useful to save insights from

32
00:01:52.561 --> 00:01:56.700
different abstraction levels and transfer them from the headquarter to the

33
00:01:56.701 --> 00:01:59.820
decoder network. So what does this mean?

34
00:01:59.821 --> 00:02:04.530
Exactly intuitively it is a bit like using the lighting estimator network to

35
00:02:04.531 --> 00:02:07.320
teach the image generator, what it has learned.

36
00:02:08.250 --> 00:02:12.180
So do we really lose a lot if we skip the skip connections?

37
00:02:13.020 --> 00:02:15.450
Well, quite a bit, have a look here.

38
00:02:15.810 --> 00:02:20.070
The image on the left shows the result using all skip connections while as we

39
00:02:20.071 --> 00:02:21.180
traverse to the right,

40
00:02:21.270 --> 00:02:26.130
we see the results or meeting them these connections indeed make the profound

41
00:02:26.131 --> 00:02:26.964
difference.

42
00:02:27.420 --> 00:02:31.110
Let's be thankful for the authors of the paper as putting together such a

43
00:02:31.740 --> 00:02:35.580
dataset and trying to get an understanding as to what network architecture is.

44
00:02:35.581 --> 00:02:39.990
It would require to get great results. Like this takes quite a bit of work.

45
00:02:40.560 --> 00:02:43.980
I'd like to make a note about modeling subsurface, light transport.

46
00:02:44.280 --> 00:02:47.880
This is a piece of footage from our earlier paper that we wrote as a

47
00:02:47.881 --> 00:02:50.160
collaboration with the Activision blizzard company.

48
00:02:50.400 --> 00:02:55.080
And you can see here that including this indeed makes a profound difference

49
00:02:55.260 --> 00:02:56.760
in the looks of a human face.

50
00:02:57.450 --> 00:03:02.380
I cannot wait to see follow-up papers that take more advanced effects like this

51
00:03:02.470 --> 00:03:04.660
into consideration for relating as well.

52
00:03:05.110 --> 00:03:07.030
If you wish to find out more about this work,

53
00:03:07.180 --> 00:03:09.310
make sure to click the link in the video description.

54
00:03:09.790 --> 00:03:13.270
This episode has been supported by weights and biases here.

55
00:03:13.271 --> 00:03:14.800
You see a write-up of theirs,

56
00:03:14.950 --> 00:03:19.630
where they explain how to visualize gradients running through your models and

57
00:03:19.660 --> 00:03:23.110
illustrated through the example of predicting protein structure.

58
00:03:23.440 --> 00:03:27.880
They also have a live example that you can try weights and biases provides tools

59
00:03:27.910 --> 00:03:30.670
to track your experiments in your deep learning projects.

60
00:03:30.940 --> 00:03:35.110
It can save you a ton of time and money in these projects and is being used by

61
00:03:35.111 --> 00:03:38.770
open AI. Third, I research Stanford and Berkeley,

62
00:03:39.100 --> 00:03:42.610
make sure to visit them through wnb.com/papers,

63
00:03:42.910 --> 00:03:45.130
or just click the link in the video description.

64
00:03:45.250 --> 00:03:47.170
And you can get a free demo today.

65
00:03:47.470 --> 00:03:51.010
Our thanks to weights and biases for helping us make better videos for you.

66
00:03:51.310 --> 00:03:55.270
Thanks for watching and for your generous support. And I'll see you next time.

