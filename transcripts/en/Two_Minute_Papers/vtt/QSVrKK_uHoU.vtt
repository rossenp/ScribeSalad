WEBVTT

1
00:00:00.480 --> 00:00:04.170
<v 0>Dear fellow scholars. This is two minute papers with Dr. Károly Zsolnai-Fehér.</v>

2
00:00:04.170 --> 00:00:04.170
When we humans look at an image or a piece of video footage, we understand the geometry of the 

3
00:00:04.170 --> 00:00:04.170
objects

4
00:00:04.170 --> 00:00:04.170
 in there. So well that if we have the time and patience, we could draw a depth map that describes the distance of each object from the camera. This goes without saying, however, what does not go without saying is that if we could teach computers to do the same, we could do incredible things. For instance, this learning based technique creates 

5
00:00:04.170 --> 00:00:04.170
real-time de-focus

6
00:00:04.170 --> 00:00:04.170
 effects for virtual reality and computer games. And this one performs this Ken

7
00:00:04.170 --> 00:00:04.170
burns effect in 3d, or in other words

8
00:00:04.170 --> 00:00:04.170
, zoom and pan around in a photograph, but with a beautiful twist, because in the meantime, it also reveals the depth of the image with this data. We can even try to teach 

9
00:00:04.170 --> 00:00:04.170
self-driving

10
00:00:04.170 --> 00:00:05.003
 cars about depth perception to enhance their ability to navigate around safely.

11
00:00:59.520 --> 00:01:03.780
However, if you look here, you see two key problems, one,

12
00:01:04.080 --> 00:01:07.680
it is a little blurry and there are lots of fine details that it couldn't

13
00:01:07.681 --> 00:01:11.490
resolve, and it is flickering. In other words,

14
00:01:11.550 --> 00:01:14.670
there are abrupt changes from one image to the next one,

15
00:01:14.790 --> 00:01:19.170
which shouldn't be there as the objects in the video feed are moving smoothly,

16
00:01:19.800 --> 00:01:24.150
smooth motion should mean smooth depth maps, and it is getting there,

17
00:01:24.390 --> 00:01:26.460
but it is still not the case here.

18
00:01:27.120 --> 00:01:31.470
So I wonder if we could teach a machine to perform this task better.

19
00:01:32.070 --> 00:01:36.600
And more importantly, what new wondrous things can we do if we pull this off,

20
00:01:37.350 --> 00:01:40.980
this new technique is called consistent video depth estimation,

21
00:01:41.280 --> 00:01:45.780
and it promises smooth and detailed depth maps that are of much higher

22
00:01:45.781 --> 00:01:48.330
quality than what previous works offer.

23
00:01:49.290 --> 00:01:53.760
And now hold onto your papers because finally these maps contain enough

24
00:01:53.761 --> 00:01:58.110
detail to open up the possibility of adding new objects to the scene,

25
00:01:58.590 --> 00:02:03.180
or even flood the room with water or add many outer,

26
00:02:03.380 --> 00:02:04.920
really cool video effects.

27
00:02:05.610 --> 00:02:09.600
All of these will take the geometry of the existing real world objects,

28
00:02:09.750 --> 00:02:14.160
for instance, cats into consideration. Very cool.

29
00:02:14.940 --> 00:02:19.170
The reason why we need such a consistent technique to pull this off is because

30
00:02:19.171 --> 00:02:22.320
if we have this flickering in time that we've seen here,

31
00:02:22.860 --> 00:02:27.840
then the depth of different objects suddenly bounces around over time.

32
00:02:28.200 --> 00:02:32.430
Even for a stationary object. This means that if one frame,

33
00:02:32.610 --> 00:02:36.060
the ball would be in front of the person went in the next one,

34
00:02:36.300 --> 00:02:39.660
it would suddenly think that it has to put the ball behind them.

35
00:02:39.900 --> 00:02:44.190
And then in the next one front again, creating a not only jarring,

36
00:02:44.220 --> 00:02:46.680
but quite unconvincing animation,

37
00:02:47.220 --> 00:02:50.730
what is really remarkable is that due to the consistency of the technique,

38
00:02:50.880 --> 00:02:53.520
none of that happens here. Love it.

39
00:02:54.000 --> 00:02:57.750
Here are some more results where you can see that the outlines of the objects in

40
00:02:57.751 --> 00:03:02.560
the depth map are really crisp and follow the changes really well. Over time.

41
00:03:05.230 --> 00:03:09.850
The snowing example here is one of my favorites and it is really convincing.

42
00:03:10.810 --> 00:03:15.250
However, there are still a few spots where we can find some visual artifacts.

43
00:03:15.700 --> 00:03:17.920
For instance, as the subject is waving,

44
00:03:18.070 --> 00:03:22.000
there's lots of fine high-frequency data around the fingers there.

45
00:03:22.240 --> 00:03:24.970
And if you look at the region behind the hat closely,

46
00:03:25.360 --> 00:03:26.500
you'll find some more issues,

47
00:03:27.580 --> 00:03:31.000
or you can find that some balls are flickering on the table.

48
00:03:31.180 --> 00:03:32.680
As we move the camera around,

49
00:03:33.670 --> 00:03:38.260
compare that to previous methods that could not do nearly as good as this.

50
00:03:38.710 --> 00:03:41.560
And now we have something that is quite satisfactory.

51
00:03:41.980 --> 00:03:45.970
I can only imagine how good this will get two more papers down the line.

52
00:03:46.630 --> 00:03:47.800
And for the meantime,

53
00:03:47.860 --> 00:03:52.510
we'll be able to run these amazing effects even without having a real depth

54
00:03:52.511 --> 00:03:54.820
camera one at a time to be alive.

55
00:03:55.330 --> 00:03:59.890
This episode has been supported by weights and biases in this post late on

56
00:03:59.891 --> 00:04:04.480
space shows you how reports from weights and biases are central to

57
00:04:04.481 --> 00:04:09.070
managing their machine learning workflow and how to use their reports to quickly

58
00:04:09.071 --> 00:04:12.850
identify and debug issues with their deep learning models,

59
00:04:13.180 --> 00:04:16.870
weights and biases provides tools to track your experiments in your deep

60
00:04:16.871 --> 00:04:21.460
learning project. Their system is designed to save you a ton of time and money,

61
00:04:21.610 --> 00:04:24.790
and it is actively used in projects at prestigious labs,

62
00:04:24.910 --> 00:04:29.080
such as open AI Toyota research GitHub and more.

63
00:04:29.470 --> 00:04:33.730
And the best part is that if you have an open source academic or personal

64
00:04:33.731 --> 00:04:38.710
project, you can use their tools for free. It really is as good as it gets,

65
00:04:39.010 --> 00:04:40.270
make sure to visit them through

66
00:04:42.730 --> 00:04:46.330
wnb.com/papers or click the link in the video description to start tracking your

67
00:04:46.331 --> 00:04:48.130
experiments in five minutes,

68
00:04:48.490 --> 00:04:52.480
our thanks to weights and biases for their longstanding support and for helping

69
00:04:52.481 --> 00:04:53.980
us make better videos for you.

70
00:04:54.280 --> 00:04:58.270
Thanks for watching and for your generous support. And I'll see you next time.

