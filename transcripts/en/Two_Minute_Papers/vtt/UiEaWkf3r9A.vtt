WEBVTT

1
00:00:00.510 --> 00:00:04.290
<v 0>Your fellow scholars. This is too many papers with Dr. Cardozo NIFA here.</v>

2
00:00:05.010 --> 00:00:08.250
Star transfer is an interesting problem in machine learning research,

3
00:00:08.490 --> 00:00:12.990
where we have two input images, one for content, and one for style.

4
00:00:13.200 --> 00:00:17.130
And the output is our content image. Re-imagined with this new style,

5
00:00:17.430 --> 00:00:21.450
the cool part is that the content can be a photo straight from our camera,

6
00:00:21.780 --> 00:00:26.700
and the style can be a painting which leads to super fun and really good looking

7
00:00:26.701 --> 00:00:31.050
results. We have seen plenty of papers doing variations of style transfer,

8
00:00:31.230 --> 00:00:34.590
but I always wonder, can we push this concept further?

9
00:00:35.100 --> 00:00:37.740
And the answer is yes, for instance,

10
00:00:37.770 --> 00:00:42.360
few people know that star transfer can also be done for video. First,

11
00:00:42.361 --> 00:00:44.010
we record a video with our camera,

12
00:00:44.310 --> 00:00:49.020
then take a still image from the video and apply our artistic style to it.

13
00:00:49.560 --> 00:00:53.400
Then our style will be applied to the entirety of the video.

14
00:00:53.880 --> 00:00:57.840
The main advantage of this new method compared to previous ones is that they

15
00:00:57.870 --> 00:01:02.370
either take too long or we have to run an expensive pre-training step

16
00:01:02.940 --> 00:01:03.773
with this. No one,

17
00:01:03.810 --> 00:01:07.350
we can just start drawing and see the output results her right away,

18
00:01:07.860 --> 00:01:12.480
but it gets even better due to the interactive nature of this new technique.

19
00:01:12.690 --> 00:01:17.370
We can even do this live. All we need to do is change our input drawing,

20
00:01:17.610 --> 00:01:19.800
and it transfers the new style to the video.

21
00:01:20.070 --> 00:01:22.800
As fast as we can draw this way,

22
00:01:22.830 --> 00:01:27.780
we can refine our input style for as long as we wish or until we find

23
00:01:27.810 --> 00:01:32.130
the perfect way to stylize the video. And there is even more,

24
00:01:32.640 --> 00:01:34.230
if this works interactively,

25
00:01:34.470 --> 00:01:38.970
then it has to be able to offer an amazing workflow where we can capture a video

26
00:01:38.971 --> 00:01:43.850
of ourselves live and Mark it up as we go. Cool. Let's see.

27
00:01:48.680 --> 00:01:50.090
Wow. Just look at that.

28
00:01:50.540 --> 00:01:55.220
It is great to see that this new method also retains tamper or consistency over

29
00:01:55.221 --> 00:01:56.240
a long timeframe,

30
00:01:56.480 --> 00:02:00.470
which means that even if the marked up key frame is from a long time ago,

31
00:02:00.680 --> 00:02:04.730
it can still be applied to the video. And the outputs will show minimal

32
00:02:10.340 --> 00:02:12.890
and note that we can not only play with the colors,

33
00:02:13.070 --> 00:02:15.800
but with the geometry to look,

34
00:02:16.220 --> 00:02:20.600
we can warp the style image and it will be reflected in the output as well.

35
00:02:21.140 --> 00:02:25.370
I bet there's going to be a followup paper on more elaborate shape modifications

36
00:02:25.371 --> 00:02:26.150
as well.

37
00:02:26.150 --> 00:02:30.620
And this new work improves upon previous methods in even more areas.

38
00:02:31.130 --> 00:02:34.190
For instance, this is a method from just one year ago,

39
00:02:34.550 --> 00:02:37.970
and here you see how it struggled with contour based styles.

40
00:02:38.630 --> 00:02:40.400
Here's the key frame of the input video,

41
00:02:40.850 --> 00:02:44.540
and here's the style that we wish to apply to it later.

42
00:02:44.630 --> 00:02:48.260
This method from last year seems to lose not only the contours,

43
00:02:48.440 --> 00:02:50.810
but a lot of visual detail is also gone.

44
00:02:51.290 --> 00:02:55.520
So how did the new method do in this case? Look,

45
00:02:56.060 --> 00:02:58.310
it not only retains the contours better,

46
00:02:58.550 --> 00:03:02.670
but a lot of the sharp details remain in the outputs. Amazing.

47
00:03:03.360 --> 00:03:07.470
Now note that this technique also comes with some limitations. For instance,

48
00:03:07.471 --> 00:03:11.640
there is still some temporal flickering in the outputs and in some cases

49
00:03:11.760 --> 00:03:14.970
separating the foreground and the background is challenging,

50
00:03:15.660 --> 00:03:19.290
but really such incredible progress in just one year.

51
00:03:19.590 --> 00:03:22.620
And I can only imagine what this method will be capable of.

52
00:03:22.800 --> 00:03:26.100
Two more papers down the line, what a time to be alive,

53
00:03:26.490 --> 00:03:29.160
make sure to have a look at the paper in the video description,

54
00:03:29.340 --> 00:03:32.460
and you will see many additional details for instance,

55
00:03:32.550 --> 00:03:37.110
how you can just partially fill in some of the key frames with your style and

56
00:03:37.111 --> 00:03:38.940
still get an excellent result.

57
00:03:39.600 --> 00:03:43.440
And this episode has been supported by weights and biases in this post.

58
00:03:43.500 --> 00:03:48.180
They show you how to test and explore putting bounding boxes around objects in

59
00:03:48.181 --> 00:03:49.014
your photos,

60
00:03:49.170 --> 00:03:52.860
weights and biases provides tools to track your experiments in your deep

61
00:03:52.861 --> 00:03:57.450
learning projects. Their system is designed to save you a ton of time and money,

62
00:03:57.570 --> 00:04:00.750
and it is actively used in projects at prestigious labs,

63
00:04:00.900 --> 00:04:05.040
such as open AI Toyota research GitHub and more.

64
00:04:05.430 --> 00:04:09.690
And the best part is that if you have an open source academic or personal

65
00:04:09.691 --> 00:04:14.670
project, you can use their tools for free. It really is as good as it gets,

66
00:04:14.970 --> 00:04:16.260
make sure to visit them through

67
00:04:18.690 --> 00:04:22.320
wmb.com/papers or click the link in the video description to start tracking your

68
00:04:22.321 --> 00:04:24.090
experiments in five minutes,

69
00:04:24.660 --> 00:04:28.680
our thanks to weights and biases for their longstanding support and for helping

70
00:04:28.681 --> 00:04:30.150
us make better videos for you.

71
00:04:30.420 --> 00:04:34.350
Thanks for watching and for your generous support. And I'll see you next time.

