WEBVTT

1
00:00:00.150 --> 00:00:05.010
<v 0>Dear fellow scholars. This is two minute papers with Dr. Károly Zsolnai-Fehér. Today.</v>

2
00:00:05.070 --> 00:00:07.710
We are going to talk about image colorization.

3
00:00:08.040 --> 00:00:10.410
This is a problem where we take an old, old,

4
00:00:10.411 --> 00:00:14.730
black and white photo or run it through one of these amazing new learning based

5
00:00:14.731 --> 00:00:18.660
algorithms and outcomes, an image that is properly colored.

6
00:00:19.050 --> 00:00:22.740
The obvious application of this is of course image restoration.

7
00:00:23.190 --> 00:00:26.520
What you see here is some amazing results with this new method.

8
00:00:26.640 --> 00:00:29.100
So clearly this can be done very well.

9
00:00:29.370 --> 00:00:32.850
And you will find out in a moment how challenging this problem is,

10
00:00:33.510 --> 00:00:37.800
but there are also less obvious applications. For instance, image compression,

11
00:00:37.830 --> 00:00:42.540
to just imagine that we wouldn't have to transmit colored images over the

12
00:00:42.541 --> 00:00:45.420
internet, black and white photos would be fine.

13
00:00:45.750 --> 00:00:49.170
And if there is an AIB as algorithm in every computer,

14
00:00:49.410 --> 00:00:52.350
they would be able to restore the colors perfectly.

15
00:00:52.860 --> 00:00:57.270
This would save a lot of bandwidth and energy and it would be a glorious thing

16
00:00:57.271 --> 00:00:58.104
to do.

17
00:00:58.140 --> 00:01:03.120
So how well do the current learning based algorithms do at image colorization?

18
00:01:03.630 --> 00:01:06.570
Wow. That is a hard, hard question.

19
00:01:06.840 --> 00:01:11.820
And let's find out together why first let's have a look at the results of this

20
00:01:11.821 --> 00:01:16.050
new method that you saw so far aghast, previously existing techniques.

21
00:01:16.620 --> 00:01:18.330
Here's the black-and-white input image,

22
00:01:18.570 --> 00:01:22.500
and here's the GroundTruth output that we have concealed from the algorithms.

23
00:01:23.190 --> 00:01:24.510
Only we see the result.

24
00:01:24.720 --> 00:01:28.980
This will remain our secret and the algorithms only have access to the black and

25
00:01:29.190 --> 00:01:32.790
white input. Let's start. Yup.

26
00:01:33.120 --> 00:01:35.070
These are all learning based methods.

27
00:01:35.220 --> 00:01:40.050
So they all appear to know what a strawberry is and they color it accordingly

28
00:01:40.590 --> 00:01:44.250
so far. So good. However, this problem is much,

29
00:01:44.310 --> 00:01:48.450
much harder than just colorizing strawberries. We have grapes too.

30
00:01:48.990 --> 00:01:52.920
That does not sound like much, but there are many problems with grapes.

31
00:01:53.490 --> 00:01:54.180
For instance,

32
00:01:54.180 --> 00:01:58.860
there are many kinds of grapes and they are also translucent and therefore their

33
00:01:58.861 --> 00:02:02.070
appearance depends way more on the lighting of the scene.

34
00:02:02.760 --> 00:02:07.440
That's a problem because the algorithm not only has to know what objects are

35
00:02:07.441 --> 00:02:08.670
present in this image,

36
00:02:08.790 --> 00:02:12.840
but what the lighting around them looks like and how their material properties

37
00:02:12.990 --> 00:02:16.110
should interact with this kind of lighting. Goodness,

38
00:02:16.230 --> 00:02:18.750
that is a tremendously difficult problem.

39
00:02:19.290 --> 00:02:22.890
Previous methods did a reasonably good job at colorizing, the image,

40
00:02:23.220 --> 00:02:28.110
but the grapes remain a challenge and now hold onto your papers and

41
00:02:28.140 --> 00:02:32.820
let's see the new method. Wow, just look at that.

42
00:02:33.270 --> 00:02:37.920
The translucency of the grapes is captured beautifully and the colors are very

43
00:02:37.921 --> 00:02:41.880
close to the ground truth. But that was just one example.

44
00:02:42.240 --> 00:02:45.840
What if we generate a lot of results and show them to a few people,

45
00:02:46.140 --> 00:02:49.560
what do the users say? Let's compare the deal.

46
00:02:49.860 --> 00:02:51.390
Defy technique with this new method.

47
00:02:51.690 --> 00:02:56.310
The old defy is an interesting specimen as it combines three previously

48
00:02:56.311 --> 00:03:00.250
published papers really well, but it not a published paper.

49
00:03:00.640 --> 00:03:04.780
Fortunately it's source code is available and it is easy to compare results

50
00:03:04.810 --> 00:03:08.980
against it. So let's see how it fares against this new technique.

51
00:03:09.460 --> 00:03:12.400
Apparently they trade blows, but more often than not,

52
00:03:12.580 --> 00:03:14.740
the new method seems to come out ahead.

53
00:03:15.430 --> 00:03:20.350
Now note that we have a secret and that secret is that we can see the reference

54
00:03:20.351 --> 00:03:22.690
images, which are hidden from the algorithms.

55
00:03:23.050 --> 00:03:27.970
This helps us a great deal because we can mathematically compare the results of

56
00:03:27.971 --> 00:03:29.890
the learning algorithms to the reference.

57
00:03:30.430 --> 00:03:35.290
So what do the numbers say he up now? You'll see why I said that.

58
00:03:35.291 --> 00:03:39.760
It is very hard to tell which algorithm is the best because they all trade

59
00:03:39.761 --> 00:03:43.840
blows. And depending on how we measure the difference between two images,

60
00:03:44.110 --> 00:03:45.700
we get a different winner.

61
00:03:46.120 --> 00:03:50.860
All of this is true until we have a look at the results with this note paper,

62
00:03:51.430 --> 00:03:53.260
if you have been holding onto your papers,

63
00:03:53.290 --> 00:03:57.850
now squeeze that paper because this new method smokes the competition

64
00:03:58.090 --> 00:04:01.150
on every dataset, regardless of what we are measuring.

65
00:04:02.230 --> 00:04:06.220
So what is this black magic? What is behind this wizardry?

66
00:04:06.670 --> 00:04:10.030
This method uses an off the shelf object detection module.

67
00:04:10.240 --> 00:04:14.590
That takes the interesting elements out of the image, which are Dan colorized.

68
00:04:14.650 --> 00:04:19.180
One by one of course, this way we haven't colorized the entire image.

69
00:04:19.240 --> 00:04:21.520
So parts of it would remain black and white,

70
00:04:21.730 --> 00:04:25.480
which is clearly not what we are looking for as a remedy.

71
00:04:25.570 --> 00:04:29.680
Let's call her the entire image independently from the previous process.

72
00:04:30.310 --> 00:04:35.200
Now we have an okay quality result where the colors have reached the entirety of

73
00:04:35.201 --> 00:04:37.660
the image, but now what do we do?

74
00:04:38.140 --> 00:04:40.600
We also colorize the objects independently.

75
00:04:40.750 --> 00:04:44.440
So some things are colorized twice and they are different.

76
00:04:44.770 --> 00:04:49.510
This doesn't make any sense whatsoever until we introduce a

77
00:04:49.511 --> 00:04:54.340
novel fusion module to the process that stitches these overlapping results into

78
00:04:54.341 --> 00:04:58.420
one coherent output and the results are absolutely amazing.

79
00:04:58.930 --> 00:05:03.670
So how much do we have to wait to get all this? If we take a smallest image,

80
00:05:03.820 --> 00:05:08.800
the colorization can take place five times a second and just two more papers

81
00:05:08.801 --> 00:05:13.240
down the line. This will easily run in real time. I am sure. You know what,

82
00:05:13.450 --> 00:05:16.330
with the pace of progress in machine learning research today,

83
00:05:16.480 --> 00:05:19.390
maybe even just one more paper down the line.

84
00:05:19.930 --> 00:05:21.790
So what about the limitations?

85
00:05:22.210 --> 00:05:26.530
You remember that this contains an object detector, and if it goes haywire,

86
00:05:26.740 --> 00:05:31.180
all bets are off and we might have to revert to the only, okay,

87
00:05:31.270 --> 00:05:33.100
full image, colorization method.

88
00:05:33.550 --> 00:05:37.900
Also note that all of these comparisons showcased image colorization

89
00:05:38.230 --> 00:05:42.520
while the old defy is also capable of colorizing videos as well.

90
00:05:43.000 --> 00:05:46.810
But of course, one step at a time what a time to be alive.

91
00:05:47.200 --> 00:05:51.130
What you see here is a report of this exact paper we have talked about,

92
00:05:51.280 --> 00:05:55.210
which was made by weights and biases. I put a link to it in the description,

93
00:05:55.480 --> 00:05:56.650
make sure to have a look.

94
00:05:56.740 --> 00:06:01.400
I think it helps you understand this paper better weights and biases provides

95
00:06:01.401 --> 00:06:04.550
tools to track your experiments in your deep learning projects.

96
00:06:04.970 --> 00:06:08.300
Their system is designed to save you a ton of time and money,

97
00:06:08.450 --> 00:06:11.630
and it is actively used in projects at prestigious labs,

98
00:06:11.750 --> 00:06:15.920
such as open AI Toyota research GitHub and more.

99
00:06:16.310 --> 00:06:20.570
And the best part is that if you have an open source academic or personal

100
00:06:20.571 --> 00:06:25.550
project, you can use their tools for free. It really is as good as it gets.

101
00:06:25.850 --> 00:06:27.110
Make sure to visit them through

102
00:06:29.570 --> 00:06:33.170
wnb.com/papers or click the link in the video description to start tracking your

103
00:06:33.171 --> 00:06:34.940
experiments in five minutes,

104
00:06:35.540 --> 00:06:39.530
our thanks to weights and biases for the long-standing support and for helping

105
00:06:39.531 --> 00:06:41.030
us make better videos for you.

106
00:06:41.270 --> 00:06:45.350
Thanks for watching and for your generous support. And I'll see you next time.

