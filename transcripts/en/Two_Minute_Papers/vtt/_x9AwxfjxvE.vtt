WEBVTT

1
00:00:00.150 --> 00:00:01.260
<v 0>Dear fellow scholars.</v>

2
00:00:01.590 --> 00:00:05.910
This is two minute papers with Dr. Károly Zsolnai-Fehér. In early 2019,

3
00:00:06.150 --> 00:00:09.450
a learning based technique appeared that could perform common,

4
00:00:09.451 --> 00:00:13.770
natural language processing operations. For instance, answering questions,

5
00:00:14.100 --> 00:00:18.780
completing text, reading, comprehension, summarization, and more.

6
00:00:19.230 --> 00:00:23.580
This method was developed by scientists at open AI and they called it

7
00:00:23.670 --> 00:00:24.720
GPT two.

8
00:00:25.200 --> 00:00:29.760
The goal was to be able to perform this task with as little supervision as

9
00:00:29.761 --> 00:00:30.594
possible.

10
00:00:30.600 --> 00:00:34.110
This means that they unleashed disagree with them to read the internet.

11
00:00:34.290 --> 00:00:38.310
And the question was what would the AI learn during this process?

12
00:00:38.890 --> 00:00:42.240
That is a tricky question. And to be able to answer it,

13
00:00:42.270 --> 00:00:44.820
have a look at this paper from 2017,

14
00:00:44.910 --> 00:00:48.600
where an AI was given a bunch of Amazon product reviews,

15
00:00:48.930 --> 00:00:53.760
and the goal was to teach it to be able to generate new ones or continue a

16
00:00:53.761 --> 00:00:58.140
review. When given one, then something unexpected happened.

17
00:00:58.530 --> 00:01:03.000
The finished neural network used surprisingly few neurons to be able to continue

18
00:01:03.001 --> 00:01:03.834
these reviews.

19
00:01:03.960 --> 00:01:08.310
They noticed that the neural network has built up a knowledge of not only

20
00:01:08.311 --> 00:01:12.210
language, but also build a sentiment detector as well.

21
00:01:12.810 --> 00:01:17.010
This means that the AI recognized that in order to be able to continue a review,

22
00:01:17.280 --> 00:01:19.200
it not only needs to learn English,

23
00:01:19.470 --> 00:01:24.300
but also needs to be able to detect whether the review seems positive or

24
00:01:24.301 --> 00:01:27.450
not. If we know that we have to complete a review,

25
00:01:27.480 --> 00:01:29.730
that seems positive from a small snippet,

26
00:01:29.880 --> 00:01:32.520
we have a much easier time doing it well.

27
00:01:33.060 --> 00:01:35.280
And now back to GPT two,

28
00:01:35.730 --> 00:01:40.170
as it was asked to predict the next character in sentences of not

29
00:01:40.171 --> 00:01:44.700
reviews, but of any kind we asked what this neural network would learn.

30
00:01:45.390 --> 00:01:46.740
Well, now we know that,

31
00:01:46.741 --> 00:01:50.760
of course it learns whatever it needs to learn to perform the sentence

32
00:01:50.761 --> 00:01:53.340
completion properly and to do this,

33
00:01:53.520 --> 00:01:57.660
it needs to learn English by itself. And that's exactly what it did.

34
00:01:58.110 --> 00:02:01.620
It also learned about a lot of topics to be able to discuss them well,

35
00:02:02.010 --> 00:02:03.780
what topics let's see,

36
00:02:04.350 --> 00:02:08.670
we gave it a try and I was somewhat surprised when I saw that it was able to

37
00:02:08.671 --> 00:02:10.860
continue a two minute paper script,

38
00:02:11.130 --> 00:02:14.370
even though it seems to have turned into a history lesson.

39
00:02:15.930 --> 00:02:20.190
What was even more surprising is that it could shoulder the two minute papers

40
00:02:20.550 --> 00:02:22.260
tests. Or in other words,

41
00:02:22.500 --> 00:02:26.730
I asked it to talk about the nature of fluid simulations and it was caught

42
00:02:26.760 --> 00:02:28.290
cheating red-handed,

43
00:02:29.820 --> 00:02:34.260
but then it continued in a way that was not only coherent bad,

44
00:02:34.261 --> 00:02:35.610
had quite a bit of truth to it.

45
00:02:36.270 --> 00:02:41.100
Note that there was no explicit instruction for the AI apart from it being

46
00:02:41.160 --> 00:02:43.290
unleashed on the internet and reading it.

47
00:02:44.040 --> 00:02:48.000
And now the next version appeared by the name GPT three.

48
00:02:48.270 --> 00:02:51.450
This version is now more than a hundred times bigger.

49
00:02:51.660 --> 00:02:55.380
So our first question is how much better can an AI get?

50
00:02:55.530 --> 00:02:59.800
If we increase the size of a neural network, let's a look together.

51
00:03:00.130 --> 00:03:04.600
These are the results on a challenging reading comprehension test as a function

52
00:03:04.660 --> 00:03:06.010
of the number of parameters.

53
00:03:06.460 --> 00:03:09.340
As you see around one and a half billion parameters,

54
00:03:09.490 --> 00:03:13.540
which is roughly equivalent to GPT two, it has learned a great deal,

55
00:03:13.780 --> 00:03:18.490
but it's understanding is nowhere near the level of human comprehension.

56
00:03:19.030 --> 00:03:23.290
However, as we grow the network, something incredible happens.

57
00:03:24.490 --> 00:03:28.210
Non-trivial capabilities start to appear as we approach the hundred billion

58
00:03:28.211 --> 00:03:32.440
parameters. Look, it nearly matched the level of humans.

59
00:03:32.980 --> 00:03:35.980
My goodness, this was possible before,

60
00:03:36.130 --> 00:03:40.990
but only with neural networks that are specifically designed for a narrow task

61
00:03:41.410 --> 00:03:42.340
in comparison,

62
00:03:42.400 --> 00:03:47.290
GPT three is much more general let's test that generality and

63
00:03:47.291 --> 00:03:50.320
have a look at five practical applications together.

64
00:03:50.800 --> 00:03:55.120
One open AI made this AI accessible to a lucky few people,

65
00:03:55.390 --> 00:03:58.660
and it turns out it has read a lot of things on the internet,

66
00:03:58.810 --> 00:04:00.520
which contains a lot of code.

67
00:04:00.760 --> 00:04:04.930
So it can generate website layouts from a written description

68
00:04:07.000 --> 00:04:07.660
tool.

69
00:04:07.660 --> 00:04:12.340
It also learned how to generate properly formatted plots from a tiny prompt

70
00:04:12.520 --> 00:04:17.140
written in plain English, not just one kind many kinds,

71
00:04:18.070 --> 00:04:22.180
perhaps the joy of technical PhD students around the world. Three,

72
00:04:22.390 --> 00:04:27.310
it can properly type set mathematical equations from a plain English description

73
00:04:27.311 --> 00:04:28.144
as well. Right?

74
00:04:30.960 --> 00:04:35.310
It understands the kind of data we have in a spreadsheet in this case

75
00:04:35.490 --> 00:04:38.520
population and feels the missing parts correctly.

76
00:04:44.970 --> 00:04:45.840
And five,

77
00:04:46.110 --> 00:04:50.820
it can also translate a complex legal text into plain language or

78
00:04:50.910 --> 00:04:53.100
the other way around. In other words,

79
00:04:53.250 --> 00:04:56.730
it can also generate legal text from our simple descriptions.

80
00:05:01.410 --> 00:05:04.440
And as you see here, it can do much, much more.

81
00:05:04.680 --> 00:05:08.940
I left the link to all of these materials in the video description. However,

82
00:05:09.120 --> 00:05:13.800
of course this iteration of GPT also has its limitations. For instance,

83
00:05:13.890 --> 00:05:18.810
we haven't seen the extent to which these examples are cherry-picked or in other

84
00:05:18.811 --> 00:05:19.350
words,

85
00:05:19.350 --> 00:05:24.210
for every good output that we Marvel at there might have been one or a dozen

86
00:05:24.211 --> 00:05:27.510
tries that did not come out well. We don't exactly know,

87
00:05:28.110 --> 00:05:32.670
but the main point is that working with GPD three is a really peculiar process

88
00:05:32.700 --> 00:05:35.820
where we know that a vast body of knowledge lies within,

89
00:05:36.120 --> 00:05:40.920
but it only emerges if we can bring it out with properly written prompts,

90
00:05:41.460 --> 00:05:45.810
it almost feels like a new kind of programming that is open to everyone,

91
00:05:46.050 --> 00:05:49.500
even people without any programming or technical knowledge.

92
00:05:49.860 --> 00:05:54.480
If a computer is a bicycle for the mind, then GP T3 is a fighter jet.

93
00:05:54.990 --> 00:05:57.500
Absolutely incredible. And to that,

94
00:05:57.501 --> 00:05:59.900
the paper is vast would be an understatement.

95
00:06:00.080 --> 00:06:02.690
We only scratched the surface of what it can do here.

96
00:06:02.930 --> 00:06:05.450
So make sure to have a look. If you wish to know more about it,

97
00:06:05.690 --> 00:06:07.700
the link is available in the video description.

98
00:06:08.090 --> 00:06:12.920
I can only imagine what we will be able to do with GPT four and GPD

99
00:06:12.921 --> 00:06:16.430
five in the near future. What a time to be alive.

100
00:06:16.760 --> 00:06:21.050
What do you see here is an instrumentation for a previous paper that we covered

101
00:06:21.080 --> 00:06:24.200
in this series, which was made by weights and biases.

102
00:06:24.470 --> 00:06:28.850
I think organizing these experiments really showcases the usability of their

103
00:06:28.851 --> 00:06:33.320
system weights and biases provides tools to track your experiments in your deep

104
00:06:33.321 --> 00:06:37.910
learning project. Their system is designed to save you a ton of time and money,

105
00:06:38.060 --> 00:06:41.240
and it is actively used in projects at prestigious labs,

106
00:06:41.360 --> 00:06:45.500
such as open AI to at our research GitHub and more.

107
00:06:45.890 --> 00:06:50.180
And the best part is that if you have an open source academic or personal

108
00:06:50.181 --> 00:06:55.160
project, you can use their tools for free. It really is as good as it gets,

109
00:06:55.460 --> 00:06:56.720
make sure to visit them through

110
00:06:59.180 --> 00:07:02.780
wnb.com/papers or click the link in the video description to start tracking your

111
00:07:02.781 --> 00:07:04.550
experiments in five minutes,

112
00:07:05.150 --> 00:07:09.380
our thanks to weights and biases for the longstanding support and for helping us

113
00:07:09.440 --> 00:07:13.010
make better videos for you. Thanks for watching and for your generous support.

114
00:07:13.250 --> 00:07:14.900
And I'll see you next time.

