WEBVTT

1
00:00:00.300 --> 00:00:05.130
<v 0>Dear fellow scholars. This is too many papers with Dr. [inaudible] today.</v>

2
00:00:05.310 --> 00:00:07.950
We are going to play with a cellular automaton.

3
00:00:08.340 --> 00:00:11.400
You can imagine these automata as small games,

4
00:00:11.580 --> 00:00:16.560
where we have a bunch of cells and a set of simple rules that describe when a

5
00:00:16.561 --> 00:00:19.200
cell should be full and when it should be empty.

6
00:00:19.650 --> 00:00:23.430
These rules typically depend on the state of the neighboring cells.

7
00:00:23.790 --> 00:00:24.600
For instance,

8
00:00:24.600 --> 00:00:29.400
perhaps the most well-known form of the cellular automaton is John Horton.

9
00:00:29.401 --> 00:00:30.990
Conway's game of life,

10
00:00:31.290 --> 00:00:36.000
which simulates a tiny world where each cell represents a little life

11
00:00:36.001 --> 00:00:40.080
form the rules. Again, depend on the neighbors of this cell.

12
00:00:40.380 --> 00:00:44.250
If there are too many neighbors, they will die due to overpopulation.

13
00:00:44.640 --> 00:00:48.330
If too few, they will die due to under population.

14
00:00:48.720 --> 00:00:51.450
And if they have just the right amount of neighbors,

15
00:00:51.630 --> 00:00:56.220
they will thrive and reproduce. So why is this so interesting?

16
00:00:56.760 --> 00:00:57.270
Well,

17
00:00:57.270 --> 00:01:02.220
the cellular automaton shows us that a small set of simple rules can

18
00:01:02.221 --> 00:01:06.450
give rise to remarkably complex life forms such as gliders,

19
00:01:06.660 --> 00:01:11.190
spaceships, and even John four, nine months universal constructor,

20
00:01:11.460 --> 00:01:14.790
or in other words, self-replicating machines.

21
00:01:15.390 --> 00:01:19.050
I hope you think that's quite something. And in this paper today,

22
00:01:19.200 --> 00:01:22.800
we are going to take this concept further way further.

23
00:01:23.310 --> 00:01:28.080
This cellular automaton is programmed to evolve a single cell to grow

24
00:01:28.170 --> 00:01:31.800
into a prescribed kind of life form. Apart from that,

25
00:01:31.980 --> 00:01:34.740
there are many other key differences from other works,

26
00:01:35.010 --> 00:01:37.950
and we will highlight two of them today. One,

27
00:01:38.040 --> 00:01:42.090
the sad state is a little different because it can either be empty

28
00:01:42.420 --> 00:01:44.550
growing or mature.

29
00:01:45.300 --> 00:01:50.070
And even more importantly to the mathematical formulation of the problem

30
00:01:50.370 --> 00:01:54.690
is written in a way that is quite similar to how we train the deep neural

31
00:01:54.691 --> 00:01:58.710
network to accomplish something. This is absolutely amazing.

32
00:01:59.220 --> 00:02:03.660
Why is that? Well, because it gives rise to a highly useful feature,

33
00:02:04.050 --> 00:02:08.010
namely that we can teach it to grow these prescribed organisms,

34
00:02:08.700 --> 00:02:12.030
but weight over time, some of them seem to decay.

35
00:02:14.760 --> 00:02:19.650
Some of them can stop growing and some of them will be responsible for

36
00:02:19.651 --> 00:02:23.280
your nightmares. So from this point on proceed with care

37
00:02:25.140 --> 00:02:26.250
in the next experiment,

38
00:02:26.310 --> 00:02:30.900
the authors describe an additional step in which it can recover from these

39
00:02:30.960 --> 00:02:32.130
undesirable States.

40
00:02:32.640 --> 00:02:37.110
And now hold onto your papers because this leads to one of the major points of

41
00:02:37.111 --> 00:02:40.620
this paper. If it can recover from undesirable States,

42
00:02:40.920 --> 00:02:43.800
can it perhaps regenerate when damaged,

43
00:02:44.490 --> 00:02:49.260
well here you will see all kinds of damage and then this

44
00:02:49.261 --> 00:02:51.300
happens. Wow.

45
00:02:52.110 --> 00:02:56.700
The best part is that this thing wasn't even trained to be able to perform this

46
00:02:56.701 --> 00:02:57.810
kind of regeneration.

47
00:02:58.170 --> 00:03:02.230
The objective for training was that it should be able to perform its task of

48
00:03:02.260 --> 00:03:04.510
growing and maintaining shape.

49
00:03:04.810 --> 00:03:08.830
And it turns out some sort of regeneration is included in that

50
00:03:09.970 --> 00:03:14.320
it can also handle rotations as well, which will give rise to a lot of fun.

51
00:03:14.650 --> 00:03:16.240
And as noted a moment ago,

52
00:03:16.450 --> 00:03:21.250
some nightmarish experiments and note that this is a paper in the

53
00:03:21.640 --> 00:03:24.280
distill journal, which not only means that it is excellent,

54
00:03:24.580 --> 00:03:29.050
but also interactive. So you can run many of these experiments yourself,

55
00:03:29.320 --> 00:03:30.340
right in your browser.

56
00:03:31.180 --> 00:03:35.500
If Alexander Marvin serve the name of the first author rings a bell,

57
00:03:35.620 --> 00:03:39.610
he worked on Google's deep dreams, approximately five years ago,

58
00:03:40.240 --> 00:03:44.650
how far we have come since my goodness loving these crazy

59
00:03:45.520 --> 00:03:46.420
non-traditional research papers.

60
00:03:46.600 --> 00:03:48.640
And I'm looking forward to seeing more of these.

61
00:03:49.210 --> 00:03:53.020
This episode has been supported by weights and biases here.

62
00:03:53.080 --> 00:03:57.010
They show you how you can visualize the training process for your boosted trees

63
00:03:57.190 --> 00:04:00.730
with XG boost using that tool. If you have a closer look,

64
00:04:01.000 --> 00:04:05.590
you'll see that all you need is one line of code weights and biases provides

65
00:04:05.591 --> 00:04:08.620
tools to track your experiments in your deep learning projects.

66
00:04:08.890 --> 00:04:12.280
Their system is designed to save you a ton of time and money,

67
00:04:12.460 --> 00:04:15.760
and it is actively used in projects at prestigious labs,

68
00:04:15.940 --> 00:04:20.050
such as open AI toured our research GitHub and more.

69
00:04:20.530 --> 00:04:24.910
And the best part is that if you're an academic or have an open source project,

70
00:04:25.060 --> 00:04:29.320
you can use their tools for free. It really is as good as it gets,

71
00:04:29.710 --> 00:04:33.190
make sure to visit them through wnb.com/papers,

72
00:04:33.340 --> 00:04:35.770
or just click the link in the video description.

73
00:04:35.890 --> 00:04:37.930
And you can get a freedom all today.

74
00:04:38.380 --> 00:04:42.730
Our thanks to weights and biases for their longstanding support and for helping

75
00:04:42.731 --> 00:04:44.230
us make better videos for you.

76
00:04:44.560 --> 00:04:48.460
Thanks for watching and for your generous support. And I'll see you next time.

