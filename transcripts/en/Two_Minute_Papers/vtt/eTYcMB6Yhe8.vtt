WEBVTT

1
00:00:00.150 --> 00:00:01.230
<v 0>Dear fellow scholars.</v>

2
00:00:01.440 --> 00:00:04.040
This is two minute papers with Dr. [inaudible]. When we humans look.

3
00:00:05.780 --> 00:00:08.930
<v 1>At an image or a piece of video footage, such as this one,</v>

4
00:00:09.200 --> 00:00:13.790
we all understand that this is just a 2d projection of the world around us

5
00:00:14.150 --> 00:00:17.150
so much so that if we have the time and patience,

6
00:00:17.390 --> 00:00:22.130
we could draw a depth map that describes the distance of each object from the

7
00:00:22.131 --> 00:00:22.964
camera.

8
00:00:22.970 --> 00:00:27.140
This information is highly useful because we can use it to create real time

9
00:00:27.710 --> 00:00:30.710
defocus effects for virtual reality and computer games,

10
00:00:34.150 --> 00:00:39.040
or even perform this Canberrans effect in 3d or in

11
00:00:39.041 --> 00:00:41.890
other words, zoom and pan around in a photograph,

12
00:00:42.220 --> 00:00:45.550
but with a beautiful twist, because in the meantime,

13
00:00:45.640 --> 00:00:49.000
we can reveal the depth of the image. However,

14
00:00:49.060 --> 00:00:53.500
when we show the same images to a machine, all it sees is a bunch of numbers.

15
00:00:54.100 --> 00:00:58.210
Fortunately, with the ascendancy of neural network based learning algorithms,

16
00:00:58.360 --> 00:01:02.230
we now have a chance to do this reasonably well. For instance,

17
00:01:02.290 --> 00:01:06.100
we discussed this depth perception neural network in an earlier episode,

18
00:01:06.280 --> 00:01:09.700
which was trained using a large number of input output pairs,

19
00:01:09.940 --> 00:01:14.470
where the inputs are a bunch of images and the outputs are there corresponding

20
00:01:14.471 --> 00:01:18.220
depth maps for the neural network to learn from the authors,

21
00:01:18.250 --> 00:01:20.800
implemented this with a random scene generator,

22
00:01:21.010 --> 00:01:25.510
which creates a bunch of these crazy configurations with a lot of occlusions and

23
00:01:25.511 --> 00:01:28.750
computes via simulation, the appropriate depth map for them.

24
00:01:29.380 --> 00:01:34.180
This is what we call supervised learning because we have all these input output

25
00:01:34.181 --> 00:01:35.014
pairs.

26
00:01:35.110 --> 00:01:38.950
The solutions are given in the training set to guide the training of the neural

27
00:01:38.951 --> 00:01:43.600
network. This is supervised learning machine learning with crutches.

28
00:01:44.080 --> 00:01:47.950
We can also use these depth information to enhance the perception of

29
00:01:48.610 --> 00:01:52.570
self-driving cars, but this application is not like the previous tool.

30
00:01:52.630 --> 00:01:54.490
I just mentioned. It is much,

31
00:01:54.730 --> 00:01:58.690
much harder because in the earlier supervised learning example,

32
00:01:58.990 --> 00:02:01.690
we have trained a neural network in a simulation.

33
00:02:02.080 --> 00:02:05.500
And then we also use it later in a computer game,

34
00:02:05.770 --> 00:02:08.140
which is of course another simulation.

35
00:02:08.590 --> 00:02:12.910
We control all the variables and the environment here. However,

36
00:02:13.750 --> 00:02:16.270
self-driving cars need to be deployed in the real world.

37
00:02:16.720 --> 00:02:20.350
These cars also generate a lot of video footage with their sensors,

38
00:02:20.470 --> 00:02:24.220
which could be fed back to the neural networks as additional training data.

39
00:02:24.520 --> 00:02:27.940
If we had the depth maps for them, which of course,

40
00:02:28.240 --> 00:02:31.840
unfortunately we don't. And now with this,

41
00:02:32.020 --> 00:02:35.170
we have arrived to the concept of unsupervised learning.

42
00:02:35.650 --> 00:02:40.150
Unsupervised learning is proper machine learning where no crutches are allowed.

43
00:02:40.540 --> 00:02:44.350
We just unleash the algorithm on a bunch of data with no labels.

44
00:02:44.710 --> 00:02:48.880
And if we do it well, the neural network, we learn something useful from it.

45
00:02:49.450 --> 00:02:54.130
It is very convenient because any video we have maybe used as training data,

46
00:02:54.730 --> 00:02:58.090
that would be great, but we have a tiny problem.

47
00:02:58.390 --> 00:03:01.780
And that tiny is that this sounds impossible,

48
00:03:02.500 --> 00:03:06.640
or it may have sounded impossible until this paper appeared.

49
00:03:07.210 --> 00:03:10.990
This work promises us no less than unsupervised depth.

50
00:03:10.991 --> 00:03:14.410
Learning from videos. Since this is unsupervised,

51
00:03:14.590 --> 00:03:16.210
it means that during training,

52
00:03:16.450 --> 00:03:21.220
all it sees is unlabeled videos from different viewpoints and somehow

53
00:03:21.370 --> 00:03:24.580
figures out a way to create these depth maps from it.

54
00:03:25.000 --> 00:03:27.880
So how is this even possible? Well,

55
00:03:27.881 --> 00:03:31.660
it is possible by adding just one ingenious idea.

56
00:03:32.050 --> 00:03:34.390
The idea is that since we don't have the labels,

57
00:03:34.540 --> 00:03:36.970
we can't teach the algorithm how to be right,

58
00:03:37.360 --> 00:03:40.450
but instead we can teach it to be consistent.

59
00:03:41.050 --> 00:03:43.930
That doesn't sound like much does it? Well,

60
00:03:43.990 --> 00:03:47.830
it makes all the difference because if we ask the algorithm to be consistent,

61
00:03:48.100 --> 00:03:53.080
it will find out that a good way to be consistent is to be right while we

62
00:03:53.081 --> 00:03:55.390
are looking at some results to make this clearer.

63
00:03:55.600 --> 00:04:00.580
Let me add one more real world example that demonstrates how cool this idea is.

64
00:04:01.060 --> 00:04:05.560
Imagine that you are a university professor overseeing an exam in mathematics,

65
00:04:05.770 --> 00:04:08.680
and someone tells you that for one of the problems,

66
00:04:08.800 --> 00:04:12.700
most of the students gave the same answer. If this is the case,

67
00:04:12.910 --> 00:04:15.460
there's a good chance that this was the right answer.

68
00:04:15.940 --> 00:04:18.670
It is not a hundred percent chance that this is the case,

69
00:04:18.940 --> 00:04:21.430
but if most of the students have the same answer,

70
00:04:21.700 --> 00:04:25.510
it is much more unlikely that they have all failed the same way.

71
00:04:25.900 --> 00:04:29.650
There are many different ways to fail, but there's only one way to succeed.

72
00:04:30.040 --> 00:04:32.110
Therefore, if there is consistency,

73
00:04:32.410 --> 00:04:37.300
often there is success and the simple but powerful thought leads

74
00:04:37.301 --> 00:04:41.200
to far-reaching conclusions. Let's have a look at some more results.

75
00:04:43.270 --> 00:04:48.040
Whew. Now this is something, let me explain who I am so excited for this.

76
00:04:48.490 --> 00:04:49.900
This is the input image,

77
00:04:50.110 --> 00:04:54.520
and this is the perfect depth map that is concealed from our beloved algorithm

78
00:04:54.820 --> 00:04:58.480
and is there for us to be able to evaluate its performance.

79
00:04:59.050 --> 00:05:02.170
These are two previous works, both use crutches.

80
00:05:02.500 --> 00:05:06.400
The first was trained via supervised learning by showing it input,

81
00:05:06.430 --> 00:05:08.560
output image pairs with depth maps.

82
00:05:08.890 --> 00:05:13.660
And it does reasonably well while the other one gets even less supervision.

83
00:05:14.020 --> 00:05:17.530
The worst crutch, if you will. And it came up with this.

84
00:05:18.490 --> 00:05:23.230
Now the answer provides no technique was not given any crutches and came

85
00:05:23.231 --> 00:05:26.680
up with this holy mother of papers.

86
00:05:26.980 --> 00:05:28.900
It looks like a somewhat coarser,

87
00:05:29.140 --> 00:05:33.940
but still very accurate version of the true depth maps. So what are, you know,

88
00:05:34.150 --> 00:05:38.770
this neural network based method looks at unlabeled videos and finds a way to

89
00:05:38.771 --> 00:05:43.420
create depth maps by not trying to be right, but trying to be consistent.

90
00:05:44.260 --> 00:05:47.590
This is one of those amazing papers where one simple,

91
00:05:47.770 --> 00:05:52.480
brilliant idea can change everything and make the impossible possible.

92
00:05:52.930 --> 00:05:54.370
What a time to be alive.

93
00:05:54.910 --> 00:05:58.700
What you see here is an instrumentation of these deaf learning paper.

94
00:05:58.701 --> 00:06:01.850
We have talked about. This was made by weights and biases.

95
00:06:02.090 --> 00:06:06.530
I think organizing these experiments really showcases the usability of their

96
00:06:06.531 --> 00:06:07.364
system.

97
00:06:07.460 --> 00:06:11.900
Also weights and biases provides tools to track your experiments in your deep

98
00:06:11.901 --> 00:06:16.520
learning project. Their system is designed to save you a ton of time and money,

99
00:06:16.760 --> 00:06:19.940
and it is actively used in projects at prestigious labs,

100
00:06:20.180 --> 00:06:24.260
such as open AI to auto research GitHub and more.

101
00:06:24.740 --> 00:06:29.270
And the best part is that if you're an academic or have an open source project,

102
00:06:29.510 --> 00:06:33.590
you can use their tools for free. It is really as good as it gets,

103
00:06:33.950 --> 00:06:37.460
make sure to visit them through wnb.com/papers,

104
00:06:37.820 --> 00:06:40.220
or just click the link in the video description.

105
00:06:40.430 --> 00:06:42.680
And you can get a freedom all today.

106
00:06:43.280 --> 00:06:47.660
Our thanks to weights and biases for the longstanding support and for helping us

107
00:06:47.720 --> 00:06:51.290
make better videos for you. Thanks for watching and for your generous support.

108
00:06:51.470 --> 00:06:53.120
And I'll see you next time.

