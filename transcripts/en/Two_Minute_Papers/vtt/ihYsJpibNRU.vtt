WEBVTT

1
00:00:00.450 --> 00:00:04.190
<v 0>The fellow scholars, this is two minute papers with Dr. [inaudible]</v>

2
00:00:04.190 --> 00:00:04.190
. When I was a bachelor student and took

3
00:00:04.190 --> 00:00:04.190
 on my first bigger undertaking in computer graphics in 2011, it was a research project for a feature length movie where the goal was to be able to learn the 

4
00:00:04.190 --> 00:00:04.190
brush strokes of an artist. You see the sample brush stroke

5
00:00:04.190 --> 00:00:04.190
 here, and what it could do is change

6
00:00:04.190 --> 00:00:04.190
 the silhouette of a digital 3d object to appear as if it were drawn with this

7
00:00:04.190 --> 00:00:05.023
 style. This way, we could use an immense amount of perfectly model geometry and make them look as if they were drawn by an artist. The project was a combination of machine learning and computer graphics, and got me hooked on this topic for life. So this was about silhouettes, but what about being able to change the lighting to address this problem? This new work promises, something that sounds like science fiction.

8
00:00:55.830 --> 00:01:00.270
The input is a painting, which is thought of as a collection of brushstrokes.

9
00:01:00.600 --> 00:01:01.380
First,

10
00:01:01.380 --> 00:01:05.880
the algorithm is trying to break down the image into these individual strokes

11
00:01:06.390 --> 00:01:10.320
here on the left with a, you see the painting itself.

12
00:01:10.590 --> 00:01:14.610
And B is the real collection of strokes that were used to create it.

13
00:01:15.000 --> 00:01:17.640
This is what the algorithm is trying to estimate it with.

14
00:01:18.870 --> 00:01:22.830
And this colorful image visualizes the difference between the two,

15
00:01:23.280 --> 00:01:27.600
the blue color denotes regions, where these brush strokes are estimated well.

16
00:01:27.930 --> 00:01:32.430
And we can find more differences as we transition into the red colored regions.

17
00:01:33.150 --> 00:01:36.990
So great. Now that we have a bunch of these brush strokes,

18
00:01:37.290 --> 00:01:39.810
but what do we do with them? Well,

19
00:01:39.870 --> 00:01:42.660
let's add one more assumption into the system,

20
00:01:42.840 --> 00:01:47.820
which is that the densely packed regions are going to be more effected by the

21
00:01:47.821 --> 00:01:52.110
lighting effects while the sparser regions will be less impacted

22
00:01:52.620 --> 00:01:53.310
this way,

23
00:01:53.310 --> 00:01:58.050
we can make the painting change as if we were to move our imaginary light source

24
00:01:58.051 --> 00:02:01.950
around no painting skills or manual labor required.

25
00:02:02.580 --> 00:02:03.413
Wonderful.

26
00:02:03.960 --> 00:02:08.370
But some of the skeptical fellow scholars out there would immediately ask the

27
00:02:08.371 --> 00:02:10.140
question. It looks great,

28
00:02:10.440 --> 00:02:14.970
but how do we know if this really is good enough to be used in

29
00:02:14.971 --> 00:02:17.670
practice? The authors thought of that too,

30
00:02:17.970 --> 00:02:22.950
and asked an artist to create some of these views by hand and what do you know?

31
00:02:23.070 --> 00:02:24.510
They are extremely good,

32
00:02:25.170 --> 00:02:29.130
very close to the real deal and all this comes for free

33
00:02:29.730 --> 00:02:30.563
insanity.

34
00:02:31.050 --> 00:02:35.430
Now we noted that the input for this algorithm is just one image.

35
00:02:35.910 --> 00:02:40.590
So what about a cheeky experiment where we would add not a painting,

36
00:02:40.890 --> 00:02:44.100
but a photo and pretend that it is a painting.

37
00:02:44.520 --> 00:02:48.780
Can it relight it properly? Well, hold onto your papers.

38
00:02:48.900 --> 00:02:51.240
And let's have a look. Here's the photo,

39
00:02:52.410 --> 00:02:54.270
the breakdown of the brush strokes.

40
00:02:54.390 --> 00:02:58.770
If this were a painting and wow,

41
00:02:59.260 --> 00:03:03.490
here are the lighting effects it worked.

42
00:03:04.200 --> 00:03:06.810
And if you enjoy these results and would like to see more,

43
00:03:07.050 --> 00:03:10.350
make sure to have a look at this beautiful paper in the video description.

44
00:03:12.930 --> 00:03:16.770
For instance, here, you see a comparison against previous works,

45
00:03:17.130 --> 00:03:21.480
and it seems quite clear that it smokes the competition on a variety of test

46
00:03:21.490 --> 00:03:26.070
cases and these papers they are comparing to are also quite recent.

47
00:03:26.400 --> 00:03:30.690
The pace of progress in computer graphics research is absolutely incredible.

48
00:03:31.140 --> 00:03:35.760
More on that in a moment. Also just look at the information density here.

49
00:03:36.120 --> 00:03:40.440
This tiny diagram shows you exactly where the light source positions are.

50
00:03:40.710 --> 00:03:45.180
I remember looking at a paper on a similar topic that did not have this thing,

51
00:03:45.390 --> 00:03:48.180
and it made the entirety of the work, a great deal,

52
00:03:48.181 --> 00:03:50.190
more challenging to evaluate properly.

53
00:03:50.640 --> 00:03:54.060
This kind of attention to detail might seem like a small thing,

54
00:03:54.270 --> 00:03:57.150
but it makes all the difference for a great paper,

55
00:03:57.480 --> 00:04:02.430
which this one is the provided user studies shows that these outputs can

56
00:04:02.431 --> 00:04:07.050
be generated within a matter of seconds and reinforces our hunch that most

57
00:04:07.051 --> 00:04:10.770
people prefer the outputs of the new technique to the previous ones.

58
00:04:11.280 --> 00:04:15.210
So much improvement in so little time with this,

59
00:04:15.270 --> 00:04:19.980
we can now create digital lighting effects from a single image for paintings

60
00:04:20.370 --> 00:04:23.760
and even photographs in a matter of seconds,

61
00:04:24.180 --> 00:04:25.620
what a time to be alive.

62
00:04:26.070 --> 00:04:30.180
What you see here is an instrumentation of this exact paper we have talked

63
00:04:30.181 --> 00:04:32.940
about, which was made by weights and biases.

64
00:04:33.270 --> 00:04:37.860
I think organizing these experiments really showcases the usability of their

65
00:04:37.861 --> 00:04:42.210
system weights and biases provides tools to track your experiments in your deep

66
00:04:42.211 --> 00:04:46.590
learning project. Their system is designed to save you a ton of time and money,

67
00:04:46.770 --> 00:04:49.890
and it is actively used in projects at prestigious labs,

68
00:04:50.010 --> 00:04:54.030
such as open AI Toyota research GitHub and more.

69
00:04:54.420 --> 00:04:59.190
And the best part is that if you're an academic or have an open source project,

70
00:04:59.310 --> 00:05:03.660
you can use their tools for free. It really is as good as it gets,

71
00:05:03.990 --> 00:05:07.530
make sure to visit them through wnb.com/papers,

72
00:05:07.710 --> 00:05:10.140
or just click the link in the video description.

73
00:05:10.380 --> 00:05:12.210
And you can get a free demo today.

74
00:05:12.750 --> 00:05:16.590
Our thanks to weights and biases for their longstanding support and for helping

75
00:05:16.591 --> 00:05:18.090
us make better videos for you.

76
00:05:18.420 --> 00:05:22.500
Thanks for watching and for your generous support. And I'll see you next time.

