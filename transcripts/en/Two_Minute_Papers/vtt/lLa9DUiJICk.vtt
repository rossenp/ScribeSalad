WEBVTT

1
00:00:07.040 --> 00:00:10.880
<v 0>The fellow scholars hold onto your papers because we have an emergency</v>

2
00:00:10.881 --> 00:00:11.714
situation.

3
00:00:11.960 --> 00:00:16.790
Everybody can make defects now by recording a voice sample such as this one and

4
00:00:16.791 --> 00:00:21.170
the lips of the target subject will move as if they themselves were saying this

5
00:00:21.890 --> 00:00:22.970
not bad, huh?

6
00:00:24.140 --> 00:00:27.980
And now let's see what else this new method can do. First.

7
00:00:28.100 --> 00:00:32.540
Let's watch this short clip of a speech and make sure to pay attention to the

8
00:00:32.541 --> 00:00:37.220
fact that the louder voice is the English translator. And if you pay attention,

9
00:00:37.370 --> 00:00:41.760
you can hear the chancellor's original voice in the background, too. Work.

10
00:00:41.760 --> 00:00:44.700
<v 1>For our future corporation. I nerd</v>

11
00:00:46.800 --> 00:00:51.780
only eight months ago you were awarded the part.

12
00:00:52.410 --> 00:00:57.060
<v 0>So what is the problem here? Strictly speaking, there is no problem here.</v>

13
00:00:57.270 --> 00:01:00.270
This is just the way the speech was recorded. However,

14
00:01:00.570 --> 00:01:05.040
what if we could recreate this video in a way that the chancellor's lips would

15
00:01:05.041 --> 00:01:09.600
be sinked, not to her own voice, but to the voice of the English interpreter.

16
00:01:10.050 --> 00:01:14.520
This would give an impression as if the speech was given in English and the

17
00:01:14.521 --> 00:01:17.460
video content would follow what we hear now.

18
00:01:17.461 --> 00:01:20.730
That sounds like something straight out of a science fiction movie,

19
00:01:20.940 --> 00:01:24.180
perhaps even with today's advanced machine learning techniques,

20
00:01:24.480 --> 00:01:26.550
but let's see if it's possible.

21
00:01:27.030 --> 00:01:31.050
This is a state of the art technique from last year that attempts to perform

22
00:01:31.051 --> 00:01:31.884
this.

23
00:01:32.880 --> 00:01:34.860
<v 1>Um, development and a very different history.</v>

24
00:01:35.610 --> 00:01:39.840
We signed this on the 50th sixth anniversary of the LSA treaty of

25
00:01:39.841 --> 00:01:41.070
1960.

26
00:01:42.420 --> 00:01:44.670
<v 0>Mm. There are extraneous lip movements,</v>

27
00:01:44.700 --> 00:01:49.170
which are the remnants of the original video so much so that she seems to be

28
00:01:49.171 --> 00:01:53.370
giving two speeches at the same time, not too convincing.

29
00:01:53.940 --> 00:01:58.830
So is this not possible to pull off well now hold onto

30
00:01:58.831 --> 00:02:03.720
your papers and let's see how this new paper does at the same problem in.

31
00:02:03.720 --> 00:02:07.920
<v 1>Germany, but also for a starting point of a very different, um,</v>

32
00:02:07.980 --> 00:02:09.810
development and a very different history.

33
00:02:10.560 --> 00:02:15.060
We signed this on the 50th sixth anniversary of the Elysee treaty of 19.

34
00:02:15.300 --> 00:02:17.790
<v 0>Wow. Now that's significantly better.</v>

35
00:02:18.120 --> 00:02:22.050
The remnants of the previous speech are still there, but the footage is much,

36
00:02:22.140 --> 00:02:23.310
much more convincing.

37
00:02:23.760 --> 00:02:27.750
What's even better is that the previous technique was published just one year

38
00:02:27.751 --> 00:02:32.640
ago by the same research group, such a great leap in just one year.

39
00:02:32.910 --> 00:02:36.540
My goodness. So apparently this is possible,

40
00:02:36.960 --> 00:02:39.870
but I would like to see another example just to make sure.

41
00:02:40.740 --> 00:02:42.780
<v 2>Well, he'd like to on the first issue,</v>

42
00:02:42.781 --> 00:02:45.750
it's not for me to comment on the visits of Madame.

43
00:02:46.860 --> 00:02:50.610
<v 0>Checkmark so far, this is an amazing leap, but believe it or not,</v>

44
00:02:50.970 --> 00:02:54.720
this is just one of the easier applications of the new model.

45
00:02:55.050 --> 00:02:58.650
So let's see what else it can do. For instance,

46
00:02:58.710 --> 00:03:02.410
many of us are sitting at home yearning for some learning materials,

47
00:03:02.560 --> 00:03:06.820
but the vast majority of these were recorded in only one language.

48
00:03:07.270 --> 00:03:12.090
What if we could read up famous lectures into many other languages, it'd.

49
00:03:12.090 --> 00:03:16.660
<v 3>Be their computer of his [inaudible]</v>

50
00:03:21.720 --> 00:03:22.553
[inaudible].

51
00:03:22.590 --> 00:03:23.423
<v 0>Look at that.</v>

52
00:03:23.610 --> 00:03:27.870
Any lecture could be available in any language and look as if they were

53
00:03:27.960 --> 00:03:30.570
originally recorded in these foreign languages.

54
00:03:30.810 --> 00:03:32.850
As long as someone says the words,

55
00:03:33.150 --> 00:03:37.290
which can also be kind of automated through speech synthesis these days.

56
00:03:38.190 --> 00:03:40.710
So it clearly works on real characters,

57
00:03:40.980 --> 00:03:44.640
but are you thinking what I am thinking? Three.

58
00:03:44.880 --> 00:03:47.550
What about lip-syncing animated characters?

59
00:03:48.180 --> 00:03:51.150
Imagine if a line has to be changed in a Disney movie,

60
00:03:51.390 --> 00:03:56.310
can we synthesize new video footage without calling in the animators for yet

61
00:03:56.370 --> 00:04:00.600
another all-nighter let's give it a try. I'll go around.

62
00:04:00.870 --> 00:04:04.380
Wait for my call. Indeed. We can loving it.

63
00:04:05.220 --> 00:04:07.140
Let's do one more for,

64
00:04:07.290 --> 00:04:10.170
of course we have a lot of these meme gifs on the internet.

65
00:04:10.530 --> 00:04:14.070
What about redoubling those with an arbitrary line of our choice?

66
00:04:16.600 --> 00:04:19.720
Yup. That is indeed also possible. Well done.

67
00:04:20.410 --> 00:04:22.570
And imagine that this is such a leap.

68
00:04:22.690 --> 00:04:26.230
Just one more work down the line from the 2019 paper.

69
00:04:26.530 --> 00:04:30.640
I can only imagine what results we will see. One more paper down the line.

70
00:04:31.240 --> 00:04:33.400
It not only does what it does better,

71
00:04:33.550 --> 00:04:38.170
but it can also be applied to a multitude of problems. What a time to be alive.

72
00:04:38.770 --> 00:04:40.030
When we look under the hood,

73
00:04:40.180 --> 00:04:45.040
we see that the two key components that enable this wizardry or here and here.

74
00:04:45.610 --> 00:04:47.560
So what does this mean? Exactly?

75
00:04:48.100 --> 00:04:52.600
It means that we jointly improve the quality of the lip sinking and the visual

76
00:04:52.601 --> 00:04:53.830
quality of the video.

77
00:04:54.220 --> 00:04:58.210
These two modules curate the results offered by the main generator and neural

78
00:04:58.211 --> 00:05:03.100
network and reject solutions that don't have enough detail or don't

79
00:05:03.101 --> 00:05:07.510
match the speech that we hear and thereby they steer it towards much higher

80
00:05:07.511 --> 00:05:08.530
quality solutions.

81
00:05:08.980 --> 00:05:12.880
If we continue this training process for 29 hours for the lip sing

82
00:05:12.881 --> 00:05:15.970
discriminator, we get these incredible results.

83
00:05:16.360 --> 00:05:21.340
Now let's have a quick look at the user study and humans appeared to

84
00:05:21.341 --> 00:05:24.970
almost never prefer the older method compared to this one.

85
00:05:25.690 --> 00:05:29.320
I tend to agree if you consider these forgeries to be defects,

86
00:05:29.410 --> 00:05:30.520
then there you go.

87
00:05:30.760 --> 00:05:34.240
Useful defects that can potentially help people around the world,

88
00:05:34.390 --> 00:05:37.480
stranded at home to study and improve themselves.

89
00:05:37.930 --> 00:05:41.140
Imagine what good this could do well done.

90
00:05:42.040 --> 00:05:45.370
This episode has been supported by Lambda GPU cloud.

91
00:05:45.580 --> 00:05:48.970
If you're looking for inexpensive cloud GPU's for AI,

92
00:05:49.150 --> 00:05:51.280
check out Lambda GPU cloud,

93
00:05:51.700 --> 00:05:55.330
they recently launched Quadro RTX 6,000 RTX,

94
00:05:55.390 --> 00:06:00.110
8,000 and V 100 instances and hold onto your papers

95
00:06:00.200 --> 00:06:04.820
because Lambda GPU cloud can cost less than half of AWS

96
00:06:04.970 --> 00:06:05.803
and Azure.

97
00:06:05.930 --> 00:06:10.160
Plus they are the only cloud service with 48 gigabyte RDX,

98
00:06:10.220 --> 00:06:14.840
eight thousands joined researchers at organizations like apple, MIT,

99
00:06:14.930 --> 00:06:19.310
and Caltech in using Lambda cloud instances, workstations or servers,

100
00:06:19.520 --> 00:06:24.260
make sure to go to Lambda labs.com/papers to sign up for one of their

101
00:06:24.261 --> 00:06:26.090
amazing GPU instances today.

102
00:06:26.360 --> 00:06:30.650
Our thanks to Lambda for their longstanding support and for helping us make

103
00:06:30.651 --> 00:06:33.920
better videos for you. Thanks for watching and for your generous support.

104
00:06:34.130 --> 00:06:35.780
And I'll see you next time.

