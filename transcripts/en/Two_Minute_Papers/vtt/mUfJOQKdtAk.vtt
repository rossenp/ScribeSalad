WEBVTT

1
00:00:08.090 --> 00:00:11.390
<v 0>Dear fellow scholars. This is two minute papers with Dr. Carol Jonah.</v>

2
00:00:11.391 --> 00:00:15.590
If I hear it is important for you to know that everybody can make the fix.

3
00:00:15.591 --> 00:00:17.750
Now you can turn your head around.

4
00:00:19.460 --> 00:00:21.290
Mouth movements are looking great.

5
00:00:21.680 --> 00:00:25.340
And eye movements are also translated into the target footage.

6
00:00:27.050 --> 00:00:30.440
And of course, as we always say, two more papers down the line,

7
00:00:30.500 --> 00:00:33.020
and it will be even better and cheaper. Dennis.

8
00:00:35.060 --> 00:00:35.780
<v 1>As you see,</v>

9
00:00:35.780 --> 00:00:40.490
some papers are so well done and are so clear that they just speak for

10
00:00:40.491 --> 00:00:44.090
themselves. This is one of them to use this technique.

11
00:00:44.330 --> 00:00:47.000
All you need to do is record a video of yourself,

12
00:00:47.480 --> 00:00:52.460
add just one image of the target subject around this learning based algorithm.

13
00:00:52.760 --> 00:00:56.000
And there you go. If you stay until the end of this video,

14
00:00:56.060 --> 00:01:00.830
you will see even more people introducing themselves as me as

15
00:01:00.831 --> 00:01:03.950
noted. Many important gestures are being translated,

16
00:01:04.040 --> 00:01:06.890
such as head mouth and eye movement,

17
00:01:07.160 --> 00:01:11.090
but what's even better is that even full body movement works.

18
00:01:12.950 --> 00:01:15.320
Absolutely incredible. Now,

19
00:01:15.350 --> 00:01:18.410
there are plenty of techniques out there that can create defects.

20
00:01:18.710 --> 00:01:23.690
Many of which we have talked about in this series. So what sets this one apart?

21
00:01:24.350 --> 00:01:28.970
Well, one most previous algorithms required additional information.

22
00:01:29.090 --> 00:01:33.710
For instance, facial landmarks or the pose estimation of the target subject.

23
00:01:34.130 --> 00:01:37.850
This one requires no knowledge of the image. As a result,

24
00:01:37.940 --> 00:01:40.340
this technique becomes so much more general.

25
00:01:40.670 --> 00:01:45.050
We can create high quality defects with just one photo of the target subject,

26
00:01:45.530 --> 00:01:48.020
make ourselves danced like a professional,

27
00:01:48.350 --> 00:01:52.850
and what's more hold onto your papers because it also works on non

28
00:01:52.851 --> 00:01:54.800
humanoid and cartoon models.

29
00:01:55.160 --> 00:01:59.960
And even that's not all we can even synthesize an animation of a robot arm

30
00:02:00.170 --> 00:02:02.810
by using another one as a driving sequence.

31
00:02:04.460 --> 00:02:08.510
So why is it that it doesn't need all this additional information?

32
00:02:09.140 --> 00:02:10.760
Well, if we look under the hood,

33
00:02:10.970 --> 00:02:14.960
we see that it is a neural network based method that generates all this

34
00:02:14.961 --> 00:02:16.730
information by itself.

35
00:02:17.270 --> 00:02:21.110
It identifies what kind of movements and transformations are taking place.

36
00:02:21.111 --> 00:02:24.740
In our driving video. You can see that the learned key points here,

37
00:02:24.800 --> 00:02:27.230
follow the motion of the videos really well.

38
00:02:30.200 --> 00:02:35.180
Now we pack up all this information and send it over to the generator to work

39
00:02:35.210 --> 00:02:36.920
the target image appropriately,

40
00:02:37.190 --> 00:02:40.640
taking into consideration possible occlusions that may occur.

41
00:02:41.240 --> 00:02:46.100
This means that some parts of the image may now be uncovered where we don't know

42
00:02:46.160 --> 00:02:47.480
what the background should look like.

43
00:02:47.840 --> 00:02:51.530
Normally we will do this by hand with an image in painting technique.

44
00:02:51.800 --> 00:02:56.030
For instance, you see the legendary patch match algorithm here that does it.

45
00:02:56.360 --> 00:02:57.950
However, in this case,

46
00:02:58.100 --> 00:03:01.630
the neural network does it automatically by itself.

47
00:03:02.140 --> 00:03:04.090
If you are seeking for flaws in the output,

48
00:03:04.270 --> 00:03:06.370
these will be important regions to look at.

49
00:03:07.090 --> 00:03:10.720
And it not only requires less information than previous techniques,

50
00:03:10.990 --> 00:03:14.290
but it also outperforms them significantly.

51
00:03:16.390 --> 00:03:19.330
Yes, there is still room to improve this. For instance,

52
00:03:19.570 --> 00:03:23.890
the sudden head rotation here seems to generate an excessive amount of visual

53
00:03:23.891 --> 00:03:27.190
artifacts, the source code, and even an example,

54
00:03:27.430 --> 00:03:28.900
core lab notebook is available.

55
00:03:29.110 --> 00:03:32.710
I think it is one of the most accessible papers in this area.

56
00:03:33.220 --> 00:03:37.330
Don't miss out and make sure to have a look in the video description and try to

57
00:03:37.331 --> 00:03:40.060
run your own experiments. Let me know in the comments,

58
00:03:40.061 --> 00:03:43.990
how they went or feel free to drop by at our discord server,

59
00:03:44.200 --> 00:03:48.310
where all of your fellow scholars are welcome to discuss ideas and learn

60
00:03:48.311 --> 00:03:51.070
together in a kind and respectful environment.

61
00:03:51.310 --> 00:03:54.640
The link is available in the video description. It is completely free.

62
00:03:54.850 --> 00:03:59.320
And if you have joined, make sure to leave a short introduction. Now, of course,

63
00:03:59.350 --> 00:04:03.520
beyond the many amazing use cases of this in reviving deceased actors,

64
00:04:04.000 --> 00:04:07.840
creating beautiful visual art without being movies. And more,

65
00:04:08.230 --> 00:04:09.100
unfortunately,

66
00:04:09.220 --> 00:04:12.610
there are people around the world who are rubbing their palms together in

67
00:04:12.611 --> 00:04:14.830
excitement to use this to their advantage.

68
00:04:15.340 --> 00:04:18.910
So you may ask why make these videos on the pics?

69
00:04:19.420 --> 00:04:24.100
Why spread this knowledge, especially now with the source codes? Well,

70
00:04:24.130 --> 00:04:28.570
I think step number one is to make sure to inform the public that these defects

71
00:04:28.600 --> 00:04:31.420
can now be created quickly and inexpensively,

72
00:04:31.750 --> 00:04:35.920
and they don't require a trained scientist anymore. If this can be done,

73
00:04:36.100 --> 00:04:38.800
it is of utmost importance that we all know about it.

74
00:04:39.490 --> 00:04:43.360
Then beyond that step, number two, as a service to the public,

75
00:04:43.540 --> 00:04:48.400
I attend to EU and NATO conferences and inform key political and

76
00:04:48.401 --> 00:04:52.840
military decision makers about the existence and details of these techniques to

77
00:04:52.841 --> 00:04:56.680
make sure that they also know about these and using that knowledge,

78
00:04:56.830 --> 00:05:01.240
they can make better decisions for us. You see me doing it here and again,

79
00:05:01.330 --> 00:05:05.470
you see this technique in action here to demonstrate that it works really well

80
00:05:05.500 --> 00:05:07.930
for video footage in the wild note,

81
00:05:07.960 --> 00:05:11.410
that these talks and consultations all happen free of charge.

82
00:05:11.680 --> 00:05:13.090
And if they keep inviting me,

83
00:05:13.300 --> 00:05:16.840
I'll keep showing up to help with this in the future as a service to the public.

84
00:05:17.170 --> 00:05:19.540
The cool thing is that later over dinner,

85
00:05:19.750 --> 00:05:23.410
they tend to come back to me with a summary of their understanding of the

86
00:05:23.411 --> 00:05:24.244
situation.

87
00:05:24.340 --> 00:05:28.630
And I highly appreciate the fact that they are open to what we scientists have

88
00:05:28.631 --> 00:05:31.990
to say. And now please enjoy the promised footage.

89
00:05:33.240 --> 00:05:36.540
<v 0>Dear fellow scholars. This is two minute papers with Dr. Carol Jonah.</v>

90
00:05:36.541 --> 00:05:40.770
If I hear it is important for you to know that everybody can make defects.

91
00:05:40.771 --> 00:05:42.930
Now you can turn your head around.

92
00:05:44.610 --> 00:05:46.470
Mouth movements are looking great.

93
00:05:46.800 --> 00:05:50.520
And eye movements are also translated into the target footage.

94
00:05:52.200 --> 00:05:53.940
And of course, as we always say,

95
00:05:54.300 --> 00:05:58.220
two more papers down the line and it will be even better and cheaper Dundas.

96
00:05:59.680 --> 00:06:03.250
<v 1>This episode has been supported by weights and biases here.</v>

97
00:06:03.310 --> 00:06:07.000
They show you how you can use sweeps their tool to search through high

98
00:06:07.001 --> 00:06:11.920
dimensional parameter spaces and find the best performing model weights and

99
00:06:11.921 --> 00:06:15.820
biases provides tools to track your experiments in your deep learning projects.

100
00:06:16.120 --> 00:06:19.450
Their system is designed to save you a ton of time and money,

101
00:06:19.630 --> 00:06:24.310
and it is actively used in projects at prestigious labs, such as open AI,

102
00:06:24.880 --> 00:06:27.130
peeled our research GitHub and more.

103
00:06:27.520 --> 00:06:32.200
And the best part is that if you're an academic or have an open source project,

104
00:06:32.380 --> 00:06:36.430
you can use their tools for free. It really is as good as it gets.

105
00:06:36.790 --> 00:06:40.060
Make sure to visit them through wmb.com/papers,

106
00:06:40.300 --> 00:06:42.670
or just click the link in the video description.

107
00:06:42.970 --> 00:06:47.110
And you can get a freedom or today our thanks to weights and biases for the

108
00:06:47.680 --> 00:06:50.920
longstanding support and for helping us make better videos for you.

109
00:06:51.310 --> 00:06:55.150
Thanks for watching and for your generous support. And I'll see you next time.

