WEBVTT

1
00:00:00.390 --> 00:00:04.290
<v 0>Dear fellow scholars. This is two minute papers with Dr. Carriage on NIFA here.</v>

2
00:00:04.740 --> 00:00:08.160
About two years ago, we worked on a neuro rendering system,

3
00:00:08.400 --> 00:00:13.230
which will perform light transport on the scene, and guess how it would change.

4
00:00:13.350 --> 00:00:16.560
If we would change the material properties of this test object,

5
00:00:17.010 --> 00:00:21.060
it was able to closely match the output of a real light simulation program.

6
00:00:21.390 --> 00:00:25.980
And it was near instantaneous as it took less than five milliseconds

7
00:00:26.190 --> 00:00:28.530
instead of the 40 to 60 seconds,

8
00:00:28.650 --> 00:00:32.370
the light transport algorithm usually requires this technique,

9
00:00:32.371 --> 00:00:34.950
went by the name Garcia and material synthesis,

10
00:00:35.250 --> 00:00:38.490
and the learned quantities were material properties,

11
00:00:39.240 --> 00:00:43.440
but this no paper sets out to learn something more difficult.

12
00:00:43.650 --> 00:00:45.270
And also more general.

13
00:00:45.630 --> 00:00:49.680
We are talking about a five D neural radiance field representation.

14
00:00:50.220 --> 00:00:54.600
So what does this mean exactly what this means is that we have the three

15
00:00:54.601 --> 00:00:59.280
dimensions for location and two for view direction or in short,

16
00:00:59.400 --> 00:01:03.960
the input is where we are in space and what we are looking at and the

17
00:01:03.961 --> 00:01:05.430
resulting image of this view.

18
00:01:06.000 --> 00:01:09.000
So here we take a bunch of this input data,

19
00:01:09.390 --> 00:01:14.310
learn it and synthesize new previously unseen views of not just

20
00:01:14.311 --> 00:01:18.210
the materials in the scene, but the entire scene itself.

21
00:01:18.930 --> 00:01:22.080
And here we are talking not only digital environments,

22
00:01:22.290 --> 00:01:26.730
but also real scenes as well. Now that's quite a value proposition.

23
00:01:26.790 --> 00:01:30.960
So let's see if it can live up to this promise. Wow.

24
00:01:31.350 --> 00:01:36.120
So good. Love it. But what is it really that we should be looking at?

25
00:01:36.600 --> 00:01:38.280
What makes a good output here?

26
00:01:38.790 --> 00:01:42.840
The most challenging part is writing an algorithm that is able to reproduce

27
00:01:42.870 --> 00:01:47.070
delicate high-frequency details while having temporal coherence.

28
00:01:47.550 --> 00:01:51.150
So what does that mean? Well, in simpler words,

29
00:01:51.270 --> 00:01:54.000
we are looking for sharp and smooth image sequences.

30
00:01:54.540 --> 00:01:58.770
Perfectly math objects are easier to learn here because they look the same from

31
00:01:58.771 --> 00:01:59.604
all directions.

32
00:01:59.910 --> 00:02:04.500
While Glossier more reflective materials are significantly more difficult

33
00:02:04.800 --> 00:02:08.070
because they change a great deal as we move our head around.

34
00:02:08.370 --> 00:02:12.930
And this highly variant information is typically not present in the learned

35
00:02:12.931 --> 00:02:15.150
input images. If you read the paper,

36
00:02:15.270 --> 00:02:18.180
you'll see these referred to as non lumbers and materials,

37
00:02:18.660 --> 00:02:22.590
the paper and the video contains a ton of examples of these view dependent

38
00:02:22.591 --> 00:02:27.150
effects to demonstrate that these difficult scenes are handled really well by

39
00:02:27.151 --> 00:02:31.230
this technique. Refractions also look great. Now,

40
00:02:31.260 --> 00:02:34.920
if we define difficulty as things that change a lot,

41
00:02:34.950 --> 00:02:37.830
when we change our position or view direction, a little,

42
00:02:38.280 --> 00:02:41.580
not only the non lumber and materials are going to give us headaches,

43
00:02:41.960 --> 00:02:45.480
occlusions can also be challenging as well. For instance,

44
00:02:45.481 --> 00:02:49.920
you can see here how well it handles the complex occlusion situation between the

45
00:02:49.921 --> 00:02:51.450
ribs of the skeleton here.

46
00:02:55.610 --> 00:03:00.550
It also has an understanding of depth and these depth is so accurate

47
00:03:00.700 --> 00:03:03.970
that we can do these nice augmented reality applications,

48
00:03:04.180 --> 00:03:08.920
where we put a new virtual object in the scene and it correctly determines

49
00:03:09.010 --> 00:03:13.540
whether it is in front of, or behind the real object in the scene,

50
00:03:14.200 --> 00:03:17.530
kind of what these new iPads do with their LIDAR sensors,

51
00:03:17.740 --> 00:03:19.180
but without the sensor,

52
00:03:21.580 --> 00:03:25.870
as you see this technique smokes the competition. So what are, you know,

53
00:03:26.050 --> 00:03:30.820
entire real world scenes can be reproduced from only a few views by using

54
00:03:31.120 --> 00:03:35.920
neural networks. And the results are just out of this world. Absolutely amazing.

55
00:03:36.580 --> 00:03:40.480
What you see here is an instrumentation of this exact paper we have talked

56
00:03:40.481 --> 00:03:43.180
about, which was made by weights and biases.

57
00:03:43.480 --> 00:03:47.920
I think organizing these experiments really showcases the usability of their

58
00:03:47.921 --> 00:03:48.754
system.

59
00:03:48.850 --> 00:03:53.080
Also weights and biases provides tools to track your experiments in your deep

60
00:03:53.081 --> 00:03:57.610
learning project. Their system is designed to save you a ton of time and money,

61
00:03:57.790 --> 00:04:00.970
and it is actively used in projects at prestigious labs,

62
00:04:01.120 --> 00:04:05.050
such as open AI Toyota research GitHub and more.

63
00:04:05.380 --> 00:04:10.360
And the best part is that if you're an academic or have an open source project,

64
00:04:10.510 --> 00:04:14.980
you can use their tools for free. It really is as good as it gets,

65
00:04:15.220 --> 00:04:18.730
make sure to visit them through wnb.com/papers,

66
00:04:19.060 --> 00:04:21.400
or just click the link in the video description.

67
00:04:21.610 --> 00:04:23.560
And you can get a free demo today.

68
00:04:23.950 --> 00:04:28.420
Our thanks to weights and biases for the longstanding support and for helping us

69
00:04:28.450 --> 00:04:32.050
make better videos for you. Thanks for watching and for your generous support.

70
00:04:32.200 --> 00:04:33.940
And I'll see you next time.

