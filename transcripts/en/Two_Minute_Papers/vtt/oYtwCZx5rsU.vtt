WEBVTT

1
00:00:00.210 --> 00:00:01.380
<v 0>Dear fellow scholars.</v>

2
00:00:01.560 --> 00:00:06.450
This is not two minute papers with Dr. [inaudible] due to popular demand.

3
00:00:06.540 --> 00:00:10.950
This is a surprise video with the talk of our new paper that we just published.

4
00:00:11.370 --> 00:00:15.360
This was the third and last paper in my PhD thesis. And hence,

5
00:00:15.510 --> 00:00:19.650
this is going to be a one-off video that is longer and a tiny bit more

6
00:00:19.680 --> 00:00:23.790
technical. I am keenly aware of it, but I hope you'll enjoy it.

7
00:00:24.180 --> 00:00:27.240
Let me know in the comments, when you have finished the video and worry,

8
00:00:27.241 --> 00:00:31.770
not all the upcoming videos are going to be in the usual two minute papers

9
00:00:31.771 --> 00:00:32.604
format,

10
00:00:32.700 --> 00:00:36.420
the paper and the source code are all available in the video description.

11
00:00:36.930 --> 00:00:39.960
And now let's dive in, in a previous paper.

12
00:00:40.020 --> 00:00:43.710
Our goal was to populate this scene with over a hundred materials,

13
00:00:43.770 --> 00:00:47.880
with a learning based technique and create a beautiful planet with rich

14
00:00:47.910 --> 00:00:50.760
vegetation. The results looked like this.

15
00:00:51.300 --> 00:00:56.280
One of the key elements to accomplish this was to use a neuro render or in other

16
00:00:56.281 --> 00:00:58.980
words, the decoder network that you see here,

17
00:00:59.190 --> 00:01:03.180
which took a material shadow description as an input and predicted its

18
00:01:03.181 --> 00:01:07.590
appearance, thereby replacing the render we used in the project,

19
00:01:08.130 --> 00:01:10.200
it had its own limitations. For instance,

20
00:01:10.350 --> 00:01:14.880
it was limited to this scene with a fixed lighting setup and only the material

21
00:01:14.881 --> 00:01:18.420
properties were subject to change, but in return,

22
00:01:18.600 --> 00:01:22.620
it mimicked the global illumination render rapidly and faithfully.

23
00:01:23.130 --> 00:01:24.420
And in this new work,

24
00:01:24.600 --> 00:01:28.920
our goal was to take a different vantage point and help artists with general

25
00:01:28.921 --> 00:01:33.090
image processing knowledge to perform material synthesis. Now,

26
00:01:33.120 --> 00:01:34.590
this sounds a little nebulous.

27
00:01:34.680 --> 00:01:39.240
So let me explain one of the key ideas is to achieve this with a

28
00:01:39.241 --> 00:01:42.990
system that is meant to take images from its own render,

29
00:01:43.260 --> 00:01:47.730
like the ones you see here, but of course we produce these ourselves.

30
00:01:47.880 --> 00:01:52.110
So obviously we know how to do it. So this is not very useful yet.

31
00:01:52.620 --> 00:01:53.250
However,

32
00:01:53.250 --> 00:01:58.200
the twist is that we only start out with an image of the source material and

33
00:01:58.201 --> 00:02:03.090
then load it into a rest or image editing program like Photoshop and edit

34
00:02:03.091 --> 00:02:08.070
it to our liking. And just pretend that this is achievable with our render.

35
00:02:08.700 --> 00:02:09.360
As you see,

36
00:02:09.360 --> 00:02:13.980
many of these target images in the middle are results of poorly executed edits.

37
00:02:14.640 --> 00:02:18.210
For instance, the staged specular highlight in the first example,

38
00:02:18.450 --> 00:02:22.740
isn't very well done and neither is the background of the gold target image in

39
00:02:22.741 --> 00:02:25.680
the middle. However, in the next step,

40
00:02:25.920 --> 00:02:30.570
our method proceeds to find a photorealistic material description that

41
00:02:30.750 --> 00:02:34.110
when rendered resembles this target image and works well,

42
00:02:34.290 --> 00:02:37.320
even in the presence of these poorly executed edits,

43
00:02:37.800 --> 00:02:42.420
the whole process executes in 20 seconds to produce a mathematical

44
00:02:42.421 --> 00:02:45.330
formulation for this problem, we started with this.

45
00:02:45.720 --> 00:02:50.580
We have an input image T and edit it to our liking to get the target image

46
00:02:50.610 --> 00:02:51.660
T with a tail.

47
00:02:52.200 --> 00:02:57.000
Now we are looking for a shader parameter set X that when rendered with the

48
00:02:57.270 --> 00:02:59.980
fi operator approximates, the edited image,

49
00:03:00.460 --> 00:03:02.410
the constraint below stipulates,

50
00:03:02.440 --> 00:03:05.920
that we would remain within the physical boundaries for each parameter.

51
00:03:06.130 --> 00:03:06.963
For instance,

52
00:03:07.000 --> 00:03:11.290
albedo is between zero and one proper indices of refraction and so on.

53
00:03:11.890 --> 00:03:13.960
So how do we deal with Phi?

54
00:03:14.560 --> 00:03:17.740
We use the previously mentioned neuro renderer to implement it.

55
00:03:18.010 --> 00:03:22.930
Otherwise this optimization process would take 25 hours later.

56
00:03:22.960 --> 00:03:27.430
We made an equivalent and constraint reformulation of this problem to be able to

57
00:03:27.431 --> 00:03:29.920
accommodate a greater set of optimizers.

58
00:03:30.370 --> 00:03:35.170
This all sounds great on paper and works reasonably well for materials that

59
00:03:35.171 --> 00:03:38.170
can be exactly matched with this shader. Like this one,

60
00:03:38.770 --> 00:03:42.220
this optimizer based solution can achieve it reasonably well.

61
00:03:43.600 --> 00:03:46.780
But unfortunately, for more challenging cases,

62
00:03:46.900 --> 00:03:49.420
as you see the target image on the lower, right,

63
00:03:49.690 --> 00:03:54.490
the optimizers output leaves much to be desired note again,

64
00:03:54.550 --> 00:03:58.990
that the result on the upper right is achievable with the shader while the lower

65
00:03:58.991 --> 00:04:03.220
right is a challenging imaginary material that we are trying to achieve.

66
00:04:03.940 --> 00:04:06.010
The fact that this is quite difficult.

67
00:04:06.280 --> 00:04:10.360
It's not a surprise because we have anomaly near and non-combat optimization

68
00:04:10.361 --> 00:04:12.730
problem, which is also high-dimensional.

69
00:04:13.330 --> 00:04:16.660
So this optimization solution is also quite slow,

70
00:04:16.900 --> 00:04:21.670
but it can start inching towards the target image as an alternative solution.

71
00:04:21.700 --> 00:04:25.150
We also developed something that we call an inversion network.

72
00:04:25.330 --> 00:04:29.830
This addresses the adjoint problem of neural rendering, or in other words,

73
00:04:29.950 --> 00:04:33.070
we show it the edited input image and outcomes,

74
00:04:33.100 --> 00:04:35.140
the shader that would produce this image.

75
00:04:35.620 --> 00:04:39.250
We have trained nine different neural network architectures for this problem,

76
00:04:39.400 --> 00:04:43.990
which sounds great. So how well did it work? Well,

77
00:04:44.200 --> 00:04:48.520
we found out that none of them are really satisfactory for more difficult edits

78
00:04:48.700 --> 00:04:52.930
because all of the target images are far far outside of the training domain.

79
00:04:53.380 --> 00:04:57.490
We just cannot prepare the networks to be able to handle the rich variety of

80
00:04:57.491 --> 00:05:00.160
edits that come from the artists. However,

81
00:05:00.280 --> 00:05:04.270
some of them are one could say almost usable. For instance,

82
00:05:04.390 --> 00:05:08.920
number one and five are not complete failures and note that these

83
00:05:08.921 --> 00:05:12.970
solutions are provided instantly. So we have two techniques,

84
00:05:13.090 --> 00:05:15.130
none of them are perfect for our task,

85
00:05:15.460 --> 00:05:19.840
a fast and approximate solution with the inversion of networks and the slower

86
00:05:19.841 --> 00:05:23.140
optimizer that can slowly inch towards the target image.

87
00:05:23.620 --> 00:05:28.090
Our key insight here is that we can produce the hybrid method that fuses the two

88
00:05:28.091 --> 00:05:31.060
solutions together. The workflow goes as follows.

89
00:05:31.450 --> 00:05:36.340
We take an image of the initial source material and edit it to our liking to get

90
00:05:36.370 --> 00:05:37.240
this target image.

91
00:05:37.630 --> 00:05:42.100
Then we create a course prediction with a selection of inversion networks to

92
00:05:42.101 --> 00:05:45.940
initialize the optimizer with the prediction of one of these neural networks.

93
00:05:46.480 --> 00:05:47.800
Preferably a good one.

94
00:05:47.890 --> 00:05:51.220
So the optimizer can start out from a reasonable initial guests.

95
00:05:51.850 --> 00:05:54.340
So how well does this hybrid method work?

96
00:05:54.910 --> 00:05:57.010
I show you in a moment here,

97
00:05:57.050 --> 00:06:01.820
we start out with an achievable target image and then try to challenging

98
00:06:01.850 --> 00:06:03.230
image editing operations.

99
00:06:03.830 --> 00:06:08.630
This image can be reproduced perfectly as long as the inversion process works

100
00:06:08.631 --> 00:06:11.630
reliably. Unfortunately, as you see here,

101
00:06:11.750 --> 00:06:16.360
this is not the case in the first row using the optimizer and the inversion

102
00:06:16.370 --> 00:06:17.840
networks separately,

103
00:06:18.110 --> 00:06:21.740
we get results that fail to capture the specular highlight properly.

104
00:06:23.240 --> 00:06:24.230
In the second row,

105
00:06:24.290 --> 00:06:28.100
we have deleted the specular highlight on the target image on the right and

106
00:06:28.101 --> 00:06:30.320
replaced it with a completely different one.

107
00:06:30.800 --> 00:06:35.000
I like to call this the Franken BRDF and it would be amazing if we could do

108
00:06:35.001 --> 00:06:37.130
this, but unfortunately,

109
00:06:37.310 --> 00:06:40.580
both the optimizer and the inversion networks flounder.

110
00:06:42.290 --> 00:06:45.890
Another thing that would be really nice to do is deleting the specular,

111
00:06:45.891 --> 00:06:48.950
highlight and filling the image via image in painting.

112
00:06:49.490 --> 00:06:51.740
This kind of works with the optimizer,

113
00:06:52.160 --> 00:06:55.520
but you'll see in a moment that it's not nearly as good as it could be.

114
00:06:56.750 --> 00:06:58.670
And now, if you look carefully,

115
00:06:58.820 --> 00:07:03.410
you see that our hybrid method outperforms both of these techniques in each of

116
00:07:03.411 --> 00:07:08.120
the three cases in the paper, we report results on a dozen more cases as well.

117
00:07:10.550 --> 00:07:14.870
We make an even stronger claim in the paper where we say that these results are

118
00:07:15.320 --> 00:07:18.980
close to the global optimum. You see the results of this hybrid method.

119
00:07:19.160 --> 00:07:23.750
If you look at the intersection of now their meat and N they are highlighted

120
00:07:23.751 --> 00:07:24.830
with the red ellipsis,

121
00:07:25.430 --> 00:07:30.050
the records in the table show the RMS errors and are subject to minimization

122
00:07:30.620 --> 00:07:34.610
with this, you see that this goes neck and neck with the global optimizer,

123
00:07:34.820 --> 00:07:37.790
which is highlighted with green in summary.

124
00:07:37.880 --> 00:07:42.050
Our technique runs in approximately 20 seconds works for specular,

125
00:07:42.051 --> 00:07:46.760
highlight editing image, Blanding stitching in painting and more.

126
00:07:47.180 --> 00:07:52.040
The proposed method is robust works even in the presence of poorly edited images

127
00:07:52.250 --> 00:07:56.840
and can be easily deployed in already existing rendering systems and allows for

128
00:07:56.841 --> 00:08:00.230
rapid material prototyping for artists working in the industry.

129
00:08:00.710 --> 00:08:04.130
It is also independent of the underlying principal shader.

130
00:08:04.310 --> 00:08:07.070
So you can also add your own and expect it to work well.

131
00:08:07.340 --> 00:08:10.160
As long as the neuro render works reliably,

132
00:08:10.640 --> 00:08:14.780
a key limitation of the work is that it only takes images in this canonical

133
00:08:14.781 --> 00:08:17.300
scene with a carved sphere and material sample,

134
00:08:17.540 --> 00:08:21.950
but we conjecture that it can be extended to be more general and propose a way

135
00:08:21.951 --> 00:08:25.640
to do it in the paper. Make sure to have a closer look. If you're interested.

136
00:08:26.330 --> 00:08:31.250
The teaser image of this paper is showcased in the 2020 computer graphics forum

137
00:08:31.310 --> 00:08:35.090
cover page. The whole thing is also quite simple to implement,

138
00:08:35.270 --> 00:08:38.600
and we provide the source code pre-train networks on our website,

139
00:08:38.900 --> 00:08:41.600
and all of them are under a permissive license.

140
00:08:42.140 --> 00:08:46.130
Thank you so much for watching this and the big thanks to pet Ivanka and Miquel

141
00:08:46.520 --> 00:08:47.630
Vima for advising this work.

