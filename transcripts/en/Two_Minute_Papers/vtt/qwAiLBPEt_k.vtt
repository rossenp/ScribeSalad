WEBVTT

1
00:00:00.150 --> 00:00:03.090
<v 0>Dear fellow scholars. This is two minute papers with Dr.</v>

2
00:00:04.050 --> 00:00:04.883
[inaudible]. If we have an animation.

3
00:00:05.460 --> 00:00:07.920
<v 1>Movie or a computer game with quadruples,</v>

4
00:00:08.160 --> 00:00:13.110
and we are yearning for really high quality life-like animations motion capture

5
00:00:13.290 --> 00:00:15.450
is often the go-to tool for the job.

6
00:00:15.930 --> 00:00:20.490
Motion capture means that we put an actor in our case, a dog in the studio,

7
00:00:20.790 --> 00:00:23.910
and we ask it to perform sitting trotting pacing,

8
00:00:23.911 --> 00:00:28.830
and jumping record its motion and transfer it onto our virtual character.

9
00:00:29.370 --> 00:00:32.310
There are two key challenges with this approach. One,

10
00:00:32.490 --> 00:00:37.020
we have to try to weave together all of these motions because we cannot record

11
00:00:37.230 --> 00:00:40.170
all the possible transitions between sitting and pacing,

12
00:00:40.440 --> 00:00:42.480
jumping and trotting and so on.

13
00:00:42.990 --> 00:00:46.290
We need some filler animations to make these transitions work.

14
00:00:46.800 --> 00:00:49.560
This was addressed by this neural network based technique here.

15
00:00:50.220 --> 00:00:54.270
The other one is trying to reduce these unnatural foot sliding motions.

16
00:00:54.660 --> 00:00:58.440
Both of these have been addressed by learning based algorithms in the previous

17
00:00:58.441 --> 00:01:02.970
works that you see here later bipeds were also taught to

18
00:01:02.971 --> 00:01:07.440
maneuver through complex geometry and sit in not one kind of chair,

19
00:01:07.710 --> 00:01:10.170
but any chair, regardless of geometry,

20
00:01:10.750 --> 00:01:13.290
this already sounds like science fiction.

21
00:01:13.590 --> 00:01:18.300
So are we done or can these amazing techniques be further improved?

22
00:01:18.810 --> 00:01:22.980
Well, we are talking about research. So the answer is of course, yes.

23
00:01:23.550 --> 00:01:27.480
Here you'll see a technique that reacts to its environment in a believable

24
00:01:27.481 --> 00:01:31.800
manner. It can accidentally step on the ball stagger a little bit,

25
00:01:33.580 --> 00:01:36.370
and then flounder on the slippery surface

26
00:01:38.160 --> 00:01:42.180
and it doesn't fall and it can do much, much more.

27
00:01:42.600 --> 00:01:47.100
The goal is that we will be able to do all this without explicitly programming

28
00:01:47.280 --> 00:01:52.140
all of these behaviors by hand. But unfortunately, there is a problem.

29
00:01:52.770 --> 00:01:55.860
If we're at an agent that behaves according to physics,

30
00:01:56.040 --> 00:01:58.020
it will be difficult to control properly.

31
00:01:58.530 --> 00:02:00.660
And this is where this new technique shines.

32
00:02:01.020 --> 00:02:05.640
It gives us physically appealing motion and we can grab a controller and play

33
00:02:05.641 --> 00:02:08.010
with the character like in a video game.

34
00:02:08.670 --> 00:02:12.090
The first step we need to perform is called imitation learning.

35
00:02:12.570 --> 00:02:17.010
This means looking at real reference movement data and trying to reproduce it.

36
00:02:17.520 --> 00:02:21.390
This is going to be motion that looks great is very natural.

37
00:02:21.720 --> 00:02:22.553
However,

38
00:02:22.650 --> 00:02:26.580
we are nowhere near done because we still don't have any control over this

39
00:02:26.581 --> 00:02:30.090
agent. Can we improve this somehow? Well,

40
00:02:30.150 --> 00:02:32.400
let's try something and see if it works.

41
00:02:32.970 --> 00:02:35.550
This paper proposes that in step number two,

42
00:02:35.730 --> 00:02:39.840
we try an architecture by the name generative adversarial network.

43
00:02:40.350 --> 00:02:40.860
Here,

44
00:02:40.860 --> 00:02:45.060
we have a neural network that generates motion and the discriminator that looks

45
00:02:45.061 --> 00:02:48.960
at these motions and tries to tell what is real and what is fake.

46
00:02:49.560 --> 00:02:51.150
However, to accomplish this,

47
00:02:51.330 --> 00:02:55.890
we need lots of real and fake data that we then use to train the

48
00:02:55.891 --> 00:02:56.724
discriminator,

49
00:02:56.850 --> 00:03:00.910
to be able to tell which one is which so how do we do that?

50
00:03:01.570 --> 00:03:02.080
Well,

51
00:03:02.080 --> 00:03:06.490
let's try to label the movement that came from the user controller inputs as

52
00:03:06.491 --> 00:03:10.270
fake and the reference movement data from before as real,

53
00:03:10.810 --> 00:03:14.950
remember that this makes sense as we concluded that the reference motion looked

54
00:03:14.951 --> 00:03:17.890
natural. If we do this over time,

55
00:03:18.010 --> 00:03:21.790
we will have a discriminator network that is able to look at a piece of

56
00:03:21.791 --> 00:03:25.390
animation data and tell whether it is real or fake.

57
00:03:26.050 --> 00:03:29.530
So after doing all this work, how does this perform?

58
00:03:29.980 --> 00:03:34.810
Does this work well sort of, but it does not react well.

59
00:03:34.840 --> 00:03:38.890
If we try to control the simulation, if we let it run undisturbed,

60
00:03:39.010 --> 00:03:43.690
it works beautifully. And now when we try to stop it with the controller,

61
00:03:46.500 --> 00:03:48.630
well, this needs some more work, doesn't it?

62
00:03:49.230 --> 00:03:53.610
So how do we adapt this architecture to the animation problem that we have here?

63
00:03:54.210 --> 00:03:58.740
And here comes one of the key ideas of the paper in step number three,

64
00:03:58.950 --> 00:04:02.550
we can rewire this whole thing to originate from the controller

65
00:04:04.400 --> 00:04:08.180
and introduce a deep reinforcement learning based fine tuning stage.

66
00:04:08.560 --> 00:04:12.800
And this was the amazing technique that DeepMind used to defeat Atari breakout.

67
00:04:13.580 --> 00:04:16.760
So what good does all this for us? Well,

68
00:04:16.790 --> 00:04:21.710
hold onto your papers because it enables true user control while synthesizing

69
00:04:21.711 --> 00:04:26.030
motion that is very robust against TAF previously unseen scenarios.

70
00:04:26.450 --> 00:04:29.870
And if you have been watching this series for awhile, you know, what is coming,

71
00:04:30.380 --> 00:04:35.150
of course, throwing blocks at it and see how well it can take the punishment.

72
00:04:35.990 --> 00:04:38.930
As you see the AI is taking it like a champ.

73
00:04:41.800 --> 00:04:44.890
We can also add pathfinding to the agent. And of course,

74
00:04:45.100 --> 00:04:46.240
being computer graphics,

75
00:04:46.241 --> 00:04:49.510
researchers throw some blocks into the mix for good measure.

76
00:04:50.050 --> 00:04:53.950
It performs beautifully. This is so realistic.

77
00:04:54.490 --> 00:04:58.930
We can also add sensors to the agent to allow them to navigate in this virtual

78
00:04:58.931 --> 00:05:03.790
world in a realistic manner. Just a note on how remarkable this is.

79
00:05:04.330 --> 00:05:07.240
So this quadruple behaves, according to physics,

80
00:05:07.600 --> 00:05:09.670
lets us control it with the controller,

81
00:05:09.940 --> 00:05:12.520
which is already somewhat of a contradiction.

82
00:05:12.940 --> 00:05:16.750
And it is robust against these perturbations at the same time.

83
00:05:17.260 --> 00:05:21.790
This is absolute witchcraft and no doubt it has earned to be accepted to

84
00:05:21.791 --> 00:05:22.510
SIGGRAPH,

85
00:05:22.510 --> 00:05:26.350
which is perhaps the most prestigious research venue in computer graphics.

86
00:05:27.760 --> 00:05:28.750
Congratulations.

87
00:05:29.260 --> 00:05:33.580
What do you see here is an instrumentation for a previous paper that we covered

88
00:05:33.610 --> 00:05:36.790
in this series, which was made by weights and biases.

89
00:05:37.090 --> 00:05:41.440
I think organizing these experiments really showcases the usability of their

90
00:05:41.441 --> 00:05:45.790
system weights and biases provides tools to track your experiments in your deep

91
00:05:45.791 --> 00:05:50.140
learning project. Their system is designed to save you a ton of time and money,

92
00:05:50.350 --> 00:05:53.440
and it is actively used in projects at prestigious labs,

93
00:05:53.590 --> 00:05:57.590
such as open AI Toyota research GitHub and more.

94
00:05:57.980 --> 00:06:02.750
And the best part is that if you're an academic or have an open source project,

95
00:06:02.900 --> 00:06:07.220
you can use their tools for free. It really is as good as it gets,

96
00:06:07.580 --> 00:06:11.090
make sure to visit them through wnb.com/papers,

97
00:06:11.300 --> 00:06:13.700
or just click the link in the video description.

98
00:06:13.970 --> 00:06:15.770
And you can get a free demo today.

99
00:06:16.310 --> 00:06:20.390
Our thanks to weights and biases for the longstanding support and for helping us

100
00:06:20.420 --> 00:06:23.960
make better videos for you. Thanks for watching and for your generous support.

101
00:06:24.140 --> 00:06:25.700
And I'll see you next time.

