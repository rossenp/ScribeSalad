WEBVTT

1
00:00:00.420 --> 00:00:04.370
<v 0>Dear fellow scholars. This is two minute papers with Dr. Károly Zsolnai-Fehér.</v>

2
00:00:04.370 --> 00:00:04.370
This paper is not your usual paper, but it does something quite novel. It appeared in the 

3
00:00:04.370 --> 00:00:04.370
distill

4
00:00:04.370 --> 00:00:04.370
 journal. One of my favorites, which offers new and exciting ways of publishing beautiful, but unusual works aiming for exceptional clarity and readability. And of course this new paper is no different. It claims that despite the fact that these neural network based learning algorithms look almost unfathomably complex inside. If we look under the hood, we can often find meaningful algorithms in there. Well, I am quite excited for this. So sign me up. Let's have a look at an example at the risk of oversimplifying the explanation, we can say that the neural network is given as a collection of neurons and connections. If you look here, you can see 

5
00:00:04.370 --> 00:00:04.370
the

6
00:00:04.370 --> 00:00:04.370
 visualization of three neurons. At first glance, they look like an absolute mess, Dante, well kind 

7
00:00:04.370 --> 00:00:04.370
of, but upon closer inspection, we see that there is

8
00:00:04.370 --> 00:00:05.203
 quite a bit of structure here.

9
00:01:04.650 --> 00:01:07.860
For instance, the upper part looks like a car window.

10
00:01:08.340 --> 00:01:13.140
The next one resembles the car body and the bottom of the third neuron

11
00:01:13.350 --> 00:01:16.440
clearly contains a wheel detector. However,

12
00:01:16.500 --> 00:01:18.840
no car looks exactly like these neurons.

13
00:01:18.990 --> 00:01:23.430
So what does the network do with all this? Well, in the next layer,

14
00:01:23.580 --> 00:01:27.720
the neurons arise as a combination of neurons in the previous layers,

15
00:01:27.930 --> 00:01:31.170
where we cherry pick parts of each neuron that we wish to use.

16
00:01:31.650 --> 00:01:34.650
So here we'd read, you'll see that we are exciting.

17
00:01:34.800 --> 00:01:39.360
The upper part of this neuron to get the window use roughly the

18
00:01:39.361 --> 00:01:44.310
entirety of the middle one and use the bottom part of the third one to assemble

19
00:01:44.790 --> 00:01:45.623
this.

20
00:01:45.870 --> 00:01:50.370
And now we have the neuron in the next layer that will help us detect whether we

21
00:01:50.371 --> 00:01:55.350
see a car in an image or not. So cool. I love this one.

22
00:01:56.070 --> 00:02:00.210
Let's look at another example here. You see a dark head detector,

23
00:02:00.600 --> 00:02:05.580
but it kind of looks like a crazy Picasso painting where he tried to paint a

24
00:02:05.581 --> 00:02:08.640
human from that one angle like everyone else,

25
00:02:08.880 --> 00:02:13.830
but from all possible angles on one image, but this is a neural network.

26
00:02:14.130 --> 00:02:17.880
So why engage in this kind of insanity? Well,

27
00:02:17.940 --> 00:02:19.500
if we have a picture of a dog,

28
00:02:19.770 --> 00:02:22.890
the orientation of the head of the dog can be anything.

29
00:02:23.310 --> 00:02:26.460
It can be a frontal image, look from the left to right,

30
00:02:26.760 --> 00:02:28.530
right to left and so on.

31
00:02:29.070 --> 00:02:32.040
So this is a pose invariant doc had detector.

32
00:02:32.550 --> 00:02:36.120
What this means is that it can detect many different orientations.

33
00:02:36.420 --> 00:02:41.370
And look here, you see that it gets very excited by all of these good boys.

34
00:02:41.970 --> 00:02:44.940
I think we even have a squirrel in here. Good thing.

35
00:02:44.941 --> 00:02:48.090
This is not the only neuron we have in the network to make a decision.

36
00:02:48.660 --> 00:02:52.830
I hope that it already shows that this is truly an ingenious design.

37
00:02:53.220 --> 00:02:55.500
If you have a look at the paper in the video description,

38
00:02:55.710 --> 00:02:57.300
which you should absolutely do,

39
00:02:57.570 --> 00:03:01.420
you will see exactly how these neurons are built from the neurons in the

40
00:03:01.421 --> 00:03:05.320
previous layers. The article contains way, way more than this.

41
00:03:05.560 --> 00:03:08.530
You'll see a lot more doc snouts curve detectors,

42
00:03:08.830 --> 00:03:13.360
and even a follow up article that you can have a look at and even comment on

43
00:03:13.390 --> 00:03:14.440
before it gets finished.

44
00:03:14.950 --> 00:03:19.630
A huge thank you to Chris Allah who devotes his time away from research and uses

45
00:03:19.631 --> 00:03:22.030
his own money to run this amazing journal.

46
00:03:22.270 --> 00:03:25.900
I cannot wait to cover more of these articles in future episodes.

47
00:03:26.080 --> 00:03:29.920
So make sure to subscribe and hit the bell icon to never miss any of those.

48
00:03:30.310 --> 00:03:31.210
So finally,

49
00:03:31.270 --> 00:03:35.740
we understand a little more how neural networks do all these amazing things.

50
00:03:35.740 --> 00:03:38.380
They are able to do one at a time to be alive.

51
00:03:38.710 --> 00:03:42.310
This episode has been supported by weights and biases here.

52
00:03:42.370 --> 00:03:46.480
They show you how you can visualize the training process for your boosted trees

53
00:03:46.630 --> 00:03:50.170
with XG boost using that tool. If you have a closer look,

54
00:03:50.200 --> 00:03:54.730
you'll see that all you need is one line of code weights and biases provides

55
00:03:54.731 --> 00:03:57.760
tools to track your experiments in your deep learning projects.

56
00:03:58.120 --> 00:04:01.330
Their system is designed to save you a ton of time and money,

57
00:04:01.510 --> 00:04:04.630
and it is actively used in projects at prestigious labs,

58
00:04:04.780 --> 00:04:08.770
such as open AI Toyota research GitHub and more.

59
00:04:09.160 --> 00:04:13.900
And the best part is that if you're an academic or have an open source project,

60
00:04:14.050 --> 00:04:18.400
you can use their tools for free. It really is as good as it gets.

61
00:04:18.760 --> 00:04:22.240
Make sure to visit them through wnb.com/papers,

62
00:04:22.480 --> 00:04:24.880
or just click the link in the video description.

63
00:04:25.120 --> 00:04:26.950
And you can get a free demo today.

64
00:04:27.340 --> 00:04:31.540
Our thanks to weights and biases for the longstanding support and for helping us

65
00:04:31.600 --> 00:04:35.110
make better videos for you. Thanks for watching and for your generous support.

66
00:04:35.260 --> 00:04:36.940
And I'll see you next time.

