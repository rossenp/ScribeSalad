WEBVTT

1
00:00:00.120 --> 00:00:04.110
<v 0>Dear fellow scholars. This is two minute papers with Dr. Károly Zsolnai-Fehér.</v>

2
00:00:04.440 --> 00:00:06.420
We're shooting feature-length movies,

3
00:00:06.660 --> 00:00:10.530
or just trying to hold meetings from home through zoom or Skype.

4
00:00:10.800 --> 00:00:15.180
We can make our appearance a little more professional by hiding the mess we have

5
00:00:15.181 --> 00:00:19.440
in the background, by changing it to something more pleasing. Of course,

6
00:00:19.470 --> 00:00:23.880
this can only happen if we have an algorithm at hand that can detect what the

7
00:00:23.881 --> 00:00:28.350
foreground and the background is, which typically is easiest.

8
00:00:28.470 --> 00:00:30.270
When we have a green screen behind us,

9
00:00:30.480 --> 00:00:35.370
that is easy to filter for even the simpler algorithms out there. However,

10
00:00:35.460 --> 00:00:38.100
of course not everyone has a green screen at home.

11
00:00:38.340 --> 00:00:42.600
And even for people who do may need to hold meetings out there in the

12
00:00:42.601 --> 00:00:44.520
wilderness, unfortunately,

13
00:00:44.610 --> 00:00:48.900
this would mean that the problem statement is the exact opposite of what we've

14
00:00:48.901 --> 00:00:53.010
said. Or in other words, the background is almost anything else,

15
00:00:53.011 --> 00:00:54.180
but a green screen.

16
00:00:54.720 --> 00:00:59.430
So is it possible to apply some of these newer neural network based learning

17
00:00:59.431 --> 00:01:02.190
algorithms to tackle this problem? Well,

18
00:01:02.370 --> 00:01:06.450
this technique promises to make this problem much, much easier to solve.

19
00:01:06.840 --> 00:01:10.080
All we need to do is take two photographs, one with,

20
00:01:10.320 --> 00:01:12.180
and one without the test subject.

21
00:01:12.390 --> 00:01:17.280
And it will automatically predict an alpha matte that isolates the test subject

22
00:01:17.281 --> 00:01:19.620
from the background. If you have a closer look,

23
00:01:19.800 --> 00:01:22.890
you'll see the first part of why this problem is difficult.

24
00:01:23.410 --> 00:01:25.200
This mat is not binary.

25
00:01:25.350 --> 00:01:30.210
So the final compositing process is given not as only foreground or

26
00:01:30.211 --> 00:01:32.820
only background for every pixel in the image,

27
00:01:33.060 --> 00:01:37.920
but there are parts typically around the silhouettes and hair that need to be

28
00:01:37.921 --> 00:01:38.970
blended together.

29
00:01:39.450 --> 00:01:44.070
This blending information is contained in the gray parts of the image and are

30
00:01:44.071 --> 00:01:47.610
especially difficult to predict. Let's have a look at some results.

31
00:01:48.030 --> 00:01:51.630
You see the captured background here and the input video below,

32
00:01:52.050 --> 00:01:55.110
and you'll see that it is truly a sight to behold.

33
00:01:55.590 --> 00:02:00.000
It seems that this person is really just casually hanging out in front of a

34
00:02:00.001 --> 00:02:02.690
place that is definitely not a whiteboard

35
00:02:05.150 --> 00:02:09.770
even works in cases where the background or the camera itself is slightly in

36
00:02:09.771 --> 00:02:14.570
motion. [inaudible]

37
00:02:14.660 --> 00:02:16.730
cool. It really is much,

38
00:02:16.850 --> 00:02:21.560
much better than these previous techniques where you see the temporal coherence

39
00:02:21.770 --> 00:02:25.460
is typically a problem. This is the flickering that you see here,

40
00:02:25.550 --> 00:02:29.690
which arises from the vastly different predictions for the alpha mat between

41
00:02:29.691 --> 00:02:33.290
neighboring frames in the video, opposed to previous methods.

42
00:02:33.480 --> 00:02:36.980
This new technique shows very little of that. Excellent.

43
00:02:37.520 --> 00:02:41.210
Now we noted that a little movement in the background is permissible,

44
00:02:41.390 --> 00:02:45.830
but it really means just a little. If things get too crazy back there,

45
00:02:45.950 --> 00:02:48.170
the outputs are also going to break down.

46
00:02:48.960 --> 00:02:53.780
This wizardry all works through a generative adversarial network in which

47
00:02:53.900 --> 00:02:58.430
one neural network generates the output results. This by itself,

48
00:02:58.520 --> 00:03:03.280
didn't all that well because the images used to train this neural network can

49
00:03:03.281 --> 00:03:07.750
differ greatly from the backgrounds that we record out there in the wild,

50
00:03:08.260 --> 00:03:09.010
in this work,

51
00:03:09.010 --> 00:03:13.750
the authors bridge the gap by introducing a detector network that tries to

52
00:03:13.751 --> 00:03:17.200
find faults in the output and tell the generator,

53
00:03:17.260 --> 00:03:21.430
if it has failed to fool it as the two neural networks, Duke it out,

54
00:03:21.670 --> 00:03:25.750
they work and improve together, yielding these incredible results.

55
00:03:26.140 --> 00:03:29.080
Note that there are plenty of more contributions in the paper.

56
00:03:29.350 --> 00:03:33.940
So please make sure to have a look for more details, what a time to be alive,

57
00:03:34.900 --> 00:03:39.310
but you see here is an instrumentation of this exact paper we have talked about,

58
00:03:39.460 --> 00:03:41.680
which was made by weights and biases.

59
00:03:41.980 --> 00:03:46.630
I think organizing these experiments really showcases the usability of their

60
00:03:46.631 --> 00:03:51.010
system weights and biases provides tools to track your experiments in your deep

61
00:03:51.011 --> 00:03:55.360
learning project. Their system is designed to save you a ton of time and money,

62
00:03:55.570 --> 00:03:58.660
and it is actively used in projects at prestigious labs,

63
00:03:58.810 --> 00:04:02.830
such as open AI Toyota research GitHub and more.

64
00:04:03.190 --> 00:04:07.960
And the best part is that if you're an academic or have an open source project,

65
00:04:08.110 --> 00:04:12.460
you can use their tools for free. It really is as good as it gets,

66
00:04:12.790 --> 00:04:16.300
make sure to visit them through wmb.com/papers,

67
00:04:16.510 --> 00:04:18.910
or just click the link in the video description.

68
00:04:19.180 --> 00:04:23.200
And you can get the freedom or today our thanks to weights and biases for their

69
00:04:23.770 --> 00:04:26.860
longstanding support and for helping us make better videos for you.

70
00:04:27.190 --> 00:04:31.150
Thanks for watching and for your generous support. And I'll see you next time.

