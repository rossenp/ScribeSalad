WEBVTT

1
00:00:00.150 --> 00:00:04.150
<v 0>Dear fellow scholars. This is two minute papers with Dr. [inaudible]</v>

2
00:00:04.150 --> 00:00:04.150
. Approximately three years ago, a magical learning based algorithm appeared that was capable of translating a photo realistic image of a zebra into a horse, or the other way around could transform apples into oranges. And more later, it become possible to do this even without the presence of a photo realistic image, where all we needed was a segmentation map. This segmentation map provides labels on what should go, where for instance, they should be the road here will be trees, traffic signs 

3
00:00:04.150 --> 00:00:04.150
out of vehicles and so on. And the output was a hopefully photorealistic

4
00:00:04.150 --> 00:00:04.983
 video. And you can see here that the results were absolutely jaw-dropping. However, look as time goes by the backside of the car morphs and warps over time, creating unrealistic results that are inconsistent even on the short term. In other words, things change around from second to second and the AI does not appear to remember what it did just a moment ago.

5
00:01:07.620 --> 00:01:12.150
This kind of consistency was solved surprisingly well in a followup paper from a

6
00:01:12.151 --> 00:01:16.770
media in which an AI would look at the footage of a video game. For instance,

7
00:01:16.980 --> 00:01:20.160
Pac-Man for approximately 120 hours,

8
00:01:20.520 --> 00:01:25.470
we could shut down the video game and the AI would understand the rules so well

9
00:01:25.620 --> 00:01:28.710
that it could recreate the game that we could even play with.

10
00:01:29.160 --> 00:01:31.050
It had memory and used it well,

11
00:01:31.230 --> 00:01:34.920
and therefore it could enforce a notion of world consistency,

12
00:01:35.190 --> 00:01:39.690
or in other words, if we returned to a state of the game that we visited before,

13
00:01:39.930 --> 00:01:43.350
it will remember to present us with a very similar information.

14
00:01:43.710 --> 00:01:45.930
So the question naturally arises.

15
00:01:46.170 --> 00:01:50.460
Would it be possible to create a photo realistic video from the segmentation

16
00:01:50.461 --> 00:01:54.300
maps that is also consistent and in today's paper,

17
00:01:54.360 --> 00:01:58.890
researchers at Nvidia proposed a new technique that requests some additional

18
00:01:58.891 --> 00:02:00.630
information, for instance,

19
00:02:00.720 --> 00:02:05.640
a depth map that provides a little more information on how far different parts

20
00:02:05.641 --> 00:02:09.780
of the image are from the camera, much like the Pac-Man paper.

21
00:02:09.810 --> 00:02:11.130
This also has memory.

22
00:02:11.340 --> 00:02:16.050
And I wonder if it is able to use it as well as that one did less

23
00:02:16.051 --> 00:02:16.884
tested out.

24
00:02:17.490 --> 00:02:22.020
This previous work is currently looking at a man with a red shirt with slowly,

25
00:02:22.021 --> 00:02:26.400
look away, disregard the warping. And when we go back, Hey,

26
00:02:26.820 --> 00:02:30.300
do you see what I see here? The shirt became white.

27
00:02:30.750 --> 00:02:34.170
This is not because the person is one of those artists who can change their

28
00:02:34.171 --> 00:02:35.790
clothes in less than a second.

29
00:02:36.060 --> 00:02:40.920
But because this older technique did not have a consistent internal model of the

30
00:02:40.921 --> 00:02:44.820
world. And now let's see the new one. Once again,

31
00:02:44.850 --> 00:02:49.350
we start with the red shirt look away and then yes,

32
00:02:49.770 --> 00:02:52.320
same red to blue gradient. Excellent.

33
00:02:52.830 --> 00:02:57.450
So it appears that this new technique also reuses information from previous

34
00:02:57.451 --> 00:03:02.440
frames efficiently is finally able to create a consistent video with much

35
00:03:02.441 --> 00:03:05.470
less morphing and warping, and even better.

36
00:03:05.650 --> 00:03:08.680
We have this advantageous consistency property,

37
00:03:08.740 --> 00:03:11.110
where if you look at something that we looked at before,

38
00:03:11.590 --> 00:03:16.480
we see very similar information there, but there is more additionally,

39
00:03:16.540 --> 00:03:19.120
it can also generate scenes from new viewpoints,

40
00:03:19.270 --> 00:03:21.520
which we also refer to as neuro rendering.

41
00:03:21.910 --> 00:03:25.270
And as you see the two viewpoints show similar objects.

42
00:03:25.390 --> 00:03:27.910
So the consistency property holds here too.

43
00:03:28.480 --> 00:03:33.220
And now hold onto your papers because we do not necessarily have to produce the

44
00:03:33.230 --> 00:03:34.750
semantic maps ourselves.

45
00:03:35.110 --> 00:03:39.490
We can let the machines do all the work by firing up a video game that we like

46
00:03:39.940 --> 00:03:44.680
to request that the different object classes are colored differently and

47
00:03:44.710 --> 00:03:46.150
get this input for free.

48
00:03:46.660 --> 00:03:51.280
And then the technique generated a photorealistic video from the game graphics.

49
00:03:51.970 --> 00:03:56.320
Absolutely amazing. Now note that it is not perfect for instance,

50
00:03:56.350 --> 00:04:00.760
it has a different notion of time as the clouds are changing in the background

51
00:04:00.850 --> 00:04:01.683
rapidly.

52
00:04:03.790 --> 00:04:08.110
And look at the end of the sequence, we got back to our starting point.

53
00:04:08.560 --> 00:04:12.760
And the first frame that we saw is very similar to the last one.

54
00:04:13.180 --> 00:04:16.180
The consistency works here too. Very good.

55
00:04:16.630 --> 00:04:19.120
I have no doubt that two more papers down the line,

56
00:04:19.210 --> 00:04:23.800
and this will be even better. And for now we can create consistent photo,

57
00:04:23.801 --> 00:04:28.750
realistic videos. Even if all we have is feely obtained, video game data,

58
00:04:29.170 --> 00:04:30.670
what a time to be alive.

59
00:04:31.210 --> 00:04:35.470
If you're a researcher or a startup looking for cheap GPU compute to run these

60
00:04:35.471 --> 00:04:39.940
algorithms, check out Lambda GPU cloud. I've talked about Lambdas,

61
00:04:39.941 --> 00:04:42.100
GPU workstations in other videos,

62
00:04:42.370 --> 00:04:46.660
and I'm happy to tell you that they are offering GPU cloud services as well.

63
00:04:47.170 --> 00:04:51.790
The Lambda GPU cloud can train image net to 93% accuracy

64
00:04:51.970 --> 00:04:54.010
for less than $19.

65
00:04:54.940 --> 00:04:58.720
Lambdas web-based IDE lets you easily access your instance right in your

66
00:04:58.721 --> 00:05:01.660
browser. And finally hold onto your papers.

67
00:05:02.050 --> 00:05:06.700
'cause the Lambda GPO cloud costs less than half of AWS and

68
00:05:06.701 --> 00:05:07.534
Asia.

69
00:05:07.630 --> 00:05:12.430
Make sure to go to Lambda labs.com/papers and sign up for one of

70
00:05:12.431 --> 00:05:14.500
their amazing GPU instances today.

71
00:05:14.890 --> 00:05:18.220
Our thanks to Lambda for helping us make better videos for you.

72
00:05:18.610 --> 00:05:22.540
Thanks for watching and for your generous support. And I'll see you next time.

