WEBVTT

1
00:00:00.300 --> 00:00:05.040
<v 0>Dear fellow scholars. This is two minute papers with Dr. [inaudible] today.</v>

2
00:00:05.310 --> 00:00:06.000
It is almost.

3
00:00:06.000 --> 00:00:10.500
<v 1>Taken for granted that neural network based learning algorithms are capable of</v>

4
00:00:10.501 --> 00:00:12.270
identifying objects in images,

5
00:00:12.570 --> 00:00:16.170
or even write full coherent sentences about them.

6
00:00:16.470 --> 00:00:20.970
But fewer people know that there's also parallel research on trying to break

7
00:00:21.000 --> 00:00:22.860
these systems. For instance,

8
00:00:22.920 --> 00:00:27.090
some of these image detectors can be fooled by adding a little noise to the

9
00:00:27.091 --> 00:00:29.580
image. And in some specialized cases,

10
00:00:29.790 --> 00:00:33.930
we can even perform something that is called the one pixel attack.

11
00:00:34.560 --> 00:00:38.400
Let's have a look at some examples, changing just this one.

12
00:00:38.401 --> 00:00:43.140
Pixel can make a classifier think that this ship is a car or

13
00:00:43.230 --> 00:00:48.180
that this horse is a frog and amusingly be quite confident about

14
00:00:48.330 --> 00:00:49.163
its guests.

15
00:00:49.290 --> 00:00:54.150
Note that the choice of this pixel and the color is by no means random and it

16
00:00:54.151 --> 00:00:59.040
needs solving a mathematical optimization problem to find out exactly how to

17
00:00:59.041 --> 00:00:59.874
perform this,

18
00:01:00.360 --> 00:01:04.230
trying to build a better image detectors while other researchers are trying to

19
00:01:04.231 --> 00:01:08.400
break them is not the only arms race we are experiencing in machine learning

20
00:01:08.401 --> 00:01:11.070
research. For instance, a few years ago,

21
00:01:11.610 --> 00:01:15.810
deep mind introduced an incredible learning algorithm that looked at the screen

22
00:01:16.050 --> 00:01:17.430
much like a human would,

23
00:01:17.820 --> 00:01:22.470
but was able to reach super human levels in playing a few Atari games.

24
00:01:22.890 --> 00:01:26.070
It was a spectacular milestone in AI research.

25
00:01:26.550 --> 00:01:30.660
They also just have published a followup paper on this that we'll cover very

26
00:01:30.661 --> 00:01:34.710
soon. So make sure to subscribe and hit the bell icon to not miss it.

27
00:01:34.740 --> 00:01:37.560
When it appears in the near future. Interestingly,

28
00:01:37.650 --> 00:01:41.400
while these learning algorithms are being improved at a staggering pace,

29
00:01:41.610 --> 00:01:46.050
there is a parallel subfield where researchers endeavor to break these learning

30
00:01:46.051 --> 00:01:50.040
systems by slightly changing the information they are presented with.

31
00:01:50.520 --> 00:01:52.440
Let's have a look at open AI as example,

32
00:01:52.980 --> 00:01:57.480
their first method adds a tiny bit of noise to a large portion of the video

33
00:01:57.481 --> 00:02:00.300
input where the difference is barely perceptible,

34
00:02:00.540 --> 00:02:05.250
but it forces the learning algorithm to choose a different action than it would

35
00:02:05.251 --> 00:02:08.610
have chosen. Otherwise. In the other one,

36
00:02:08.850 --> 00:02:12.240
the different modification was used that has a smaller footprint,

37
00:02:12.600 --> 00:02:15.750
but is more visible. For instance, in pong,

38
00:02:16.080 --> 00:02:20.880
adding a tiny fake ball to the game can coerce the learner into going down

39
00:02:21.030 --> 00:02:23.610
when it was originally planning to go up.

40
00:02:24.300 --> 00:02:28.290
It is important to emphasize that the researchers did not do this by hand.

41
00:02:28.710 --> 00:02:33.510
The algorithm itself is able to pick up game specific knowledge by itself

42
00:02:33.840 --> 00:02:37.140
and find out how to fool the other AI using it,

43
00:02:37.740 --> 00:02:41.190
both attacks perform remarkably well. However,

44
00:02:41.370 --> 00:02:45.330
it is not always true that we can just change these images or the playing

45
00:02:45.331 --> 00:02:49.860
environment to our desire to follow these algorithms. So with this,

46
00:02:50.130 --> 00:02:52.590
an even more interesting question arises,

47
00:02:53.010 --> 00:02:57.600
is it possible to just enter the game as a player and perform interesting

48
00:02:58.110 --> 00:03:02.080
stance that can win against these AIS? And with this,

49
00:03:02.230 --> 00:03:06.160
we have arrived to the subject of today's paper. This is the,

50
00:03:06.161 --> 00:03:07.570
you shall not pass game.

51
00:03:07.780 --> 00:03:12.730
Whether an agent is trying to hold back the blue character and not let it cross

52
00:03:12.731 --> 00:03:16.540
the line here, you'll see two regular AIS duking it out.

53
00:03:16.870 --> 00:03:18.400
Sometimes the red wins.

54
00:03:18.640 --> 00:03:22.630
Sometimes the blue is able to get through nothing too crazy here.

55
00:03:23.260 --> 00:03:26.410
This is the reference case, which is somewhat well-balanced.

56
00:03:27.010 --> 00:03:31.780
And now hold onto your papers because this adversarial agent that this new paper

57
00:03:31.781 --> 00:03:33.910
proposes does this,

58
00:03:35.140 --> 00:03:37.240
you may think this was some kind of glitch.

59
00:03:37.450 --> 00:03:42.040
And I put the incorrect footage here by accident. No, this is not an error.

60
00:03:42.220 --> 00:03:45.310
You can believe your eyes. It basically collapses.

61
00:03:45.490 --> 00:03:50.320
And that's absolutely nothing. This can't be a useful strategy.

62
00:03:50.680 --> 00:03:55.300
Can it? Well, look at that. It still wins the majority of the time.

63
00:03:55.720 --> 00:04:00.190
This is very confusing. How can that be? Let's have a closer look.

64
00:04:00.460 --> 00:04:05.290
This red agent is normally a somewhat competent player. As you can see here,

65
00:04:05.560 --> 00:04:08.290
it can punch the blue victim and make it four.

66
00:04:09.520 --> 00:04:14.440
We now replaced this red player with the adversarial agent, which collapsed,

67
00:04:14.890 --> 00:04:19.600
and it almost feels like it hypnotized the blue agent to also fall.

68
00:04:20.170 --> 00:04:24.730
And now squeeze your papers because the normal red opponents win rate was

69
00:04:24.731 --> 00:04:25.990
47%.

70
00:04:26.440 --> 00:04:30.520
And this collapsing chap wins 86% of the time.

71
00:04:31.210 --> 00:04:33.640
It not only wins, but it wins much,

72
00:04:33.730 --> 00:04:38.260
much more reliably than a competent AI. What is this wizardry?

73
00:04:38.680 --> 00:04:43.150
The answer is that the adversary in doses of distribution activations,

74
00:04:43.690 --> 00:04:47.380
to understand what that exactly means. Let's have a look at this chart.

75
00:04:47.950 --> 00:04:52.450
This tells us how likely it is that the actions of the AI against different

76
00:04:52.451 --> 00:04:54.850
opponents are normal. As you see,

77
00:04:54.910 --> 00:04:58.300
when this agent named a zoo plays against itself,

78
00:04:58.600 --> 00:05:03.250
the bars are in the positive region, meaning that normal things are happening.

79
00:05:03.760 --> 00:05:08.470
Things go as expected. However, that's not the case for the blue lines,

80
00:05:08.620 --> 00:05:12.190
which are the actions. When we play against this adversarial agent,

81
00:05:12.370 --> 00:05:16.630
in which case the blue victim's actions are not normal in the slightest.

82
00:05:17.140 --> 00:05:20.500
So the adversarial agent is really doing nothing,

83
00:05:20.890 --> 00:05:25.660
but it is doing nothing in a way that reprograms its opponent to make

84
00:05:25.661 --> 00:05:29.530
mistakes and behave close to a completely randomly acting agent.

85
00:05:30.100 --> 00:05:34.000
This paper is absolute insanity. I love it.

86
00:05:34.480 --> 00:05:37.810
And if you look here, you'll see that the more the blue curve improves,

87
00:05:38.050 --> 00:05:41.320
the better the scheme works for a given game. For instance,

88
00:05:41.350 --> 00:05:46.240
it is doing real good on kick and defend fairly good on Sumo

89
00:05:46.241 --> 00:05:47.074
humans.

90
00:05:47.170 --> 00:05:50.770
And that there is something about the Sumo aunt's game that prevents this

91
00:05:50.800 --> 00:05:52.900
interesting kind of hypnosis from happening.

92
00:05:53.470 --> 00:05:56.980
I love to see a follow up paper that can pull this off a little more,

93
00:05:58.010 --> 00:05:59.450
what a time to be alive.

94
00:06:00.290 --> 00:06:04.370
What you see here is an instrumentation of this exact paper we have talked

95
00:06:04.371 --> 00:06:07.130
about, which was made by weights and biases.

96
00:06:07.550 --> 00:06:12.230
I think organizing these experiments really showcases the usability of their

97
00:06:12.231 --> 00:06:16.580
system weights and biases provides tools to track your experiments in your deep

98
00:06:16.581 --> 00:06:20.960
learning project. Their system is designed to save you a ton of time and money,

99
00:06:21.140 --> 00:06:24.230
and it is actively used in projects at prestigious labs,

100
00:06:24.380 --> 00:06:28.400
such as open AI Toyota research GitHub and more.

101
00:06:28.760 --> 00:06:33.530
And the best part is that if you're an academic or have an open source project,

102
00:06:33.680 --> 00:06:38.030
you can use their tools for free. It really is as good as it gets,

103
00:06:38.360 --> 00:06:41.870
make sure to visit them through wmb.com/papers,

104
00:06:42.080 --> 00:06:44.480
or just click the link in the video description.

105
00:06:44.750 --> 00:06:48.770
And you can get the freedom or today our thanks to weights and biases for their

106
00:06:49.340 --> 00:06:52.430
longstanding support and for helping us make better videos for you.

107
00:06:52.760 --> 00:06:56.660
Thanks for watching and for your generous support. And I'll see you next time.

