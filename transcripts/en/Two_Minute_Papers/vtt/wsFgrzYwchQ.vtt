WEBVTT

1
00:00:00.330 --> 00:00:02.880
<v 0>Dear fellow scholars. This is two minute papers with,</v>

2
00:00:03.630 --> 00:00:07.980
if I hit the opening video sequence of this paper immediately starts with a

3
00:00:07.981 --> 00:00:09.510
beautiful snow simulation,

4
00:00:09.810 --> 00:00:14.250
which I presume is on a Marsh to a legendary Disney paper from

5
00:00:14.251 --> 00:00:18.690
2013, by the name a material point method for snow simulation,

6
00:00:18.930 --> 00:00:22.830
which to the best of my knowledge was used for the first frozen movie.

7
00:00:23.220 --> 00:00:27.660
I was super excited to showcase that paper when this series started. However,

8
00:00:27.810 --> 00:00:30.780
unfortunately I was unable to get the rights to do it,

9
00:00:30.990 --> 00:00:34.800
but I make sure to put a link to the original paper with the same scene in the

10
00:00:34.801 --> 00:00:37.290
video description. If you're interested now,

11
00:00:37.380 --> 00:00:41.550
typically we are looking to produce high resolution simulations with lots of

12
00:00:41.551 --> 00:00:45.630
detail. However, this takes from hours to days to compute.

13
00:00:46.080 --> 00:00:49.740
So how can we deal with this kind of complexity? Well,

14
00:00:49.830 --> 00:00:54.360
approximately 400 videos ago in two minute papers, episode 10,

15
00:00:54.630 --> 00:00:58.440
we talked about this technique that introduced spacial ad activity to this

16
00:00:58.441 --> 00:00:59.274
process.

17
00:00:59.610 --> 00:01:03.840
The adaptive part means that it made the simulation finer and Courser,

18
00:01:04.050 --> 00:01:07.200
depending on the, what parts of the simulation are visible.

19
00:01:07.620 --> 00:01:11.970
The parts that we don't see can be ran through the course of simulation because

20
00:01:11.971 --> 00:01:14.850
we won't be able to see the difference. Very smart.

21
00:01:15.330 --> 00:01:19.920
The spatial part means that we use particles and subdivide the 3d space into

22
00:01:20.220 --> 00:01:24.630
grid points in which we compute the necessary quantities like velocities and

23
00:01:24.631 --> 00:01:28.680
pressures. This was a great paper on adaptive fluid simulations.

24
00:01:29.040 --> 00:01:30.900
But now look at this new paper.

25
00:01:31.200 --> 00:01:34.440
This one says that it is about temporal adaptivity.

26
00:01:34.980 --> 00:01:38.490
There are two issues that immediately arise first.

27
00:01:38.640 --> 00:01:42.300
We don't know what temporal adaptivity means. And even if we did,

28
00:01:42.420 --> 00:01:46.680
we'll find out that this is something that is almost impossible to pull off.

29
00:01:47.250 --> 00:01:48.083
Let me explain.

30
00:01:48.480 --> 00:01:52.470
There's a great deal of difficulty in choosing the right timestamps for such a

31
00:01:52.471 --> 00:01:53.304
simulation.

32
00:01:53.580 --> 00:01:58.200
These simulations are run in a way that we check and resolve all the collisions,

33
00:01:58.470 --> 00:02:02.340
and then we can advance the time forward by a tiny amount.

34
00:02:02.880 --> 00:02:07.080
This amount is called a timestamp and choosing the appropriate timestamp has

35
00:02:07.110 --> 00:02:11.160
always been a challenge. You see, if we set it to too large,

36
00:02:11.370 --> 00:02:14.940
we will be done faster and compute less. However,

37
00:02:15.150 --> 00:02:18.870
we will almost certainly miss some collisions because we skipped over them.

38
00:02:19.410 --> 00:02:23.520
It gets even worse because the simulation may end up in a state that is so

39
00:02:23.521 --> 00:02:26.610
incorrect, that it is impossible to recover from.

40
00:02:26.730 --> 00:02:30.990
And we have to throw the entire thing out. If we set it to too low,

41
00:02:31.290 --> 00:02:34.080
we get a more robust simulation. However,

42
00:02:34.290 --> 00:02:37.410
it will take from many hours to days to compute.

43
00:02:37.680 --> 00:02:41.760
So what does this temporal activity mean? Exactly? Well,

44
00:02:41.880 --> 00:02:45.180
it means that there is not one global timestamp for the simulation,

45
00:02:45.390 --> 00:02:50.010
but time is advanced differently at different places you see here,

46
00:02:50.220 --> 00:02:54.750
this data T this means the numbers chosen for the timestamps and the blue color

47
00:02:54.751 --> 00:02:58.470
coding means a simple region where there isn't much going on.

48
00:02:58.680 --> 00:03:03.100
So we get away with bigger timestamps and less computation without missing

49
00:03:03.130 --> 00:03:04.120
important events.

50
00:03:04.630 --> 00:03:08.620
The red regions have to be simulated with smaller timestamps because there's a

51
00:03:08.621 --> 00:03:11.080
lot going on. And we would miss out on that.

52
00:03:11.530 --> 00:03:16.210
Hence the new technique is called an asynchronous method because it is a crazy

53
00:03:16.211 --> 00:03:21.130
simulation where time advances in different amounts at different spatial

54
00:03:21.131 --> 00:03:25.960
regions. So how do we test this solution? Well, of course,

55
00:03:25.990 --> 00:03:29.650
ideally, this should look the same as the synchronized simulation.

56
00:03:30.100 --> 00:03:34.390
So that's it. You bet your papers. It does look at that.

57
00:03:34.750 --> 00:03:39.250
Absolutely fantastic. And since we can get away with less computation,

58
00:03:39.550 --> 00:03:43.510
it is faster how much faster in the worst cases,

59
00:03:43.630 --> 00:03:47.860
40% faster in the better ones, 10 times faster.

60
00:03:48.220 --> 00:03:52.390
So 10 of my all-nighter fluid simulations can be done in one night.

61
00:03:52.840 --> 00:03:55.420
Sign me up one at a time to be alive.

62
00:03:55.840 --> 00:04:00.220
This episode has been supported by Lambda. If you're a researcher or a startup,

63
00:04:00.430 --> 00:04:03.430
looking for cheap GPU compute to run these algorithms,

64
00:04:03.610 --> 00:04:07.210
check out Lambda GPU cloud. I've talked about Lambdas,

65
00:04:07.240 --> 00:04:09.280
GPU workstations in other videos,

66
00:04:09.490 --> 00:04:13.630
and I'm happy to tell you that they are offering GPU cloud services as well.

67
00:04:13.930 --> 00:04:18.820
The Lambda GPU cloud can train image net to 93% accuracy for

68
00:04:18.821 --> 00:04:20.530
less than $19.

69
00:04:20.890 --> 00:04:25.210
Lambda is a web based IDE lets you easily access your instance right in your

70
00:04:25.211 --> 00:04:25.990
browser.

71
00:04:25.990 --> 00:04:30.790
And finally hold on to your papers because the Lambda GPU cloud costs less than

72
00:04:30.791 --> 00:04:32.950
half of AWS and Asia,

73
00:04:33.310 --> 00:04:37.930
make sure to go to Lambda labs.com/papers and sign up for one of their

74
00:04:37.990 --> 00:04:39.760
amazing GPU instances today.

75
00:04:40.060 --> 00:04:43.180
Our thanks to Lambda for helping us make better videos for you.

76
00:04:43.420 --> 00:04:47.290
Thanks for watching and for your generous support. And I'll see you next time.

