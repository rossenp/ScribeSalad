WEBVTT

1
00:00:04.130 --> 00:00:06.590
<v 1>Algorithms you hear about them, you know,</v>

2
00:00:06.740 --> 00:00:10.730
they're behind your screens and you may even be here as a result of one right

3
00:00:10.731 --> 00:00:14.480
now, do algorithms amplify our worst human behaviors.

4
00:00:14.540 --> 00:00:17.390
How do algorithms influence your worldviews?

5
00:00:17.630 --> 00:00:20.330
Our social media algorithms making the world worse.

6
00:00:20.390 --> 00:00:24.530
I'm going to talk to an ex YouTube engineer who built the algorithms that keep

7
00:00:24.531 --> 00:00:29.360
us hooked a former design ethicist from Google and a professor from Oxford who

8
00:00:29.361 --> 00:00:32.210
believes things may not be as bad as we think they are.

9
00:00:32.300 --> 00:00:35.580
But first an experiment to see how, what we feed them.

10
00:00:35.660 --> 00:00:38.840
Algorithm feeds us in return. I'm going to simulate,

11
00:00:38.841 --> 00:00:42.980
searching for a video for three different political perspectives in order to see

12
00:00:43.010 --> 00:00:47.210
how one simple YouTube search influences the algorithm will check a neutral

13
00:00:47.211 --> 00:00:49.310
page, make three different profiles.

14
00:00:49.370 --> 00:00:51.650
Watch the first video that comes up for each,

15
00:00:51.770 --> 00:00:55.580
then observe how searches affect the homepage and recommendations for control

16
00:00:55.581 --> 00:00:57.710
measures. I have three separate laptops.

17
00:00:57.800 --> 00:01:00.050
I've cleared the history and the cash on all of them.

18
00:01:00.140 --> 00:01:02.330
So we're starting from a neutral slate. All right.

19
00:01:02.331 --> 00:01:05.840
So now we're going to see just the regular YouTube homepage before I've created

20
00:01:05.841 --> 00:01:10.070
anything. I see a few different news videos, a few funny videos, ed Sheeran,

21
00:01:10.100 --> 00:01:13.790
some Alabama football, my dog enters fan questions,

22
00:01:13.880 --> 00:01:18.500
a few different news clips. And of course the viral Tik TOK Fleetwood Mac video.

23
00:01:18.590 --> 00:01:22.400
All right. So it's pretty neutral. Let's make these profiles. Okay.

24
00:01:22.401 --> 00:01:25.940
So now after I've made my profile, the homepage doesn't look too different.

25
00:01:26.060 --> 00:01:29.030
It has a little bit of that on me now because I made that profile.

26
00:01:29.031 --> 00:01:32.270
But for the most part, the homepage is pretty similar.

27
00:01:34.640 --> 00:01:34.970
<v 0>Looking.</v>

28
00:01:34.970 --> 00:01:36.440
<v 1>At this media bias chart,</v>

29
00:01:36.500 --> 00:01:41.090
I can see that Fox news sways more to the right MSNBC sways to the

30
00:01:41.091 --> 00:01:44.120
left. And ABC news is somewhere in the middle.

31
00:01:44.270 --> 00:01:47.510
So I'm going to start with my first simulated political perspective.

32
00:01:47.600 --> 00:01:51.800
I've looked at Fox news and I'm going to click on the first video CSPAN

33
00:01:52.460 --> 00:01:55.520
suspends, Steve Scully. After he missed a line about Twitter hack,

34
00:01:55.580 --> 00:01:57.560
I'm going to watch the video all the way through,

35
00:01:57.680 --> 00:02:00.290
give it a like let's make the second profile.

36
00:02:03.920 --> 00:02:08.360
Okay. Now I'm going to simulate the second political perspective from MSNBC.

37
00:02:08.450 --> 00:02:12.110
Watch all in with Chris Hayes highlights, watch the video all the way through.

38
00:02:12.410 --> 00:02:14.960
I'm going to give it a like one thing. That's really interesting.

39
00:02:15.110 --> 00:02:18.980
The recommended videos follow the same order for both profiles,

40
00:02:19.010 --> 00:02:21.860
but just recommending the different news stations.

41
00:02:21.920 --> 00:02:26.660
The first profile recommended three Fox news videos and one PBS video.

42
00:02:26.750 --> 00:02:31.610
The second profile recommended three MSNBC profiles and one

43
00:02:31.640 --> 00:02:35.030
PBS profile. It seems like PBS is considered quite neutral.

44
00:02:35.150 --> 00:02:37.250
It's more educational and academic.

45
00:02:37.280 --> 00:02:39.710
So if you're going down some form of a rabbit hole,

46
00:02:39.770 --> 00:02:42.170
it's not likely to throw off your search.

47
00:02:42.320 --> 00:02:43.700
I think that's something interesting to note

48
00:02:45.590 --> 00:02:48.920
simulating switching a video from the third political perspective,

49
00:02:49.010 --> 00:02:52.880
ABC news spotlight on early voting in North Carolina.

50
00:02:52.970 --> 00:02:55.670
I'm going to watch the full video and give it a like.

51
00:02:58.250 --> 00:02:59.950
<v 0>Now let's see how homepage.</v>

52
00:03:00.010 --> 00:03:04.840
<v 1>Changed for the first profile. I'm already seen a big difference.</v>

53
00:03:05.050 --> 00:03:07.120
Fox news Fox business.

54
00:03:07.240 --> 00:03:10.060
I see some favorable videos about president Trump,

55
00:03:10.090 --> 00:03:14.350
some unfavorable videos about Nancy Pelosi. I do see a wall street journal,

56
00:03:14.380 --> 00:03:17.320
but it's mostly Fox channels that are dominating.

57
00:03:17.380 --> 00:03:20.380
I do see some positive videos about Amy Coney Barrett,

58
00:03:20.410 --> 00:03:22.120
the Supreme court confirmation.

59
00:03:22.180 --> 00:03:24.790
I do see some negative videos about Hillary Clinton,

60
00:03:25.120 --> 00:03:29.440
Ted talk inside the mind of a master procrastinator. Maybe I should watch that.

61
00:03:30.190 --> 00:03:34.930
And a quirky one about president Trump's dance moves and that's just from one

62
00:03:35.050 --> 00:03:39.190
video search. I saw quite a few videos in favor of president Trump,

63
00:03:39.220 --> 00:03:43.090
quite a few videos against Joe Biden. A few funny videos,

64
00:03:43.150 --> 00:03:44.470
a few videos about meditation.

65
00:03:44.471 --> 00:03:48.280
I'm going to save for later for the profile where I searched MSNBC.

66
00:03:48.370 --> 00:03:51.610
I see another recommended video for MSNBC.

67
00:03:51.670 --> 00:03:54.700
I'm not seeing too many other news stations recommended.

68
00:03:54.730 --> 00:03:58.330
I see another one from MSNBC, Notre Dame, faculty sign,

69
00:03:58.340 --> 00:04:02.530
open letter urging judge Barrett to, okay. I see one from CNN.

70
00:04:02.560 --> 00:04:06.130
I see a C-SPAN I see another MSNBC boy surprises,

71
00:04:06.131 --> 00:04:09.280
childhood best friend dressed as a FedEx driver.

72
00:04:09.310 --> 00:04:12.130
That could be a little bit terrifying. Okay. So for the most part,

73
00:04:12.160 --> 00:04:16.480
it was a lot of MSNBC, no Fox news outlets, some funny videos,

74
00:04:16.481 --> 00:04:20.260
some sports videos, and now the ABC news search. Okay.

75
00:04:20.261 --> 00:04:23.170
So let's see how the homepage looks after watching this video.

76
00:04:23.230 --> 00:04:26.380
I see a lot of ABC news right off the bat.

77
00:04:26.410 --> 00:04:30.790
I do see a video for Fox news video from MSNBC,

78
00:04:30.820 --> 00:04:35.560
more ABC news CNBC, one from CBS, ABC news. Again,

79
00:04:35.590 --> 00:04:39.700
a video from GQ. I'm noticing more variety. This is interesting.

80
00:04:39.730 --> 00:04:42.730
Trump and Clinton are asked to say something nice about each other.

81
00:04:42.760 --> 00:04:46.210
So this is actually bringing both sides together,

82
00:04:46.300 --> 00:04:51.130
a hundred year old veteran and his secrets to life saving that video at video in

83
00:04:51.131 --> 00:04:54.220
favor of Joe Biden here, a video from business insider.

84
00:04:54.250 --> 00:04:55.870
There's a lot of writing and content.

85
00:04:55.871 --> 00:04:59.800
There's a lot of variety in news stations and it really isn't a polarizing

86
00:04:59.890 --> 00:05:01.330
homepage. Okay.

87
00:05:01.331 --> 00:05:05.290
So what we can see here is that even by searching just one video,

88
00:05:05.350 --> 00:05:09.700
all of the recommendations afterwards reinforced that point of view,

89
00:05:09.760 --> 00:05:11.950
there were no recommendations that challenged it.

90
00:05:12.040 --> 00:05:15.820
So you'll likely stay longer and continue to watch more videos,

91
00:05:15.880 --> 00:05:19.870
which would be in the best interest of both the advertisers and the creators.

92
00:05:20.110 --> 00:05:23.830
What does that say about the algorithm? We called up ex YouTube engineer,

93
00:05:24.370 --> 00:05:24.970
Kim shallow,

94
00:05:24.970 --> 00:05:29.350
who built the artificial intelligence that powers these algorithms in an article

95
00:05:29.351 --> 00:05:32.980
that you wrote for wired about YouTubes feedback loop.

96
00:05:33.010 --> 00:05:37.540
You stated that you could have predicted that the AI would deliberately promote

97
00:05:37.541 --> 00:05:41.860
harmful videos behind some of the conspiracies that we've seen recently.

98
00:05:41.890 --> 00:05:46.330
How could you have known that ahead of time and was it flat?

99
00:05:46.680 --> 00:05:49.710
<v 2>So you can detect everything a lot of time.</v>

100
00:05:50.100 --> 00:05:52.920
The idea is to try to detect as soon as possible.

101
00:05:53.010 --> 00:05:56.940
So you can predict that there is going to be toxic feedback loops.

102
00:05:56.990 --> 00:06:00.320
How can you see where the feedback loops are taking place?

103
00:06:00.920 --> 00:06:05.000
And for that you need a number of recommendation that video is presented.

104
00:06:05.001 --> 00:06:07.100
So if you see that a video is

105
00:06:08.660 --> 00:06:13.160
being promoted by a YouTube on it's toxic, then you know,

106
00:06:13.161 --> 00:06:14.630
it's going to create a feedback loop.

107
00:06:15.160 --> 00:06:19.000
<v 1>And do you think just the executives had the best interest at heart?</v>

108
00:06:19.330 --> 00:06:23.800
So people promoting things like fake news and conspiracy theories,

109
00:06:23.801 --> 00:06:28.300
just wasn't thought of you thought people would just genuinely post truthful

110
00:06:28.480 --> 00:06:29.380
videos and communities.

111
00:06:30.070 --> 00:06:34.210
<v 2>So they were really focusing on out to gain market share,</v>

112
00:06:34.390 --> 00:06:37.930
be like 20, 30% more efficient. Every year,

113
00:06:38.300 --> 00:06:43.300
this few millions people were going to conspiracy videos because of a

114
00:06:43.301 --> 00:06:46.090
usual recommendations. It was not a big deal for them.

115
00:06:46.960 --> 00:06:51.010
<v 1>I talked to Tristan Harris who studies the impact of technology on the human</v>

116
00:06:51.011 --> 00:06:53.290
mind. It's not just an technology.

117
00:06:53.291 --> 00:06:58.180
This is psychology at the intersection of technology fueled

118
00:06:58.181 --> 00:06:59.350
by supercomputers.

119
00:06:59.500 --> 00:07:00.130
<v 3>That's right.</v>

120
00:07:00.130 --> 00:07:03.160
And let me give you one concrete example of where this can show up in a way.

121
00:07:03.161 --> 00:07:05.950
It actually was told to me by Dom Cheslow the former YouTube engineer.

122
00:07:05.980 --> 00:07:07.750
If YouTube found, for example,

123
00:07:07.751 --> 00:07:10.360
as it's predicting which videos should it show you over time,

124
00:07:10.450 --> 00:07:14.500
it's not just trying to maximize your individual watch time for that 30 minute

125
00:07:14.501 --> 00:07:14.890
session.

126
00:07:14.890 --> 00:07:18.550
It also wants to show you the kind of videos that tend to keep you coming back

127
00:07:18.551 --> 00:07:19.930
in a long-term sense.

128
00:07:19.960 --> 00:07:23.860
So it's actually identifying these super high level hyperobject patterns.

129
00:07:23.920 --> 00:07:28.600
Let's say one of them is called the media is lying. Let's say that that pattern,

130
00:07:28.601 --> 00:07:33.310
that title is associated with a longer like year long trajectory of

131
00:07:33.311 --> 00:07:36.790
YouTube usage. Because obviously if I don't trust the media as much,

132
00:07:37.120 --> 00:07:38.530
then I'm going to trust YouTube a little bit more.

133
00:07:38.590 --> 00:07:41.650
And so that's an example where there's no human who's picking.

134
00:07:41.740 --> 00:07:43.210
The media is lying as a phrase,

135
00:07:43.480 --> 00:07:47.530
but the computer is predicting and finding out that this is a pattern that works

136
00:07:47.560 --> 00:07:50.860
in a super big abstract scale. That for 3 billion humans,

137
00:07:50.861 --> 00:07:54.100
that pattern can actually cause people to use YouTube more than others.

138
00:07:54.370 --> 00:07:58.600
<v 1>It's not a race that humans are designed to win. Yeah.</v>

139
00:07:58.660 --> 00:08:02.560
<v 3>Pattern like that can be incredibly dangerous because it erodes trust in society</v>

140
00:08:02.561 --> 00:08:05.620
and immediate, which is exactly what in general conspiracy theories do.

141
00:08:05.621 --> 00:08:09.400
And there's a study in there on sense-making that I think only after two minutes

142
00:08:09.401 --> 00:08:11.200
of exposure to a conspiracy theory,

143
00:08:11.620 --> 00:08:13.900
that people actually have less pro-social attitudes.

144
00:08:13.901 --> 00:08:16.630
And they're more doubtful of the narratives that they've been told. Anyway,

145
00:08:16.700 --> 00:08:17.950
there's, there's many more aspects to it.

146
00:08:18.670 --> 00:08:21.970
<v 1>On the topic of YouTube and yam. We actually just spoke with him.</v>

147
00:08:22.120 --> 00:08:26.800
Do you believe that YouTube and social media in general can send us down

148
00:08:26.980 --> 00:08:27.850
echo chambers?

149
00:08:27.910 --> 00:08:30.190
<v 3>Absolutely. I mean, I think that's the entire model.</v>

150
00:08:30.220 --> 00:08:32.350
The issue is that they're a bad Apple farm.

151
00:08:32.380 --> 00:08:35.920
Like the soil that they create is actually about rewarding,

152
00:08:36.070 --> 00:08:41.020
the most extreme and conspiracy minded views that you could create because

153
00:08:41.021 --> 00:08:42.340
they tend to get the most clicks.

154
00:08:42.341 --> 00:08:45.670
And as Renee Resta our other colleague who studies conspiracy theories will say,

155
00:08:45.671 --> 00:08:47.530
if you can make a conspiracy trend,

156
00:08:47.560 --> 00:08:50.830
you can make it true because once it's trending like Biden,

157
00:08:50.831 --> 00:08:54.460
earpiece or something like that, it can be on either side. Once it's trending,

158
00:08:54.520 --> 00:08:55.353
if the media,

159
00:08:55.560 --> 00:08:58.320
it they're endorsing the conspiracy theory and elevating its response,

160
00:08:58.410 --> 00:09:01.350
if they don't cover it and they don't talk about Biden your piece.

161
00:09:01.351 --> 00:09:04.410
Now it's a conspiracy that the media is not covering something that might be

162
00:09:04.411 --> 00:09:06.720
true, exponential rumors and exponential hearsay.

163
00:09:06.750 --> 00:09:09.870
That's the kind of thing that wins. And it becomes our information, ecology.

164
00:09:10.130 --> 00:09:14.030
<v 1>It's 700 million hours a day. Um,</v>

165
00:09:14.060 --> 00:09:18.260
people watch on YouTube videos that have been recommended by the algorithm.

166
00:09:18.530 --> 00:09:23.480
So the recommendation portion is actually very significant on the content

167
00:09:23.481 --> 00:09:24.370
that we end up seeing.

168
00:09:24.740 --> 00:09:29.270
<v 2>Right? Exactly. So that's significant, but that pushes, uh,</v>

169
00:09:29.750 --> 00:09:33.200
some creators to go in the same direction as the other isn't Mimi constant

170
00:09:33.201 --> 00:09:36.500
creators. When they talk about what would they want to do,

171
00:09:36.510 --> 00:09:41.450
they try to understand the Edwards. They try to just give what Taylorism legs.

172
00:09:41.660 --> 00:09:44.570
And don't try to understand what you like to try to understand tourism.

173
00:09:45.860 --> 00:09:50.150
<v 1>I spoke with Dr. Grant blank. Who's more optimistic about our online habits.</v>

174
00:09:50.180 --> 00:09:53.420
And I want to reference a point in your research that I found really

175
00:09:53.421 --> 00:09:57.830
interesting. You often hear people go down a rabbit hole, for example,

176
00:09:57.831 --> 00:10:02.510
on YouTube. And they come up believing that the world is flat, for example.

177
00:10:02.810 --> 00:10:04.370
But according to your research,

178
00:10:04.460 --> 00:10:08.840
if somebody goes on social media and ends up changing their mind,

179
00:10:09.320 --> 00:10:11.570
they're not in an echo chamber,

180
00:10:11.630 --> 00:10:15.740
despite the perception that somebody goes down this rabbit hole and has now

181
00:10:15.741 --> 00:10:19.520
fallen victim to these theories, because they've changed their mind.

182
00:10:19.640 --> 00:10:22.040
That actually proves they're not in an echo chamber.

183
00:10:22.220 --> 00:10:27.080
<v 2>I'm not too worried about the situation that you described of people going down,</v>

184
00:10:27.081 --> 00:10:31.610
these YouTube rabbit holes, people acquire information in a broad environment.

185
00:10:32.510 --> 00:10:33.650
Um, and Broadway's,

186
00:10:33.800 --> 00:10:38.630
they also change their mind fairly often about a lot of topics, not every topic,

187
00:10:38.660 --> 00:10:40.010
but about many topics.

188
00:10:40.130 --> 00:10:43.880
<v 1>Yeah. That were a bit more malleable than, than we think we are.</v>

189
00:10:44.150 --> 00:10:47.990
<v 2>Right. And valuable almost gives the idea that you already had an opinion.</v>

190
00:10:48.020 --> 00:10:49.160
It's in most cases,

191
00:10:49.161 --> 00:10:53.510
people didn't have an opinion beforehand and they only have a vague sense of

192
00:10:53.511 --> 00:10:57.590
what's going on. And then they encountered a more detailed argument somewhere.

193
00:10:57.920 --> 00:10:59.750
And that convinces them,

194
00:10:59.960 --> 00:11:02.360
that the real situation is different from whatever they thought.

195
00:11:03.050 --> 00:11:04.400
<v 1>Tech executives would say.</v>

196
00:11:04.460 --> 00:11:08.930
What we see on social media is actually just a reflection of what was always

197
00:11:08.931 --> 00:11:12.440
happening. And social media is just shining a mirror on it.

198
00:11:12.710 --> 00:11:15.350
Do you think that's true or do you think it's fueling it?

199
00:11:15.590 --> 00:11:17.450
<v 2>I think it's true in this sense.</v>

200
00:11:17.540 --> 00:11:22.370
It used to be reasonable for you would go to a bar and complained to your

201
00:11:22.371 --> 00:11:26.330
friends about stuff you read or you saw on television.

202
00:11:26.570 --> 00:11:29.960
And it couldn't go any further than that because you had no way to reach a

203
00:11:29.961 --> 00:11:30.980
broader audience.

204
00:11:31.040 --> 00:11:35.930
What has happened with the internet and the technologies of social media is all

205
00:11:35.931 --> 00:11:38.330
of a sudden you can complain to everybody.

206
00:11:38.600 --> 00:11:43.160
And so the things that once were kept in small circles are now

207
00:11:43.280 --> 00:11:45.200
visible to the entire world.

208
00:11:45.290 --> 00:11:50.120
And so these sort of conspiracy theories and hatred of different groups of

209
00:11:50.121 --> 00:11:51.830
people and things a lot like that,

210
00:11:51.831 --> 00:11:56.620
which would normally be expressed loudly in the public square are now

211
00:11:56.621 --> 00:12:00.910
being expressed loudly on Twitter and Facebook and other places.

212
00:12:00.970 --> 00:12:05.860
And so what has happened is that technology has amplified all these voices.

213
00:12:06.220 --> 00:12:10.090
And so that they're no longer talking in private environments,

214
00:12:10.120 --> 00:12:11.590
but they're now talking in public.

215
00:12:11.910 --> 00:12:16.650
<v 1>One of your criticisms of YouTube is the</v>

216
00:12:16.651 --> 00:12:18.750
secrecy of the algorithm.

217
00:12:18.840 --> 00:12:23.460
And you believe that if there was more transparency about how the YouTube

218
00:12:23.490 --> 00:12:24.540
algorithm works,

219
00:12:24.780 --> 00:12:29.130
people would have a bit more control and we wouldn't see such massive problems

220
00:12:29.131 --> 00:12:29.964
as we do today.

221
00:12:30.210 --> 00:12:34.290
<v 2>Yeah. So what I want is not YouTube to give the source code of the algorithm,</v>

222
00:12:34.291 --> 00:12:35.250
that's their secret.

223
00:12:35.490 --> 00:12:40.120
So what I want is that we have an idea of the impact of YouTube

224
00:12:40.650 --> 00:12:43.800
algorithm. What is it showing our kids right now?

225
00:12:43.950 --> 00:12:47.970
I want moms to be able to know what the YouTube algorithm is showing to their

226
00:12:47.971 --> 00:12:48.804
kids.

227
00:12:48.870 --> 00:12:53.760
I won't say on to know what type of say on Stitcher is showing,

228
00:12:53.761 --> 00:12:56.010
is it promoting fake science?

229
00:12:56.011 --> 00:12:59.400
Is it promoting things that have been researched on proof? That's,

230
00:12:59.460 --> 00:13:00.420
that's important to know.

231
00:13:00.630 --> 00:13:04.770
<v 1>So while it seems like these algorithms are built to reinforce our likes and our</v>

232
00:13:04.771 --> 00:13:09.030
interests, how we seek out information can give us some control.

233
00:13:09.060 --> 00:13:11.850
These algorithms are powerful and impressionable,

234
00:13:12.390 --> 00:13:16.500
but they're not the entire story I've been aware of how they work and being

235
00:13:16.501 --> 00:13:20.070
intentional and finding information. We can get back some control.

