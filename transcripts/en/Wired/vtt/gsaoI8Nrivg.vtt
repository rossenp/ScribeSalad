WEBVTT

1
00:00:00.150 --> 00:00:02.760
<v 0>Russia is by far the most advanced, uh,</v>

2
00:00:02.790 --> 00:00:06.990
foreign adversary sending manipulative messages over social media.

3
00:00:07.170 --> 00:00:11.220
They're even more nuanced today than they were in 2016.

4
00:00:11.221 --> 00:00:14.610
And we are no more prepared today than we were in 2016

5
00:00:20.300 --> 00:00:21.530
Wired. I'm sitting on a roll.

6
00:00:21.531 --> 00:00:26.150
I'm the David Austin professor of management at MIT director of MIT's initiative

7
00:00:26.151 --> 00:00:30.350
on the digital economy and author of the hype machine about how social media

8
00:00:30.351 --> 00:00:31.340
disrupts our world.

9
00:00:31.460 --> 00:00:36.050
And I'm here to debunk some myths about the role of social media in

10
00:00:36.051 --> 00:00:39.620
elections, and specifically in the upcoming election of 2020.

11
00:00:41.330 --> 00:00:41.900
<v 1>Social.</v>

12
00:00:41.900 --> 00:00:43.790
<v 0>Media sways elections,</v>

13
00:00:44.120 --> 00:00:47.390
did Russia sway the election of 2016?

14
00:00:47.420 --> 00:00:51.890
We know they sent manipulative messages to 126 million people on

15
00:00:51.891 --> 00:00:55.940
Facebook, 20 million on Instagram and 10 million tweets.

16
00:00:55.941 --> 00:01:00.140
Do these things sway the election. There's really three things to know.

17
00:01:00.170 --> 00:01:01.970
Does it change vote choice?

18
00:01:02.270 --> 00:01:06.980
Does it change voter turnout and is the reach scope and

19
00:01:06.981 --> 00:01:11.870
targeting of misinformation or campaign information enough to sway

20
00:01:11.871 --> 00:01:16.520
an election voter choice is simply who you choose to vote for

21
00:01:16.700 --> 00:01:20.120
given that you're voting. Do you vote for Republicans or Democrats?

22
00:01:20.121 --> 00:01:21.260
That's a vote choice.

23
00:01:21.410 --> 00:01:25.160
The evidence on vote choice is relatively clear.

24
00:01:25.430 --> 00:01:29.840
Social media messaging and digital advertising in general has a very

25
00:01:29.841 --> 00:01:34.640
small to negligible to zero effect on vote choice

26
00:01:34.670 --> 00:01:38.810
voter turnout. On the other hand is do you choose to vote at all?

27
00:01:38.960 --> 00:01:42.710
And the number of people who vote in an election and there,

28
00:01:42.770 --> 00:01:47.720
the evidence is a little bit scarier in the sense that large scale experiments

29
00:01:47.721 --> 00:01:50.300
have shown that social media messages,

30
00:01:50.420 --> 00:01:55.160
digital advertising can have statistically significant effects on voter

31
00:01:55.161 --> 00:01:55.700
turnout.

32
00:01:55.700 --> 00:02:00.350
Facebook ran an experiment with 61 million people in it in

33
00:02:00.351 --> 00:02:03.710
2010, which showed that with a simple message.

34
00:02:03.740 --> 00:02:08.330
They could create votes in congressional elections that wouldn't have

35
00:02:08.331 --> 00:02:10.040
happened without their message.

36
00:02:10.250 --> 00:02:14.540
They replicated that experiment in 2012 and demonstrated again,

37
00:02:14.750 --> 00:02:18.140
the ability for social media to create voter turnout.

38
00:02:18.170 --> 00:02:22.100
Many studies indicate how digital messaging can get out the vote.

39
00:02:22.101 --> 00:02:26.930
And that's an important part of changing or swaying elections targeting

40
00:02:26.931 --> 00:02:31.130
is who you direct digital social media messages to which

41
00:02:31.160 --> 00:02:35.090
populations in which regions, in which districts, uh,

42
00:02:35.150 --> 00:02:36.230
in the voting electorate,

43
00:02:36.290 --> 00:02:41.210
the evidence in 2016 indicated that Russian interference was

44
00:02:41.211 --> 00:02:46.100
targeted at swing States and that the reach and scope of it was large

45
00:02:46.101 --> 00:02:50.840
enough to affect voters in a way that could change the election through voter

46
00:02:50.841 --> 00:02:52.280
turnout. In addition,

47
00:02:52.281 --> 00:02:57.140
we know that a lot of the manipulative messages sent by Russia in

48
00:02:57.141 --> 00:03:00.730
2016 were about voter suppression,

49
00:03:00.760 --> 00:03:04.900
voter suppression memes tend to be targeted at specific communities.

50
00:03:04.900 --> 00:03:09.880
So for instance, we know that in 2016 on Instagram, in particular,

51
00:03:10.030 --> 00:03:13.810
African-American voters were targeted with voter suppression memes,

52
00:03:13.870 --> 00:03:15.490
indicating for example,

53
00:03:15.580 --> 00:03:20.440
Hillary Clinton is not a fan of the black voter and therefore we should

54
00:03:20.441 --> 00:03:24.310
stay home or there's really no one to vote for in this election.

55
00:03:24.311 --> 00:03:25.600
There's no reason to vote.

56
00:03:25.660 --> 00:03:30.460
Those types of memes were targeted through at mentions at communities

57
00:03:30.461 --> 00:03:35.200
that were African-American or in this year's election follow the

58
00:03:35.201 --> 00:03:36.760
black lives matter movement.

59
00:03:36.761 --> 00:03:41.590
And so on trying to suppress specific communities of voters in

60
00:03:41.591 --> 00:03:42.760
key swing States,

61
00:03:42.910 --> 00:03:47.770
the number one culprit in spreading misinformation and voter suppression memes

62
00:03:47.800 --> 00:03:50.920
in 2016 and likely in 2020 is Russia.

63
00:03:50.980 --> 00:03:55.870
While Russian misinformation was scary in the 2016 election,

64
00:03:55.900 --> 00:03:59.590
they are much more sophisticated today than they were four years ago.

65
00:03:59.680 --> 00:04:00.401
In addition,

66
00:04:00.401 --> 00:04:04.840
this is happening during a global pandemic with civil unrest in the

67
00:04:04.841 --> 00:04:05.500
streets,

68
00:04:05.500 --> 00:04:09.520
arising from the justifiable social movements against police brutality in the

69
00:04:09.521 --> 00:04:11.890
United States. With all of this uncertainty,

70
00:04:11.891 --> 00:04:16.330
we are at a dramatic risk for foreign interference in our election in 2020.

71
00:04:16.510 --> 00:04:17.231
And of course,

72
00:04:17.231 --> 00:04:22.120
social media is nowhere near the only factor affecting

73
00:04:22.121 --> 00:04:26.500
elections. Certainly the candidates, their charisma, their policies,

74
00:04:26.530 --> 00:04:31.420
advertising their ability to connect with voters as well as news of

75
00:04:31.421 --> 00:04:32.254
the day,

76
00:04:32.290 --> 00:04:36.880
what is hitting the pocket books and the homes and families of

77
00:04:37.000 --> 00:04:40.270
everyday voters obviously has the largest effect.

78
00:04:40.480 --> 00:04:44.560
Social media spreads fake news faster than the truth. That's true.

79
00:04:45.070 --> 00:04:49.960
We did a 10 year longitudinal study of all of the verified true and

80
00:04:49.961 --> 00:04:54.880
false news that spread on Twitter between 2006 and 2017.

81
00:04:54.970 --> 00:04:55.631
In fact,

82
00:04:55.631 --> 00:05:00.550
we found that false news was 70% more likely to be retweeted and

83
00:05:00.580 --> 00:05:04.930
false news traveled about six times faster than true news. Online,

84
00:05:05.170 --> 00:05:09.520
fake news is not a new term. It was not invented by Donald Trump. In fact,

85
00:05:09.670 --> 00:05:14.230
it first appeared, I believe in a Harper's magazine news story.

86
00:05:14.380 --> 00:05:15.580
And we've had, uh,

87
00:05:15.610 --> 00:05:20.050
the concept of falsity in journalism for many years in

88
00:05:20.051 --> 00:05:24.370
decades prior to today, the thing that makes today different, however,

89
00:05:24.371 --> 00:05:28.780
is the speed and breadth and depth with which social media can

90
00:05:28.781 --> 00:05:33.610
spread fake news so much faster than the truth online and how

91
00:05:33.640 --> 00:05:38.440
that can be targeted at specific individuals and communities creating

92
00:05:38.470 --> 00:05:42.820
separate realities for people who are seeing one type of news in one

93
00:05:42.821 --> 00:05:46.470
community and a different type of news in a different community. So when,

94
00:05:46.471 --> 00:05:48.850
when we found these results in our Twitter data,

95
00:05:48.851 --> 00:05:51.580
the natural next question for us was why,

96
00:05:51.790 --> 00:05:54.880
why does fake news spreads so much farther, faster, deeper,

97
00:05:54.881 --> 00:05:56.680
and more broadly than the truth?

98
00:05:56.840 --> 00:06:00.710
And what we came up with was what we called the novelty hypothesis.

99
00:06:00.740 --> 00:06:04.190
So if you read the cognitive science literature, you know,

100
00:06:04.310 --> 00:06:08.150
that human attention is drawn to novelty new things in the environment.

101
00:06:08.420 --> 00:06:10.760
And if you read the sociology literature, you know,

102
00:06:10.940 --> 00:06:14.990
that we gain in status when we share novel information,

103
00:06:14.991 --> 00:06:18.770
because it makes us look like we're in the know or that we have inside

104
00:06:18.771 --> 00:06:22.880
information that other people don't have. So these two factors, uh,

105
00:06:22.910 --> 00:06:25.550
make it more likely that we share novelty.

106
00:06:25.551 --> 00:06:28.880
So when we check the novelty of true and false news,

107
00:06:28.881 --> 00:06:32.660
compared to everything that a given individual on Twitter had seen in the two

108
00:06:32.661 --> 00:06:33.500
months prior,

109
00:06:33.501 --> 00:06:37.670
we found that indeed false news was way more novel than the truth.

110
00:06:37.671 --> 00:06:40.880
And when we checked the replies to true and false tweets,

111
00:06:40.881 --> 00:06:45.080
to see how people were expressing sentiment about what they were reading,

112
00:06:45.230 --> 00:06:49.670
we found that indeed in reply to false news, people express, surprise,

113
00:06:50.030 --> 00:06:54.860
anger, and discussed while in reply to true news, they expressed anticipation,

114
00:06:54.861 --> 00:06:55.700
joy and trust.

115
00:06:55.880 --> 00:07:00.290
So the surprise confirmed our novelty hypothesis that yes,

116
00:07:00.320 --> 00:07:04.430
false news is more novel. People spread more novel information,

117
00:07:04.431 --> 00:07:07.400
more often than less novel information.

118
00:07:07.490 --> 00:07:11.060
And people were genuinely surprised by fake news.

119
00:07:11.090 --> 00:07:13.280
Voting booths are not hackable,

120
00:07:14.330 --> 00:07:17.060
false voting booths can be hacked,

121
00:07:17.300 --> 00:07:22.130
have been hacked and abnormal ballot results in a number of examples

122
00:07:22.160 --> 00:07:24.920
over the last decade or so in the United States.

123
00:07:25.190 --> 00:07:29.780
Many people believe that because the United States has a Federalist system in

124
00:07:29.781 --> 00:07:34.580
which each state tallies its votes in its own way uses different

125
00:07:34.581 --> 00:07:35.690
computer systems.

126
00:07:35.840 --> 00:07:40.610
That there's no centralized tallying of ballots that this somehow protects the

127
00:07:40.611 --> 00:07:44.990
American voting system from hacking at the voting booth itself.

128
00:07:45.020 --> 00:07:46.160
But that's not true.

129
00:07:46.190 --> 00:07:50.480
It just means that there are 50 different types of systems that a hacker can

130
00:07:50.481 --> 00:07:52.010
attack. So for instance,

131
00:07:52.011 --> 00:07:56.030
we know that in 2016,

132
00:07:56.090 --> 00:08:01.010
a hacker named cyber Zeist hacked the Alaska voting

133
00:08:01.040 --> 00:08:05.690
systems and claimed that he could change the voting tallies in any

134
00:08:05.691 --> 00:08:08.150
direction that he wanted in Alaska.

135
00:08:08.480 --> 00:08:13.370
There are also a number of myths surrounding voting that are spreading on

136
00:08:13.371 --> 00:08:14.151
social media.

137
00:08:14.151 --> 00:08:18.530
The major one is that there is widespread voter fraud.

138
00:08:18.560 --> 00:08:23.000
There is no real evidence of a systematic voter

139
00:08:23.001 --> 00:08:27.350
fraud at the level of ballots or other types of voter fraud.

140
00:08:27.351 --> 00:08:30.830
People voting twice, dead people voting and so on.

141
00:08:31.130 --> 00:08:35.720
Although there have been a very, very small handful of, uh,

142
00:08:35.780 --> 00:08:39.860
incidents that may have happened, where there is an error on a ballot.

143
00:08:39.890 --> 00:08:43.250
There has been no evidence of systematic voter fraud.

144
00:08:43.280 --> 00:08:47.210
Since we can remember about elections in the United States,

145
00:08:47.390 --> 00:08:51.950
which means that despite all of the myths floating around social media,

146
00:08:52.220 --> 00:08:56.910
we as citizens can be confident in the integrity of our elections.

147
00:08:57.090 --> 00:09:01.920
So my advice to all of us is that we vote and vote as quickly as possible

148
00:09:01.921 --> 00:09:03.980
ahead of November 3rd,

149
00:09:04.640 --> 00:09:07.700
social media algorithms are dividing our society.

150
00:09:07.940 --> 00:09:12.890
There is evidence that recommendation algorithms that social media

151
00:09:12.891 --> 00:09:17.240
uses does tend to give us more of what we want and

152
00:09:17.241 --> 00:09:21.680
therefore lock us into narrower and narrower sets of

153
00:09:21.681 --> 00:09:26.660
information. Filter. Bubbles refers to the fact that in an algorithmic world,

154
00:09:26.661 --> 00:09:30.590
we are each living in our own information bubble,

155
00:09:31.070 --> 00:09:36.050
meaning that what I see on social media is not what you see and not

156
00:09:36.051 --> 00:09:37.490
what your friends see,

157
00:09:37.730 --> 00:09:41.150
because everything that you see is tailored to you.

158
00:09:41.210 --> 00:09:46.010
And it's tailored to you by algorithms that are designed to give you more of

159
00:09:46.011 --> 00:09:48.080
what you want to keep you engaged.

160
00:09:48.320 --> 00:09:52.550
That creates these filter bubbles of information that are unique to every

161
00:09:52.551 --> 00:09:57.530
individual echo chambers are groups or communities of people

162
00:09:57.710 --> 00:10:02.180
that are sharing the same information over and over again with each other.

163
00:10:02.390 --> 00:10:05.450
And that that information stays locked in that community.

164
00:10:05.690 --> 00:10:10.010
And doesn't crossover, for instance, to the other side of the aisle,

165
00:10:10.160 --> 00:10:14.780
where different information is being constantly shared amongst a different set

166
00:10:14.781 --> 00:10:17.810
of people. So there are certain algorithms, for instance,

167
00:10:17.811 --> 00:10:22.790
the YouTube algorithm that tends to recommend more and more of the

168
00:10:22.791 --> 00:10:25.520
type of content that you seem engaged with.

169
00:10:25.520 --> 00:10:30.470
And interested in studies have shown that these types of algorithms can

170
00:10:30.471 --> 00:10:35.270
tend to lead to more extreme content being shown to the viewer.

171
00:10:35.570 --> 00:10:39.590
These algorithms are designed to be bottomless or endless meaning they keep you

172
00:10:39.591 --> 00:10:44.450
engaged in a constantly updating reel of new videos while the jury is

173
00:10:44.451 --> 00:10:49.100
out on whether this can radicalize someone or the degree to which there are

174
00:10:49.101 --> 00:10:53.900
systematic extremism outcomes that are created by these

175
00:10:53.901 --> 00:10:54.734
algorithms,

176
00:10:54.800 --> 00:10:59.750
the fact that they are sending you down rabbit holes of more and

177
00:10:59.751 --> 00:11:01.250
more content,

178
00:11:01.251 --> 00:11:05.300
similar to what you like and engage with is troubling.

179
00:11:05.450 --> 00:11:09.800
Given the notion of the filter bubble, in order to fight the filter bubble,

180
00:11:09.830 --> 00:11:12.680
we have to seek out diverse content.

181
00:11:12.710 --> 00:11:16.460
We have to follow people whose opinions are different than our own.

182
00:11:16.790 --> 00:11:20.060
We have to do searches for content.

183
00:11:20.061 --> 00:11:23.000
That is contrary to what we believe.

184
00:11:23.300 --> 00:11:26.960
We have to demonstrate to the hype machine,

185
00:11:26.961 --> 00:11:31.190
to the social media industrial complex, that we're interested in diversity,

186
00:11:31.430 --> 00:11:36.080
and that we are seeking diversity or opinions that are different from our own.

187
00:11:36.200 --> 00:11:40.790
That will help us break out of the filter bubbles that we find ourselves

188
00:11:40.820 --> 00:11:45.290
in with these algorithms. You can easily spot a deep, fake,

189
00:11:45.920 --> 00:11:50.900
deep fakes synthetic video that are generated by machine learning

190
00:11:50.960 --> 00:11:54.280
algorithms called generative adversarial networks.

191
00:11:54.310 --> 00:11:58.270
These networks have a generator and a discriminator where the

192
00:11:59.380 --> 00:12:03.910
discriminator's job is to tell real from fake videos and the generator

193
00:12:03.970 --> 00:12:08.800
tries to generate more and more convincing synthetic video till it

194
00:12:08.801 --> 00:12:13.210
fools the discriminator into believing that it's true. Now,

195
00:12:13.240 --> 00:12:17.920
the problem with deep fakes is that they're more difficult to spot every

196
00:12:17.921 --> 00:12:19.300
day that goes by.

197
00:12:19.360 --> 00:12:23.230
There are instances of audio deep fakes,

198
00:12:23.410 --> 00:12:27.220
where companies have been defrauded out of millions of dollars,

199
00:12:27.430 --> 00:12:32.050
where the CFO will be called by a synthetic

200
00:12:32.170 --> 00:12:36.430
attacker that is using the voice of the CEO

201
00:12:36.580 --> 00:12:40.990
requesting that large sums of money be transferred before the end of the

202
00:12:40.991 --> 00:12:42.700
quarter, or to close the deal.

203
00:12:42.910 --> 00:12:47.320
The reason why deep fakes are so troubling is because seeing is

204
00:12:47.321 --> 00:12:50.320
believing and a picture is worth a thousand words.

205
00:12:50.410 --> 00:12:54.820
I have seen some incredibly professionally created and

206
00:12:54.821 --> 00:12:56.680
convincing deep fakes, for instance,

207
00:12:56.681 --> 00:13:01.450
of president Barack Obama or Mark Zuckerberg or prime

208
00:13:01.451 --> 00:13:02.950
minister, Boris Johnson,

209
00:13:02.980 --> 00:13:07.690
or Kim John UHIN that really kind of skate the line

210
00:13:07.691 --> 00:13:11.800
between is this convincing or is this not as deep fakes become more

211
00:13:11.801 --> 00:13:15.850
commonplace as the technology used to create them becomes more

212
00:13:15.851 --> 00:13:18.880
democratized and more people have access to it.

213
00:13:19.120 --> 00:13:23.830
I think we're going to see a rise of a wave of synthetic

214
00:13:23.860 --> 00:13:28.510
audio and video that could become very dangerous in a political

215
00:13:28.511 --> 00:13:33.400
environment or in a commercial environment, either through fraud or through,

216
00:13:33.790 --> 00:13:35.140
uh, political manipulation.

217
00:13:35.440 --> 00:13:40.060
I think the most effective way to spot a deep fake is to, uh,

218
00:13:40.090 --> 00:13:44.140
distinguish the content of what's being said in the film.

219
00:13:44.200 --> 00:13:48.730
If you can't imagine those words coming out of the mouth of the person that

220
00:13:48.731 --> 00:13:51.040
you're viewing that as a good sign,

221
00:13:51.070 --> 00:13:55.890
that this is a deep fake social media can bring about positive change.

222
00:13:56.280 --> 00:14:00.960
Most recently we've been focused based on the potential disasters that social

223
00:14:00.961 --> 00:14:02.700
media can create in our world.

224
00:14:02.940 --> 00:14:07.830
But it's important not to forget about the tremendous potential for

225
00:14:07.831 --> 00:14:12.120
promise that social media can also bring, we know for instance,

226
00:14:12.121 --> 00:14:16.230
that when Nepal experienced the greatest earthquake that it's seen in a hundred

227
00:14:16.231 --> 00:14:16.651
years,

228
00:14:16.651 --> 00:14:21.540
Facebook spun up a donate now button and raised $15.5 million

229
00:14:21.541 --> 00:14:26.130
from 770,000 people in over a hundred countries,

230
00:14:26.160 --> 00:14:30.390
which just shows you the mobilization potential of this technology.

231
00:14:30.570 --> 00:14:35.430
It's certainly played a catalyzing and accelerant role in

232
00:14:35.431 --> 00:14:38.870
important social movements around the world. Like black lives matter,

233
00:14:38.880 --> 00:14:41.790
the Arab spring, the snow revolution in Russia,

234
00:14:41.820 --> 00:14:45.420
social mobilization in Japan and Hong Kong.

235
00:14:45.450 --> 00:14:50.100
And these kinds of social movements can really be accelerated by

236
00:14:50.130 --> 00:14:54.770
social media research at MIT and at Stanford shows that Facebook

237
00:14:54.771 --> 00:14:59.570
creates $370 billion a year in consumer

238
00:14:59.571 --> 00:15:02.150
surplus in the United States alone.

239
00:15:02.360 --> 00:15:05.510
Imagine that for the entire world,

240
00:15:05.720 --> 00:15:09.590
that's economic opportunity. That's the ability to find jobs,

241
00:15:09.591 --> 00:15:13.490
access to life-saving health information and real human connection.

242
00:15:13.700 --> 00:15:17.390
In some countries around the world, Facebook is the internet.

243
00:15:17.420 --> 00:15:22.070
It's the way that people conduct any number of human activities from market

244
00:15:22.071 --> 00:15:24.080
transactions to running their businesses,

245
00:15:24.081 --> 00:15:26.090
to staying in touch with their friends and family,

246
00:15:26.330 --> 00:15:29.930
or finding out about where to vote or how to get healthcare.

247
00:15:30.230 --> 00:15:33.950
These types of benefits are actually tremendous.

248
00:15:33.980 --> 00:15:38.570
Social media is a very powerful tool for creating such change in

249
00:15:38.571 --> 00:15:42.290
society. The real question is what are we going to use it for?

250
00:15:42.500 --> 00:15:46.430
Are we going to use it for the nefarious purposes that we've seen it be used for

251
00:15:46.431 --> 00:15:50.360
recently, or we're going to use it to bring about a better world?

252
00:15:53.920 --> 00:15:53.920
<v 1>[Inaudible].</v>

