WEBVTT

1
00:00:00.030 --> 00:00:04.770
<v 0>Let's imagine that, um, I'm the president or you're talking to the president.</v>

2
00:00:04.771 --> 00:00:08.910
I know you you've been, uh, advising the Biden campaign. Um, is that correct?

3
00:00:09.650 --> 00:00:14.190
<v 1>I advise Biden on deep fakes, uh, two years ago when they started.</v>

4
00:00:15.230 --> 00:00:17.660
<v 0>Pretty, yeah, pretty recent vintage. Um,</v>

5
00:00:19.850 --> 00:00:23.300
what are the safeguards and countermeasures that we could install?

6
00:00:23.301 --> 00:00:25.400
Because a lot of people listening to this right now and Nina are like,

7
00:00:25.430 --> 00:00:27.590
Oh my gosh, like I give up, like,

8
00:00:27.620 --> 00:00:31.310
I think I'm watching Andrew Yang talk to Nina shake, but maybe I'm not like,

9
00:00:31.311 --> 00:00:34.250
you know, this whole thing could just be like a giant fabrication,

10
00:00:34.251 --> 00:00:37.130
which it could be theoretically. Uh, how do we know? So,

11
00:00:37.460 --> 00:00:39.410
so you do make some suggestions,

12
00:00:39.411 --> 00:00:42.890
but we'd love to talk to you about what possible solutions look like.

13
00:00:42.891 --> 00:00:46.370
What could we do if let's say hypothetically,

14
00:00:46.371 --> 00:00:49.940
we have control of the government of the United States of America, 2021,

15
00:00:49.941 --> 00:00:53.780
we look up and say, all right, we have to address this problem.

16
00:00:54.380 --> 00:00:57.230
<v 1>Broadly, the solutions kind of fall into two categories.</v>

17
00:00:57.530 --> 00:01:00.110
The first is the technical solutions.

18
00:01:00.170 --> 00:01:04.340
So building the AI software to detect video

19
00:01:04.341 --> 00:01:06.800
manipulations because as synthetic media.

20
00:01:07.220 --> 00:01:09.290
<v 0>Yeah. So let's start with that. Nina,</v>

21
00:01:09.650 --> 00:01:13.910
like let's say you're Russia and you make a hundred synthetic, uh,

22
00:01:13.940 --> 00:01:16.040
media videos and audio recordings.

23
00:01:18.260 --> 00:01:21.470
Do we have the technology right now that could go through and be like fake,

24
00:01:21.471 --> 00:01:22.940
fake, fake, fake, fake, fake,

25
00:01:22.941 --> 00:01:24.860
and then there's one real one in there and be like real.

26
00:01:26.510 --> 00:01:28.600
<v 1>So right. Um,</v>

27
00:01:29.060 --> 00:01:33.920
ish is the answer because there are several kind of deep

28
00:01:33.950 --> 00:01:35.750
fake detection tools,

29
00:01:35.990 --> 00:01:40.580
but given that they can be generated using different kind of, um,

30
00:01:40.730 --> 00:01:42.000
machine learning systems,

31
00:01:42.290 --> 00:01:46.340
there's no one detection tool that can detect them all.

32
00:01:46.640 --> 00:01:51.350
And the other crazy thing about trying to build the detection tools is that

33
00:01:51.560 --> 00:01:54.800
every time you think you've built a really cool piece of software,

34
00:01:54.801 --> 00:01:58.430
AI software to detect all the fakes, um,

35
00:01:58.520 --> 00:02:02.660
you're giving the person who's making the fakes a foot up.

36
00:02:02.661 --> 00:02:04.880
So it's always going to be a cat and mouse game.

37
00:02:04.881 --> 00:02:06.600
So there are some tools out there.

38
00:02:06.601 --> 00:02:09.020
There is no one universal tool is the short answer.

39
00:02:09.800 --> 00:02:14.060
<v 0>Could you have a tool that gives you like a spectrum of</v>

40
00:02:14.061 --> 00:02:17.960
probability where you could say this thing is probably real,

41
00:02:18.140 --> 00:02:22.430
this thing might be fake, uh, within like a certain measure of confidence.

42
00:02:22.431 --> 00:02:26.360
I get that you can't categorically say real fake, but you could be like,

43
00:02:26.361 --> 00:02:29.630
we think this one's probably real, or we think this one's probably fake.

44
00:02:30.260 --> 00:02:33.200
<v 1>Yeah. And, and that is how those detection tools work right now.</v>

45
00:02:33.201 --> 00:02:35.060
The ones that are already out there, they're like, Oh,

46
00:02:35.061 --> 00:02:39.650
this is 90% probably synthetic. Um, but, uh,

47
00:02:39.680 --> 00:02:44.120
I would say that because we're just at the start of this synthetic media

48
00:02:44.121 --> 00:02:44.750
revolution,

49
00:02:44.750 --> 00:02:49.670
like we haven't actually come to the stage yet where it's ubiquitous,

50
00:02:49.880 --> 00:02:53.660
but in four years' time, when it is time for the next election,

51
00:02:53.900 --> 00:02:58.850
some experts who I speak to think that as much as 90% of video content online

52
00:02:58.970 --> 00:03:00.250
can be synthetic. So

53
00:03:03.520 --> 00:03:07.330
there is an urgency which isn't,

54
00:03:07.480 --> 00:03:10.420
so it hasn't hit us yet, but in four years' time,

55
00:03:10.421 --> 00:03:12.400
we're looking at something completely different.

56
00:03:12.401 --> 00:03:16.960
I actually think this election is more likely to be disrupted by

57
00:03:16.961 --> 00:03:20.830
something called the Liar's dividend. If everything can be faked,

58
00:03:20.890 --> 00:03:23.080
then everything can also be denied.

59
00:03:23.380 --> 00:03:27.040
And that is something called a Liar's dividend.

60
00:03:27.041 --> 00:03:30.310
And you already see that happening now.

61
00:03:30.370 --> 00:03:33.880
And this is something that's only going to get more potent as time goes on.

62
00:03:33.881 --> 00:03:36.070
There's actually one candidate,

63
00:03:36.071 --> 00:03:40.840
a Republican sounded at standing for Congress who took the George Floyd video,

64
00:03:40.900 --> 00:03:44.290
that powerful video, which unleashed a movement,

65
00:03:44.320 --> 00:03:47.710
not only across the U S but across the world.

66
00:03:48.040 --> 00:03:51.820
And she's written a 24 page documents saying that it's a deep fake,

67
00:03:52.000 --> 00:03:53.380
so you're going to see a lot more of that.

68
00:03:55.470 --> 00:03:59.190
<v 0>Okay. So there, isn't an easy technical fix. Uh,</v>

69
00:03:59.191 --> 00:04:03.660
you can have some AI tools that catch some degree of,

70
00:04:04.350 --> 00:04:07.500
uh, synthetic media or some proportion of it,

71
00:04:07.860 --> 00:04:11.250
but not enough where you can say problem solved. So what else,

72
00:04:11.280 --> 00:04:12.600
what else lies in the technical? Yeah.

73
00:04:12.630 --> 00:04:16.590
<v 1>So, so that the one, the first part of technical solutions is detection.</v>

74
00:04:16.890 --> 00:04:19.620
The second part is provenance, right?

75
00:04:19.621 --> 00:04:24.000
So you actually try to prove what media is

76
00:04:24.001 --> 00:04:24.691
authentic.

77
00:04:24.691 --> 00:04:28.710
And this is going to become increasingly important for like activists,

78
00:04:28.770 --> 00:04:29.640
journalists,

79
00:04:30.000 --> 00:04:34.110
people who are trying to record something and hold

80
00:04:35.010 --> 00:04:38.490
someone either to account as a kind of human rights abuse,

81
00:04:38.491 --> 00:04:41.190
or to show that what they have done is authentic.

82
00:04:41.191 --> 00:04:45.930
And you basically try and embed that into almost the hardware of your

83
00:04:45.931 --> 00:04:49.620
phone or your camera. It's almost like a watermark, which is indelible.

84
00:04:49.650 --> 00:04:53.820
You can't tamper it. So that always is shown to be authentic.

85
00:04:54.090 --> 00:04:58.260
So those are the kinds of two technical solutions, detection and provenance,

86
00:04:58.830 --> 00:05:03.780
but ultimately this isn't a PR this isn't a technology problem

87
00:05:03.781 --> 00:05:07.860
in the sense that the technology here until, I mean, we have fully sentient AI,

88
00:05:07.861 --> 00:05:11.910
which we don't the technology assistant amplifier of human intention.

89
00:05:12.210 --> 00:05:16.470
So the real solution comes with policy

90
00:05:16.890 --> 00:05:21.570
legal, uh, digital and digital literacy and education.

91
00:05:22.350 --> 00:05:25.380
Um, this is something that just the technology isn't going to make go away.

92
00:05:25.381 --> 00:05:29.820
It's something that society needs to grasp as a national

93
00:05:29.821 --> 00:05:32.070
security. And I would say, you know,

94
00:05:32.071 --> 00:05:35.520
one of the most important issues for the future of liberal democracy.

95
00:05:35.521 --> 00:05:40.080
So we need people like you who understand how

96
00:05:40.081 --> 00:05:42.810
technology is reshaping politics and society,

97
00:05:42.811 --> 00:05:47.580
and then can communicate that to the public in a way that they understand

98
00:05:47.581 --> 00:05:52.260
what's at stake here then only can I hope that there is kind of the

99
00:05:52.261 --> 00:05:56.970
political and public will galvanized in order to make this, uh,

100
00:05:57.050 --> 00:05:59.690
the priority, which I believe it needs to be,

101
00:05:59.691 --> 00:06:04.190
because I think there's a short window in which something can still be done.

102
00:06:04.430 --> 00:06:09.170
And otherwise the kind of vested powers that benefit from the

103
00:06:09.171 --> 00:06:11.810
information ecosystem being the way that it is.

104
00:06:12.230 --> 00:06:15.980
It becomes increasingly difficult to do anything about it.

105
00:06:17.020 --> 00:06:19.180
<v 0>So right now, in the United States,</v>

106
00:06:19.210 --> 00:06:23.920
we have deep skepticism of the media. I think the trust in media that,

107
00:06:24.400 --> 00:06:27.970
um, I saw most recently, it was 41%, the major media organizations,

108
00:06:28.360 --> 00:06:31.300
and it runs the gamut, uh, along party lines.

109
00:06:31.360 --> 00:06:33.880
So if you are a Democrat in the United States of America,

110
00:06:34.120 --> 00:06:38.590
you trust national media at a higher level, let's call it 58% or 60%.

111
00:06:38.950 --> 00:06:41.620
And then independence are around 40%.

112
00:06:41.621 --> 00:06:45.850
And then Republicans are at something like 20%. So, so that's, uh,

113
00:06:46.090 --> 00:06:50.800
it's been migrating downward over time. So that's where we are right now.

114
00:06:51.280 --> 00:06:55.780
So then if you go and say, look, we have to get our arms around this, uh,

115
00:06:55.840 --> 00:06:59.050
and establish what the heck is real. And what's not real,

116
00:06:59.080 --> 00:07:00.880
like what's true and false.

117
00:07:01.150 --> 00:07:04.570
You have a significant proportion of the American people already that are like,

118
00:07:04.571 --> 00:07:09.130
whatever you're doing. Like I don't trust it is nonsense. Like,

119
00:07:09.160 --> 00:07:13.780
you know, you're, you're trying to control me, control us. And there's a very,

120
00:07:13.781 --> 00:07:18.220
very deep heritage in the U S um, which I agree with,

121
00:07:18.910 --> 00:07:21.340
uh, of freedom of the press, freedom of information,

122
00:07:21.520 --> 00:07:23.110
like censorship is terrible.

123
00:07:23.111 --> 00:07:26.650
And it seems like if you're trying to establish what's real or true,

124
00:07:27.340 --> 00:07:28.450
then what you're really trying to do,

125
00:07:28.451 --> 00:07:32.120
essentially the heck out of a bunch of people. So that's like the,

126
00:07:32.121 --> 00:07:36.430
the current landscape. Um, and then you go to them and say, no, look,

127
00:07:36.460 --> 00:07:38.650
this is a massive problem. Like,

128
00:07:38.651 --> 00:07:43.360
you don't even know if I'm real right now because I got a video

129
00:07:43.361 --> 00:07:45.640
screen. Uh,

130
00:07:45.670 --> 00:07:50.530
so we would need to line up in my view, major media organizations,

131
00:07:50.560 --> 00:07:55.210
major technology companies, uh, nonprofits and the government,

132
00:07:55.510 --> 00:07:58.180
and get everyone together and say, all right, like,

133
00:07:58.210 --> 00:08:02.800
let us try and establish some standards and rules of the road to say that, uh,

134
00:08:02.920 --> 00:08:06.670
you have to use these devices. And if you don't use these devices, then, uh,

135
00:08:06.671 --> 00:08:09.620
it will be marked as don't know if this is real, uh, you know, and,

136
00:08:09.730 --> 00:08:12.160
and that can include our cell phones. So at least, you know,

137
00:08:12.161 --> 00:08:15.010
if I take a cell phone video, it's attributed to me and like, you know,

138
00:08:15.040 --> 00:08:18.580
so it's not like you necessarily need to have a giant camera from, um,

139
00:08:18.610 --> 00:08:22.270
from CNN or wherever, but like there, there needs to be some, uh,

140
00:08:22.930 --> 00:08:23.560
provenance to your point,

141
00:08:23.560 --> 00:08:26.770
there needs to be some kind of chain of ownership of this.

142
00:08:26.771 --> 00:08:29.950
And then someone like puts their name on it and says, yeah, I was there.

143
00:08:29.951 --> 00:08:32.230
And that happened. Um, so that's,

144
00:08:32.231 --> 00:08:36.970
the challenge is trying to bring that coalition together and then try and get

145
00:08:36.971 --> 00:08:41.410
this trust in whatever standards you establish, um,

146
00:08:41.440 --> 00:08:43.420
to rise over time.

147
00:08:43.630 --> 00:08:47.830
<v 1>If you look at other places where society-wide</v>

148
00:08:47.831 --> 00:08:51.850
mobilization to disinformation has been really effective, take, for example,

149
00:08:51.851 --> 00:08:54.070
the small Baltic state of Estonia,

150
00:08:54.100 --> 00:08:58.560
some 3 million they've often been in the eye of kind of Russian disinformation

151
00:08:58.561 --> 00:08:59.394
campaigns.

152
00:08:59.520 --> 00:09:04.170
And they really turned it around by basically adopting what you just pointed

153
00:09:04.171 --> 00:09:07.380
out, which is a whole society wide mobilization,

154
00:09:07.381 --> 00:09:11.130
where you have kind of industry working with policy makers,

155
00:09:11.131 --> 00:09:13.560
working with citizens, working with NGOs,

156
00:09:13.980 --> 00:09:18.240
but taking that network approach is really difficult. Um,

157
00:09:18.300 --> 00:09:22.890
and I would say that if the current presidency continues in the

158
00:09:22.891 --> 00:09:23.910
United States,

159
00:09:24.150 --> 00:09:28.920
then that challenge becomes even more difficult because what's quite clear

160
00:09:29.310 --> 00:09:33.600
is that the president has a vested interest in making sure that the information

161
00:09:33.601 --> 00:09:36.090
ecosystem is corroded.

162
00:09:36.270 --> 00:09:38.760
He is one of the biggest purveyors of disinformation.

163
00:09:39.210 --> 00:09:42.180
And if this continues for another four years,

164
00:09:42.210 --> 00:09:46.830
I find it difficult to see the way back for the United States.

165
00:09:47.300 --> 00:09:47.451
<v 0>No,</v>

166
00:09:47.451 --> 00:09:50.210
that's the argument that so many people are making is that we can't handle four

167
00:09:50.211 --> 00:09:52.130
more years of Trump. I happen to agree with that.

168
00:09:52.190 --> 00:09:55.760
I think that four more years of disintegration, uh,

169
00:09:55.761 --> 00:09:59.840
will lead us to a point where it's very difficult to come back. Um,

170
00:09:59.960 --> 00:10:03.200
though many people think of it for different reasons than what you're describing

171
00:10:03.620 --> 00:10:08.600
Nina, but there have been folks who've said that U S democracy

172
00:10:08.601 --> 00:10:09.740
itself is on the line.

173
00:10:10.850 --> 00:10:15.360
<v 1>Yeah. And I, I don't think that's an exaggeration at all, because if, I mean,</v>

174
00:10:15.450 --> 00:10:17.300
democracy can only work.

175
00:10:17.301 --> 00:10:21.920
Liberal democracy can only work if you have some kind of objective

176
00:10:21.921 --> 00:10:25.580
reality and some commitment to truth and the pursuit of truth.

177
00:10:25.880 --> 00:10:29.900
If the pursuit of truth no longer matters than the only thing that matters is

178
00:10:29.901 --> 00:10:32.840
power and who has the power to control narratives.

179
00:10:33.140 --> 00:10:36.920
So that then ends up becoming very authoritarian.

180
00:10:37.280 --> 00:10:38.960
<v 0>Well, you know what? This is Nina. Like,</v>

181
00:10:38.961 --> 00:10:43.280
this is a problem that prevents us from solving other problems. Uh, and so that,

182
00:10:43.580 --> 00:10:46.910
and so that, that's where my attention is focused right now is, um,

183
00:10:47.990 --> 00:10:51.470
is we have to fix the machinery of democracy. Uh,

184
00:10:51.530 --> 00:10:55.790
and at this point it's not enough to try and get the right people embedded into

185
00:10:55.791 --> 00:10:56.271
the machine.

186
00:10:56.271 --> 00:10:59.780
You have to try and fix the machine itself because right now this machine,

187
00:11:00.530 --> 00:11:04.280
this machine is breaking down in a way that even if I stuck like a good person,

188
00:11:04.940 --> 00:11:07.310
uh, in Congress, it's not going to save us.

189
00:11:08.090 --> 00:11:11.000
<v 1>Absolutely focus on the architecture of the machine.</v>

190
00:11:11.030 --> 00:11:13.820
I think we're absolutely on the same page here, Andrew.

191
00:11:14.480 --> 00:11:18.410
<v 0>Okay. I'm going to become the mechanic Nina, and I know you are too.</v>

192
00:11:18.650 --> 00:11:20.810
Thank you so much for edifying us.

193
00:11:20.870 --> 00:11:25.220
Nina's book is deep fakes that coming in FACA lips, uh,

194
00:11:25.250 --> 00:11:27.110
it's compelling. It's scary.

195
00:11:27.140 --> 00:11:29.240
And hopefully we will do something about it together.

196
00:11:30.500 --> 00:11:34.580
Thank you for listening in. I hope you enjoyed this conversation. If you did,

197
00:11:34.820 --> 00:11:36.200
please do subscribe to yang,

198
00:11:36.201 --> 00:11:39.800
speaks and click on notifications so we can let you know every time we have a

199
00:11:39.801 --> 00:11:40.310
new episode.

