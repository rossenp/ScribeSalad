WEBVTT

1
00:00:00.300 --> 00:00:02.790
<v 0>I ran for president as you know, uh,</v>

2
00:00:02.850 --> 00:00:07.500
and there was a time when it felt like there are game mechanics at play during

3
00:00:07.501 --> 00:00:09.180
the campaign there. Uh,

4
00:00:09.181 --> 00:00:12.480
when I called it out as a reality TV show at one point on the debates,

5
00:00:12.481 --> 00:00:16.560
but then I was still participating in the reality TV show. And for me,

6
00:00:18.270 --> 00:00:22.650
it hit home when I did something that was totally random where I was in South

7
00:00:22.650 --> 00:00:23.550
Carolina. And there were,

8
00:00:23.640 --> 00:00:27.720
there was a Jazzercise class and there were women doing the Cupid shuffle,

9
00:00:27.750 --> 00:00:30.240
and then not wanting to be rude. I was like, okay, like,

10
00:00:30.270 --> 00:00:33.420
let me do it with you and just have some fun. Um,

11
00:00:33.480 --> 00:00:38.070
and I literally forgot that we had like a journalist in tow, um,

12
00:00:38.100 --> 00:00:42.870
at the time who just took the video on their phone and then it

13
00:00:42.871 --> 00:00:44.640
wound up. And then to me,

14
00:00:44.760 --> 00:00:49.470
the shockingly people gave a about me doing the Cupid shuffle,

15
00:00:49.830 --> 00:00:54.450
such that it went around the, uh, social media landscape.

16
00:00:54.480 --> 00:00:58.800
And then it went to mainstream news where I was part of like a,

17
00:00:59.190 --> 00:01:03.180
I did a Sunday morning network TV appearance,

18
00:01:03.181 --> 00:01:07.920
and they use that as like my intro clip. Um, whereas like here's Andrew. Yeah.

19
00:01:07.921 --> 00:01:11.970
He's having fun on the trail. And I was like, what the hell is going on?

20
00:01:12.720 --> 00:01:15.730
Where were literally running?

21
00:01:15.731 --> 00:01:18.990
I got this whole big set of ideas that obviously like, you know, I,

22
00:01:18.991 --> 00:01:23.250
I think vitally important for our nation's future. And it's like,

23
00:01:23.251 --> 00:01:27.600
but that doesn't matter. You don't matter. It's like you keep it shuffling, uh,

24
00:01:27.630 --> 00:01:28.463
in South Carolina.

25
00:01:29.120 --> 00:01:30.890
<v 1>Yeah. And I, I mean, I think that there's,</v>

26
00:01:30.920 --> 00:01:34.280
I think there's like some legitimacy to this and you've talked about it a little

27
00:01:34.281 --> 00:01:39.200
bit on the podcast before that, you know, people believe stories and like,

28
00:01:39.201 --> 00:01:43.160
we want to find stories that we fit into and we want to find stories that other

29
00:01:43.161 --> 00:01:47.720
people fit into. And so if, you know, that's kind of a lot of what politics is,

30
00:01:47.840 --> 00:01:51.860
is like trying to tell a story around your opponent and tell a story around

31
00:01:51.861 --> 00:01:55.310
yourself and make those the most compelling stories and more compelling than

32
00:01:55.311 --> 00:01:59.900
their stories, because, and like, this is a shame to some extent,

33
00:02:00.320 --> 00:02:02.870
but, but also, you know,

34
00:02:02.900 --> 00:02:07.670
this is a democracy and most people do not have anything like the

35
00:02:07.671 --> 00:02:10.340
amount of time that you and I have to think about this stuff.

36
00:02:10.880 --> 00:02:13.130
And so we need simplifying stories.

37
00:02:13.131 --> 00:02:17.750
We need ways for people to be engaged and informed and to, and to,

38
00:02:17.930 --> 00:02:22.790
you know, without having to spend 40 hours a week on it. And,

39
00:02:23.570 --> 00:02:26.750
uh, and so like, you know, to some extent it's like, this is,

40
00:02:26.780 --> 00:02:29.720
this is terrible and, and very concerning. And to some extent,

41
00:02:29.721 --> 00:02:31.700
like those things do have to exist.

42
00:02:31.701 --> 00:02:36.620
We just have to be as citizens really aware of how stories function

43
00:02:36.710 --> 00:02:40.010
and, and wary of, uh, how,

44
00:02:40.100 --> 00:02:42.170
how they're going to be used to manipulate us.

45
00:02:43.460 --> 00:02:46.310
<v 0>So if we look at the social media platforms, we say, okay,</v>

46
00:02:46.311 --> 00:02:48.050
there are serious problems with them.

47
00:02:48.290 --> 00:02:52.970
And I have a few policy proposals that I put out on the trail

48
00:02:53.720 --> 00:02:56.330
and we launched the data dividend project,

49
00:02:56.331 --> 00:02:58.100
which I just had this long meeting about.

50
00:02:58.520 --> 00:03:00.730
And here's where gets interesting is where, uh,

51
00:03:00.760 --> 00:03:04.390
this is the story we're trying to tell to your point is like story one.

52
00:03:04.391 --> 00:03:07.420
When you say to the average American, Hey,

53
00:03:07.690 --> 00:03:10.960
your data is yours and you should have data dignity.

54
00:03:10.990 --> 00:03:13.630
It's about like human autonomy and free will.

55
00:03:13.990 --> 00:03:17.290
And then an American will hear that and be like, yeah, yeah, I agree with that.

56
00:03:17.560 --> 00:03:20.140
Um, but then they, but they won't really do anything. I mean,

57
00:03:20.141 --> 00:03:23.080
it's like just click on, I consent, uh, you know,

58
00:03:23.110 --> 00:03:26.680
use these platforms and just ignore and hope for the best. Cause like you said,

59
00:03:26.681 --> 00:03:27.790
I don't have any time I'm busy.

60
00:03:28.770 --> 00:03:32.610
<v 1>Yep. Oh yeah. I'd rather trade my, my convenience for my privacy any day,</v>

61
00:03:32.700 --> 00:03:33.600
I'll take that deal.

62
00:03:34.230 --> 00:03:38.370
<v 0>So that was a story one and a number of people have tried that story and it</v>

63
00:03:39.180 --> 00:03:43.050
has not gone anywhere. And then we'd looked at it and said, okay,

64
00:03:43.051 --> 00:03:44.340
you know what our story is going to be.

65
00:03:44.850 --> 00:03:49.080
They are making tens of billions of dollars a year off of you and your data.

66
00:03:49.320 --> 00:03:50.580
Shouldn't you get some of that.

67
00:03:50.610 --> 00:03:54.720
Why don't we just get you this data dividend sign up here and we'll go fight for

68
00:03:54.721 --> 00:03:57.030
your right to get paid for your data.

69
00:03:57.660 --> 00:04:02.280
And so I'm very excited about that. Cause I think that's a winning story. And,

70
00:04:02.370 --> 00:04:07.230
uh, we we've now had, um, tens of thousands of people sign up, um, in,

71
00:04:07.231 --> 00:04:09.990
in the first week and we were fighting on that side.

72
00:04:10.290 --> 00:04:12.930
And then there were some people that came back and said, Hey,

73
00:04:13.200 --> 00:04:17.100
that isn't the policy goal. You should have, uh,

74
00:04:17.101 --> 00:04:19.920
people getting paid for their data because they're not going to get paid enough.

75
00:04:19.920 --> 00:04:20.650
And it, you know,

76
00:04:20.650 --> 00:04:24.520
it's still going to continue some of like the darker issues that I think you

77
00:04:24.690 --> 00:04:28.860
driving at, um, in your book. And I looked at it, I, you know,

78
00:04:28.861 --> 00:04:29.940
looked at it and was like,

79
00:04:30.450 --> 00:04:35.220
so you don't want us to try and get people paid for their data because you

80
00:04:35.221 --> 00:04:39.270
think there's like a better policy that frankly I wouldn't even necessarily

81
00:04:39.271 --> 00:04:39.931
disagree with.

82
00:04:39.931 --> 00:04:44.700
Like if you gave me the power to pass like a better data rights policy that made

83
00:04:44.701 --> 00:04:48.120
it. So these practices were off the table, like I would be thrilled.

84
00:04:48.121 --> 00:04:52.230
I would sign that. But you know, like right now that's not feasible.

85
00:04:52.770 --> 00:04:54.750
Like Congress is not going to take that up. So in the meantime,

86
00:04:54.751 --> 00:04:56.970
let's like try and activate people around this. Um,

87
00:04:56.971 --> 00:05:01.740
so I hear what you're saying that like a lot of what we're

88
00:05:01.741 --> 00:05:06.600
engaged with is different appeals, different stories, different storylines,

89
00:05:07.500 --> 00:05:12.120
uh, thinking that somehow we're going to be communicating purely on a rational

90
00:05:12.121 --> 00:05:16.170
basis, uh, is silly, I suppose.

91
00:05:16.770 --> 00:05:18.120
<v 1>Yeah. Yeah. I mean, I think that,</v>

92
00:05:18.220 --> 00:05:21.330
I think that there's a story to tell around rationality, for sure.

93
00:05:21.450 --> 00:05:24.120
And I think that that will resonate with some people and not with others.

94
00:05:24.710 --> 00:05:26.490
<v 0>And that's what happened with my campaign, Hank,</v>

95
00:05:26.520 --> 00:05:31.200
where like I started out communicating very rationally and I think it attracted

96
00:05:31.201 --> 00:05:35.790
a particular tribe people now that tribe was less than

97
00:05:35.791 --> 00:05:39.510
51%. So then we were looking around being like, okay,

98
00:05:39.511 --> 00:05:44.010
like how can we try and get someone else to join this tribe?

99
00:05:44.220 --> 00:05:46.560
<v 1>We've got all the people we're going to get with this strategy.</v>

100
00:05:46.590 --> 00:05:49.080
So let's expand to the strategy. Yeah.

101
00:05:49.530 --> 00:05:53.910
<v 0>Yeah. So, so that was one of the things that I was engaged with. Uh, but, uh,</v>

102
00:05:53.911 --> 00:05:55.350
to push a little bit further.

103
00:05:55.770 --> 00:05:59.900
So if you were in the room with,

104
00:05:59.930 --> 00:06:04.610
let's say members of Congress or the president or something, and they were just,

105
00:06:04.640 --> 00:06:09.600
and I'm going to play act, um, as Joe Biden, let's say maybe I'll,

106
00:06:09.601 --> 00:06:12.770
I'll pretend to be Joe. Sure. Why not? Um, it'll be.

107
00:06:13.100 --> 00:06:14.960
<v 1>Easier than the alternative. Yeah.</v>

108
00:06:15.140 --> 00:06:18.860
<v 0>Yeah. I mean, I think it's going to be, Joe is people know. So,</v>

109
00:06:20.750 --> 00:06:25.520
uh, so Joe looks at you and says, Hank, uh, congratulations. My, um,

110
00:06:26.000 --> 00:06:28.670
grandchildren are fans of yours. Uh,

111
00:06:28.790 --> 00:06:32.090
what should we do about these social media platforms, uh,

112
00:06:32.091 --> 00:06:36.740
to improve things for the next generation? Well.

113
00:06:37.600 --> 00:06:38.440
<v 1>I mean, this is,</v>

114
00:06:38.560 --> 00:06:42.190
so the first thing I think is that there are antitrust concerns here.

115
00:06:42.250 --> 00:06:44.530
I don't think that YouTube and Google should be the same company.

116
00:06:44.800 --> 00:06:47.440
I don't know that Google and Android should be the same company.

117
00:06:47.830 --> 00:06:52.810
I don't think that AWS and Amazon should be the same company like that to

118
00:06:52.811 --> 00:06:56.560
me, feels like there's just too much consolidated power there.

119
00:06:56.561 --> 00:07:01.060
And it's interesting because like I know some of the people high up at YouTube

120
00:07:01.090 --> 00:07:02.860
fairly well. I think that they're really good people.

121
00:07:02.861 --> 00:07:06.100
And I think that they believe that what they are doing is the right thing for

122
00:07:06.101 --> 00:07:06.934
the world.

123
00:07:07.150 --> 00:07:12.100
But what I don't believe is that any unelected leader should have as

124
00:07:12.101 --> 00:07:15.170
much power as they have. And, and,

125
00:07:15.240 --> 00:07:18.970
and it's not power over the government, it's power over our lives.

126
00:07:19.210 --> 00:07:23.860
And w and so I think that we need to make space for there to be

127
00:07:23.861 --> 00:07:28.030
competition here. And if there are,

128
00:07:28.120 --> 00:07:31.810
if there is a great D if there are these moats that are created by these

129
00:07:31.840 --> 00:07:35.290
companies, like, and maybe the, the, the anti-trust, isn't just breaking up,

130
00:07:35.320 --> 00:07:39.130
or maybe it's, it's something other than breaking up where, you know,

131
00:07:39.160 --> 00:07:42.190
you prevent companies from buying competitors,

132
00:07:42.191 --> 00:07:44.830
which used to be a thing like this used to be rules around this. You,

133
00:07:44.831 --> 00:07:49.750
you can't have them buy competitors to put them out of business. Um,

134
00:07:49.960 --> 00:07:52.240
or, and you also just, can't like,

135
00:07:52.270 --> 00:07:55.720
just sort of like gobble up the people who are going to take your market share,

136
00:07:56.020 --> 00:08:00.040
because like, we need more than one and we need more than 10,

137
00:08:00.041 --> 00:08:03.310
ideally different companies in this so that they can innovate different ways.

138
00:08:03.311 --> 00:08:06.310
They can create platforms that are going to appeal to different types of people

139
00:08:06.760 --> 00:08:10.390
there, and like create, um, you know,

140
00:08:10.391 --> 00:08:12.370
tools that they wouldn't have thought of.

141
00:08:12.400 --> 00:08:15.160
And that are going to be better for folks. Now,

142
00:08:16.000 --> 00:08:18.250
obviously there are network effects when it comes to this stuff.

143
00:08:18.251 --> 00:08:19.540
Like I upload stuff to YouTube,

144
00:08:19.570 --> 00:08:21.580
not because I think it's the best video platform,

145
00:08:21.581 --> 00:08:24.490
but because that's where the eyeballs are. And I,

146
00:08:24.520 --> 00:08:27.100
I'm never going to change that until the eyeballs are somewhere else,

147
00:08:27.101 --> 00:08:31.900
but I think that there are ways to open that up and, um, and,

148
00:08:32.110 --> 00:08:35.800
and create. And so I, I think that's, that's the first thing is to say,

149
00:08:35.801 --> 00:08:39.970
how do we like recreate a competitive landscape here?

150
00:08:40.180 --> 00:08:43.740
And whether that's breaking them up, like we did with, uh, the,

151
00:08:43.741 --> 00:08:48.490
the bells or whether it's, um, you know, creating some kind of,

152
00:08:49.090 --> 00:08:52.870
uh, forces against monopoly, because right now,

153
00:08:53.770 --> 00:08:56.430
like it's great for their stock. Like, but like,

154
00:08:56.580 --> 00:08:59.520
there's no doubt in anyone's mind that these companies are building really big

155
00:08:59.521 --> 00:09:04.440
moats and that they're getting, you know, and they're, they're building moats,

156
00:09:04.470 --> 00:09:07.110
not just around sort of customer interactions, but in,

157
00:09:07.111 --> 00:09:11.070
in some cases what I'm starting to imagine as more citizenship interactions

158
00:09:11.071 --> 00:09:15.050
where we don't these aren't, these aren't companies, we buy stuff from there.

159
00:09:15.060 --> 00:09:18.180
Companies who create the places in which we live.

160
00:09:18.750 --> 00:09:21.840
And that's a very different dynamic than like somebody who, you know,

161
00:09:21.841 --> 00:09:26.560
I'm buying pants from when, when it's, uh, when it's in many ways, the,

162
00:09:26.561 --> 00:09:29.920
the spaces on which, uh, the,

163
00:09:29.921 --> 00:09:31.800
the public discourse is had,

164
00:09:31.801 --> 00:09:34.470
and in places in which I sort of feel like I live my life.

165
00:09:35.000 --> 00:09:38.780
<v 0>That's actually a great observation where people, I think,</v>

166
00:09:38.781 --> 00:09:43.040
think to themselves, well, what's the big deal. Like I go on these things and,

167
00:09:43.041 --> 00:09:45.590
you know, they, they reach me with ads,

168
00:09:45.591 --> 00:09:50.450
but you said it in a really profound way. It's like, this is where you live now,

169
00:09:51.020 --> 00:09:54.140
you know, like, this is how you're experiencing the world.

170
00:09:54.350 --> 00:09:57.290
This is how you're developing your point of view.

171
00:09:57.770 --> 00:10:01.580
<v 1>Your worldviews and your values, and like your relationships. You know,</v>

172
00:10:01.670 --> 00:10:02.331
I have many,

173
00:10:02.331 --> 00:10:05.750
many friends who I know entirely through social media and have never met in the

174
00:10:05.751 --> 00:10:09.650
real world. I started a business that is now like, I, I, you know,

175
00:10:09.680 --> 00:10:13.460
makes millions of dollars a year with a guy who I did not meet.

176
00:10:13.630 --> 00:10:18.140
And for like five years into running the business with him and, uh, not,

177
00:10:18.370 --> 00:10:21.800
not in the real world. I mean, and that, like,

178
00:10:23.510 --> 00:10:24.860
I really do think that, like,

179
00:10:25.820 --> 00:10:30.410
we are going to eventually realize that these companies are more than companies

180
00:10:30.411 --> 00:10:32.300
and in a way they are their own governments.

181
00:10:32.540 --> 00:10:37.160
And that like the board of Facebook and Cheryl, uh,

182
00:10:37.220 --> 00:10:40.940
and, and Mark are like the government of that company.

183
00:10:40.941 --> 00:10:44.240
Like they're the leaders and that we need to, you know,

184
00:10:44.270 --> 00:10:46.460
like there are different ways to exert pressure on them.

185
00:10:46.700 --> 00:10:50.900
And like there's internal ways like their employees and there's advertisers,

186
00:10:50.901 --> 00:10:54.740
which we're seeing now there's actual regulations from the government.

187
00:10:54.770 --> 00:10:58.520
There's also pushes from users and pushers from pushes,

188
00:10:58.521 --> 00:11:02.270
from like the creators who are, you know, really big on the platforms.

189
00:11:02.600 --> 00:11:07.520
We need to start thinking about how we like affect the governance of those

190
00:11:07.521 --> 00:11:11.420
companies, because they govern the spaces in which we live and,

191
00:11:11.820 --> 00:11:14.750
and imagining. And I do this, you know, in the second book,

192
00:11:14.780 --> 00:11:18.680
pretty obviously imagining that these places are, are not companies,

193
00:11:18.681 --> 00:11:23.300
but they are in a way countries is, and they are countries that are, you know,

194
00:11:23.330 --> 00:11:27.710
sort of totalitarian autocracies by capitalist autocracy.

195
00:11:28.640 --> 00:11:31.790
<v 0>Like presiding over his</v>

196
00:11:32.090 --> 00:11:37.010
2.2 billion subjects say today,

197
00:11:37.340 --> 00:11:41.300
you will see this and you will like it. And it will be good.

198
00:11:42.110 --> 00:11:45.590
<v 1>Yeah. And, and like the, the algorithms now are extremely sophisticated,</v>

199
00:11:45.591 --> 00:11:49.580
but I think they're nothing like what they're going to be and 10 or 20 years.

200
00:11:50.060 --> 00:11:52.250
And when I talked to it, like well-meaning, uh,

201
00:11:52.280 --> 00:11:55.310
business leaders in Valley who are running these big companies and,

202
00:11:55.311 --> 00:11:59.020
and have a lot of, you know, maybe more influenced than they realize they have.

203
00:12:00.040 --> 00:12:03.040
Um, I kind of want to ask them, like, not just like,

204
00:12:03.041 --> 00:12:04.510
how do you make sure you do this right.

205
00:12:04.511 --> 00:12:07.210
But how do you make sure that once you've consolidated this bunch of power in

206
00:12:07.211 --> 00:12:10.090
one place, the person who takes over after you're gone,

207
00:12:10.210 --> 00:12:11.440
how do you make sure that they do it right?

208
00:12:11.710 --> 00:12:15.160
How do you make sure in like 50 years when, you know,

209
00:12:15.220 --> 00:12:20.020
Facebook looks a lot sort of older and stodgier, and it's sort of like on its,

210
00:12:20.380 --> 00:12:21.730
you know, maybe on its way out,

211
00:12:21.731 --> 00:12:24.430
maybe there are competitors and they're trying everything they can to,

212
00:12:24.630 --> 00:12:27.820
to keep that shareholder value increasing. Like, what does that world look like?

213
00:12:28.120 --> 00:12:29.770
Not just, what does the world now look like?

214
00:12:30.220 --> 00:12:33.310
And they don't really tend to respond. What will that question?

215
00:12:33.311 --> 00:12:35.740
I think it kind of scares them to think 50 years in the future.

216
00:12:36.600 --> 00:12:40.650
<v 2>Thank you for listening in. I hope you enjoyed this conversation. If you did,</v>

217
00:12:40.950 --> 00:12:42.300
please do subscribe to yang,

218
00:12:42.301 --> 00:12:45.030
speaks and click on notifications so we can let you know,

219
00:12:45.031 --> 00:12:46.470
every time we have a new episode.

