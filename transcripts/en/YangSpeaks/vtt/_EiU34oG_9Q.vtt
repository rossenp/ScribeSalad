WEBVTT

1
00:00:00.150 --> 00:00:03.990
<v 0>One of the things that I think politics systematically,</v>

2
00:00:05.130 --> 00:00:05.880
underwrites and importance,

3
00:00:05.880 --> 00:00:09.450
and tends to be very bad at talking about is technological change just in

4
00:00:09.451 --> 00:00:12.750
general, how important it is, what is happening with it?

5
00:00:12.780 --> 00:00:16.140
We functionally live in a gerontocracy where, um,

6
00:00:16.200 --> 00:00:21.180
I think Nancy Pelosi's 80 Trump is 74, Joe Biden to 77.

7
00:00:21.450 --> 00:00:25.020
Um, Mitch McConnell is in the upper seventies. I think he might be 77.

8
00:00:25.530 --> 00:00:28.650
Like this is not the most tech forward leadership, uh,

9
00:00:28.680 --> 00:00:30.210
group you could possibly imagine.

10
00:00:30.510 --> 00:00:33.060
And so a lot of what is changing in technology, which is really important,

11
00:00:33.070 --> 00:00:34.830
I think it gets under played.

12
00:00:35.250 --> 00:00:38.790
So let's have a little bit of a conversation there. AI,

13
00:00:39.150 --> 00:00:43.560
how has your thinking changed on it in the past few years compared to, you know,

14
00:00:43.710 --> 00:00:45.660
two years ago or whatever it was when we talked,

15
00:00:46.380 --> 00:00:49.200
do you think the landscape there has changed? Um,

16
00:00:49.230 --> 00:00:51.780
do you think there are things people should know that have happened?

17
00:00:52.190 --> 00:00:57.050
<v 1>A lot of the things I was concerned about have now come into full</v>

18
00:00:57.051 --> 00:00:59.690
view because of the pandemic and you know,

19
00:00:59.691 --> 00:01:04.040
that companies that were considering investing in AI and

20
00:01:04.880 --> 00:01:09.230
robotic meatpackers and the rest of it, and a robot, aisle cleaners,

21
00:01:09.470 --> 00:01:13.970
and a grocery store. Now those companies are accelerating the investments.

22
00:01:14.420 --> 00:01:19.070
And the example I use is that now if you get a Domino's

23
00:01:19.071 --> 00:01:23.240
pizza and a self-driving car, you're excited because you had less human contact.

24
00:01:23.270 --> 00:01:25.910
Whereas before it seemed a little bit creepy, um,

25
00:01:25.940 --> 00:01:27.650
same with the self checkout aisle.

26
00:01:28.340 --> 00:01:32.750
And if you look at AI that there's this new release that now,

27
00:01:33.350 --> 00:01:37.610
uh, can do a lot of writing and basic summaries on a level that's

28
00:01:37.611 --> 00:01:40.190
indistinguishable from human journalists. You're.

29
00:01:40.190 --> 00:01:42.410
<v 0>Talking to you about opening the eyes, GPT three. Yeah.</v>

30
00:01:42.770 --> 00:01:44.600
<v 1>Yeah, yeah. Open the eyes GPT.</v>

31
00:01:44.780 --> 00:01:47.480
<v 0>Yeah. So if people haven't heard of this, I'm going to put in show notes.</v>

32
00:01:47.660 --> 00:01:52.310
My colleague, Kelsey Piper did a great piece on this, but it's surreal.

33
00:01:52.700 --> 00:01:53.480
Let me put it that way.

34
00:01:53.480 --> 00:01:56.390
W what it's able to do in terms of language processing is wild.

35
00:01:56.840 --> 00:01:59.120
<v 1>And two years ago, if you'd asked me, Hey,</v>

36
00:01:59.121 --> 00:02:02.540
is software going to get to this point? I would say, I would have said yes,

37
00:02:03.360 --> 00:02:06.080
but then you could still argue over it. You know what I mean? Uh,

38
00:02:06.130 --> 00:02:08.390
and now two years later,

39
00:02:08.391 --> 00:02:12.830
it's here and you can see the commercial potential of this

40
00:02:12.831 --> 00:02:13.664
technology.

41
00:02:13.700 --> 00:02:17.180
One of the reasons why I was so concerned about the impact of AI on our labor

42
00:02:17.181 --> 00:02:21.290
force Ezra, is that I know how many frankly,

43
00:02:21.291 --> 00:02:23.900
inefficient jobs there are. And a lot of these major companies,

44
00:02:24.020 --> 00:02:27.980
where if you gobble up another company and you have two different systems,

45
00:02:28.310 --> 00:02:30.320
you might keep dozens,

46
00:02:30.380 --> 00:02:33.680
even hundreds of folks around just to keep the systems talking to each other.

47
00:02:35.240 --> 00:02:37.400
It'd be like this. There's a lot of weird stuff.

48
00:02:37.430 --> 00:02:41.510
When you get into the bowels of some of these organizations. Uh,

49
00:02:41.540 --> 00:02:46.490
and I knew that over time, AI could end up making a lot of those types of jobs,

50
00:02:46.850 --> 00:02:51.200
less and less central, the arguments about human labor and AI,

51
00:02:51.280 --> 00:02:54.560
they tend to be non-market driven and kind of romantic. It'd be like, Oh,

52
00:02:54.561 --> 00:02:55.100
you know,

53
00:02:55.100 --> 00:02:59.920
like you can ever replace a human who does what Ezra does or after

54
00:02:59.930 --> 00:03:02.730
does, or, or whatnot. But, you know, a,

55
00:03:02.731 --> 00:03:06.640
a lot of our work is more replaceable than we'd like to think.

56
00:03:06.850 --> 00:03:09.490
And what's funny is if you ask Americans about this,

57
00:03:09.850 --> 00:03:14.050
they will actually say a majority of other people's jobs are automateable and

58
00:03:14.051 --> 00:03:15.700
subject to technological replacement.

59
00:03:16.000 --> 00:03:18.520
And then if you ask them about their own job, the vast majority will say,

60
00:03:18.521 --> 00:03:20.890
not my job. Uh, you know, that's just the way we're wired.

61
00:03:21.720 --> 00:03:24.450
<v 0>I'm fleeing to the high ground of podcasting. I mean,</v>

62
00:03:24.451 --> 00:03:26.460
they're going to come for the column this way before they come for the

63
00:03:26.461 --> 00:03:28.110
podcasters. It's much, much,

64
00:03:29.100 --> 00:03:32.220
much harder for AI to have a good conversation than for it to just write up a,

65
00:03:32.760 --> 00:03:34.200
write up a take on the GDP report.

66
00:03:35.280 --> 00:03:37.920
<v 1>Anyone, anyone like us, we're already at the top of the pyramid.</v>

67
00:03:37.921 --> 00:03:40.410
You know what I mean? Like if you have a podcast with a significant audience,

68
00:03:40.710 --> 00:03:43.860
you're, you're, you've already made it to a certain point. Like,

69
00:03:43.970 --> 00:03:47.160
and if you think about all the folks who are coming of age right now,

70
00:03:47.190 --> 00:03:50.220
like the younger versions of us who are graduating from college right now,

71
00:03:50.250 --> 00:03:51.240
it's like, you know, uh,

72
00:03:51.241 --> 00:03:55.770
it's w w the winner take all economy applies in many

73
00:03:55.771 --> 00:03:58.340
spaces, including the ones that you and I are.

74
00:03:58.800 --> 00:04:00.930
<v 0>Oh, absolutely. I want to talk about something else.</v>

75
00:04:01.440 --> 00:04:03.720
A lot of the problems that I care most about solving,

76
00:04:03.900 --> 00:04:06.750
they have a huge technological dimension to them. Um,

77
00:04:06.751 --> 00:04:09.660
I care a lot about animal suffering, and by far the most, uh,

78
00:04:09.720 --> 00:04:12.990
promising way to do something about that is plant cell-based meat,

79
00:04:13.680 --> 00:04:15.930
Kellogg about climate change. And while I've had Saul Griffith on the show,

80
00:04:15.931 --> 00:04:18.570
talking about how we could fix climate change without anything,

81
00:04:18.930 --> 00:04:20.520
any new technology being invented,

82
00:04:20.521 --> 00:04:24.870
there's just no doubt that given the level of mandate and adoption and take up

83
00:04:24.871 --> 00:04:27.120
that we'll need, if we can invent some great new stuff,

84
00:04:27.150 --> 00:04:31.380
it would become a lot easier. AI often gets talked about in a dystopic way,

85
00:04:31.381 --> 00:04:34.530
but if we really could invent it, it could make life pretty amazing.

86
00:04:35.190 --> 00:04:39.300
And one of the things that has struck me is that I don't think progressives in

87
00:04:39.301 --> 00:04:42.990
general have much of a theory of technology anymore. Um,

88
00:04:43.560 --> 00:04:46.290
I think that it's often talked about, and is this topic way,

89
00:04:46.470 --> 00:04:49.140
there's all the frustration at the billionaires who own it,

90
00:04:49.590 --> 00:04:53.400
I will often felt you sort of like oscillated between like a very scary story

91
00:04:53.401 --> 00:04:54.181
about AI,

92
00:04:54.181 --> 00:04:57.630
and then also like a real interest in technology as a way of solving problems.

93
00:04:57.960 --> 00:05:00.060
And I'd love to just hear you reflect on that. Like, what should,

94
00:05:00.390 --> 00:05:04.470
what do you think the political orientation to technology in general should be?

95
00:05:05.130 --> 00:05:07.260
<v 1>I love where you are on this Ezra. Like,</v>

96
00:05:07.290 --> 00:05:12.180
I think that ideally Progressive's are for progress and you

97
00:05:12.181 --> 00:05:15.420
know, what is going to enable a ton of progress is technology.

98
00:05:15.840 --> 00:05:18.330
And one of the fun things about, um,

99
00:05:18.360 --> 00:05:21.870
my campaign was I was making some very dark arguments about the impact of

100
00:05:21.871 --> 00:05:24.210
technology, which I completely believe, uh, you know,

101
00:05:24.211 --> 00:05:28.200
like if you have autonomous trucks in the next, you know,

102
00:05:28.201 --> 00:05:31.080
I'll extend the timeline. Cause I know you and I differ on this, but with like,

103
00:05:31.120 --> 00:05:33.330
let's say like 20 years, you know,

104
00:05:33.840 --> 00:05:38.640
like there'll be millions of folks who used to drive a truck for a living who,

105
00:05:38.670 --> 00:05:40.160
you know, will need something else to do. So, like,

106
00:05:40.161 --> 00:05:41.730
I believe all of that wholeheartedly,

107
00:05:42.360 --> 00:05:45.720
but I also agree with you that if you're trying to make people smarter,

108
00:05:45.750 --> 00:05:49.890
healthier, mentally healthier, uh, trying to clean up our planet, uh,

109
00:05:49.920 --> 00:05:54.180
trying to feed people in a way that doesn't brutalize animals. Uh,

110
00:05:54.240 --> 00:05:59.030
technology is a part of every single of those solutions in a very central

111
00:05:59.031 --> 00:06:00.860
way. And the,

112
00:06:00.870 --> 00:06:05.060
one of the dangerous to me about a lot of our politics now is where we're each

113
00:06:05.061 --> 00:06:09.830
arguing for different brands of nostalgia. You know what I mean? Meanwhile,

114
00:06:10.070 --> 00:06:13.610
time only goes in one direction, you know, it's 20, 20,

115
00:06:13.611 --> 00:06:15.750
you're now out in California. I got,

116
00:06:15.751 --> 00:06:20.600
I spent a lot of time there and you go to some of the folks who are working on

117
00:06:20.601 --> 00:06:22.310
the future and it is wild.

118
00:06:22.670 --> 00:06:25.460
Like some of the things that they're working on are very, um,

119
00:06:25.640 --> 00:06:29.390
positive and inspirational. Uh, some of them are very depressing and dystopian,

120
00:06:30.110 --> 00:06:30.140
um,

121
00:06:30.140 --> 00:06:35.030
but they're all packaged together and we should not be a group of people or a

122
00:06:35.031 --> 00:06:39.890
party that also has our heads in the sand about the

123
00:06:40.430 --> 00:06:42.650
positive and negative changes that technology brings.

124
00:06:42.651 --> 00:06:46.250
Like we have to be the party that is hard-nosed and realistic,

125
00:06:46.280 --> 00:06:50.510
but also willing to embrace the technologies that could

126
00:06:51.050 --> 00:06:56.000
lead us to something that more closely resembles utopia than this current mess

127
00:06:56.001 --> 00:06:59.720
we're living. Uh, so I, I love what you just said as well. I was like, uh,

128
00:06:59.770 --> 00:07:01.340
you know, I completely agree.

129
00:07:01.760 --> 00:07:04.850
Like we need to be embracing these technologies at a much higher level and I'm,

130
00:07:04.910 --> 00:07:09.620
I may be a part of that in this next administration, if, um, you know, uh,

131
00:07:09.650 --> 00:07:12.440
if we succeed in getting Trump out of their way, uh.

132
00:07:13.180 --> 00:07:16.000
<v 0>Follow up on that tantalizing, I may be a part of that. Um,</v>

133
00:07:16.180 --> 00:07:17.350
is there a job you've been talking about?

134
00:07:17.590 --> 00:07:21.700
<v 1>Oh, so I've had just very, very general informal conversations with, uh, uh,</v>

135
00:07:21.800 --> 00:07:25.540
the bind camp about trying to take on some sort of technology facing role,

136
00:07:25.930 --> 00:07:29.620
both some of the concerns that I campaigned on and trying to address them.

137
00:07:29.621 --> 00:07:31.900
And one thing we haven't talked about, but I'm very passionate about that.

138
00:07:31.901 --> 00:07:36.850
You will be two in a few years is the effect of social media and

139
00:07:36.851 --> 00:07:39.130
technology on our kids' mental health. Uh, you know,

140
00:07:39.131 --> 00:07:42.190
like right now you have massive levels of anxiety and depression among teenage

141
00:07:42.191 --> 00:07:45.700
girls in particular. Um, in order to make, uh,

142
00:07:45.730 --> 00:07:48.940
certain companies like Facebook richer, which, you know, is not a good look,

143
00:07:49.480 --> 00:07:52.390
but our government is way behind the curve on these issues.

144
00:07:52.480 --> 00:07:56.860
And I've offered my help in trying to catch us up. And there's some interest,

145
00:07:57.250 --> 00:07:58.390
uh, in taking me up on that.

146
00:07:58.870 --> 00:08:01.720
<v 0>And so, so then to go back to this underlying question,</v>

147
00:08:01.900 --> 00:08:04.480
you said something I thought that's worth picking up on about,

148
00:08:04.660 --> 00:08:07.180
you've talked to people in Silicon Valley and you go look at what they're doing.

149
00:08:07.181 --> 00:08:11.260
And some of it is very utopian and some of it is very dystopian and I've had

150
00:08:11.500 --> 00:08:14.080
probably some of the same conversations with even some of the same people you

151
00:08:14.081 --> 00:08:19.000
have. And what always strikes me is that it's not in most

152
00:08:19.001 --> 00:08:21.850
cases intrinsic to the technology, which way that plays out.

153
00:08:22.450 --> 00:08:27.190
And oftentimes it is actually about the way the government and both the

154
00:08:27.191 --> 00:08:30.730
rules of markets and regulations, but also to some degree,

155
00:08:30.760 --> 00:08:33.160
government implementation, taxation, et cetera,

156
00:08:33.400 --> 00:08:35.470
is going to interface with the technology.

157
00:08:35.471 --> 00:08:38.500
That's going to decide whether it ends up being like utopian or dystopian.

158
00:08:39.040 --> 00:08:43.060
And so there's sometimes seems to me to be a more obvious approach here,

159
00:08:43.061 --> 00:08:47.290
which is that I would say that it should be a more central

160
00:08:47.770 --> 00:08:50.140
priority, frankly,

161
00:08:50.141 --> 00:08:54.760
for both parties to use the government as a research and innovation accelerant.

162
00:08:55.440 --> 00:08:58.830
But then it also has to be a more central priority again,

163
00:08:58.831 --> 00:09:00.090
hopefully for both parties,

164
00:09:00.660 --> 00:09:05.310
for the government to have views and to care about how the

165
00:09:05.311 --> 00:09:07.650
technology is rolled out, who has access to it,

166
00:09:07.950 --> 00:09:10.650
and also what rules it operates under.

167
00:09:11.000 --> 00:09:14.570
<v 1>Yeah. That it's spot on as rhe. Uh,</v>

168
00:09:14.630 --> 00:09:19.190
and right now our government is out to lunch on most of these

169
00:09:19.191 --> 00:09:23.270
technology issues and we need to change it, you know, and I agree with you,

170
00:09:23.360 --> 00:09:27.740
they should be essential for both parties because the rate of change is just

171
00:09:27.741 --> 00:09:29.030
getting faster and faster.

172
00:09:29.540 --> 00:09:34.400
We got rid of the office of technology assessment in 1995, 25 years ago. Uh,

173
00:09:34.401 --> 00:09:37.670
and we can all sense that, well, there was no more technology. Oh yeah, no,

174
00:09:37.671 --> 00:09:40.400
we invented everything 95, 95.

175
00:09:42.440 --> 00:09:44.480
It's embarrassing. Really. And we can all sense it,

176
00:09:44.510 --> 00:09:48.080
like everyone just has this collective groan when it comes to government and

177
00:09:48.081 --> 00:09:49.820
technology. And then again,

178
00:09:49.821 --> 00:09:52.250
you have something like healthcare.gov where you're like, Oh no,

179
00:09:52.460 --> 00:09:53.540
they tried to make a website

180
00:09:55.900 --> 00:09:57.620
mean I shouldn't laugh because it was freaking terrible.

181
00:09:58.280 --> 00:10:02.570
<v 0>So if you were helping to modernize government on technology in a,</v>

182
00:10:02.571 --> 00:10:06.280
in a future administration, how would you think about that? Uh, where,

183
00:10:06.281 --> 00:10:07.460
where would you start? What would,

184
00:10:07.580 --> 00:10:10.130
what would be the kinds of things you would imagine doing.

185
00:10:10.970 --> 00:10:12.290
<v 1>Ramp up the United States,</v>

186
00:10:12.291 --> 00:10:17.180
digital service and empower it and you and I both know that there are hundreds,

187
00:10:17.181 --> 00:10:21.050
maybe even thousands of very talented technologists

188
00:10:21.650 --> 00:10:22.760
designers, coders,

189
00:10:22.761 --> 00:10:27.620
who would help a government if they had a runway to do so and

190
00:10:27.621 --> 00:10:31.220
felt like they could actually be empowered as opposed to going into sitting in

191
00:10:31.580 --> 00:10:34.760
meetings and getting their hands tied and red tape and bureaucracy and the rest

192
00:10:34.761 --> 00:10:37.100
of it. Uh, so that would be step one, you take,

193
00:10:37.101 --> 00:10:40.670
what's been working and then you pour fuel on it. Um, that the,

194
00:10:40.700 --> 00:10:43.590
the other thing I would be trying to do is, um, I,

195
00:10:43.591 --> 00:10:46.700
I think that there should be some kind of West coast base of operation and part

196
00:10:46.701 --> 00:10:49.130
of the talent enlistment move because of,

197
00:10:49.131 --> 00:10:52.730
there are a lot of techies that might not want to uproot their families and head

198
00:10:52.731 --> 00:10:57.470
to DC. And so you need to give them a place to reside in and land.

199
00:10:57.740 --> 00:11:02.180
And then we need to have actual experts getting the guts of the social media

200
00:11:02.181 --> 00:11:03.110
companies and the apps,

201
00:11:03.560 --> 00:11:07.580
because each of these companies is its own thing.

202
00:11:07.730 --> 00:11:10.700
Like you can't really have one size fits all. For example,

203
00:11:10.701 --> 00:11:14.690
20th century antitrust rules when it comes to Facebook and Instagram and the

204
00:11:14.691 --> 00:11:15.201
rest of it, though,

205
00:11:15.201 --> 00:11:18.920
I do think that they should not have allowed them to acquire Instagram. Um,

206
00:11:18.950 --> 00:11:19.701
but that there,

207
00:11:19.701 --> 00:11:23.810
there are all of these features that are company specific that you need real

208
00:11:23.811 --> 00:11:27.860
expertise to try and get to the bottom of and see if you can curb the worst of

209
00:11:27.861 --> 00:11:28.790
the excesses.

210
00:11:29.300 --> 00:11:33.410
<v 2>Thank you for listening in. I hope you enjoyed this conversation. If you did,</v>

211
00:11:33.650 --> 00:11:35.030
please do subscribe to yang,

212
00:11:35.031 --> 00:11:38.630
speaks and click on notifications so we can let you know every time we have a

213
00:11:38.631 --> 00:11:39.170
new episode.

