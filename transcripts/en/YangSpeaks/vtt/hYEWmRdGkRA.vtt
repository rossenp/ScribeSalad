WEBVTT

1
00:00:00.030 --> 00:00:04.080
<v 0>Are you on Tech-Talk? Um, on and off,</v>

2
00:00:04.140 --> 00:00:08.880
I do find the algorithm to be unsafely

3
00:00:08.881 --> 00:00:10.140
addictive. So.

4
00:00:11.030 --> 00:00:12.920
<v 1>I'm bingo. That's where it was going with it.</v>

5
00:00:15.380 --> 00:00:18.050
<v 0>Any technology center for humane tech? Yeah, we were talking about Tristan here.</v>

6
00:00:20.260 --> 00:00:22.160
Yeah. And, you know, I think, for example,

7
00:00:22.161 --> 00:00:24.920
like I was listening to a podcast with Tressa, Tristan Harris,

8
00:00:24.921 --> 00:00:29.570
and I think it's interesting, whatever I'm saying, his name wrong. Um, and, um,

9
00:00:29.810 --> 00:00:33.140
you know, the guy who invented the endless scroll, uh, you know,

10
00:00:33.141 --> 00:00:33.974
and it's just like,

11
00:00:34.100 --> 00:00:38.420
like a lot of this stuff is like really addictive and also like, um,

12
00:00:39.200 --> 00:00:44.180
prioritizes, hysteria and rage, um, and also just,

13
00:00:44.660 --> 00:00:47.300
uh, misinformation. Um, and you know, I,

14
00:00:47.301 --> 00:00:50.750
I think it's like leading to not only like a political crisis,

15
00:00:50.751 --> 00:00:55.280
but a mental health crisis. Um, you know, I was also,

16
00:00:55.281 --> 00:00:58.490
I was reading this thing about how, like you go on Twitter or whatever, and,

17
00:00:58.520 --> 00:01:02.150
and you, your brain is kind of getting the social chemicals and you're like,

18
00:01:02.151 --> 00:01:04.430
feel like you're getting the social time, but you know,

19
00:01:04.431 --> 00:01:08.060
you're not like there's different hormonal releases when you're actually in a

20
00:01:08.061 --> 00:01:11.030
room with real people, you know? And it's like, you're, you're actually,

21
00:01:11.420 --> 00:01:15.740
you feel like you're getting social a social experience, but you're really not.

22
00:01:15.790 --> 00:01:19.640
And so it was like people are kind of having the byproducts of being quite

23
00:01:19.641 --> 00:01:24.350
isolated. Um, not to mention the lack of sleep, like, you know,

24
00:01:24.351 --> 00:01:26.030
like these things, like you're on your phone late at night.

25
00:01:26.031 --> 00:01:27.260
Like it's going to be harder to sleep.

26
00:01:27.290 --> 00:01:29.840
Like I think sleeping pill usage is going way up,

27
00:01:31.460 --> 00:01:34.400
which is leads to less REM sleep, blah, blah, blah, blah.

28
00:01:34.401 --> 00:01:38.060
And all of this like feeds into like mental health crisis on top of, you know,

29
00:01:38.061 --> 00:01:42.710
all the political, political stuff and everything. Um, but yeah, I,

30
00:01:43.970 --> 00:01:46.280
I it's quite concerning. And, you know,

31
00:01:46.281 --> 00:01:50.030
I was thinking the other day I was talking to some people the other day, um,

32
00:01:50.240 --> 00:01:51.110
about how, you know,

33
00:01:51.111 --> 00:01:54.440
it's like we have global warming and then we have all these global warming

34
00:01:54.441 --> 00:01:55.700
scientists or whatever,

35
00:01:55.701 --> 00:01:58.640
or like we have pandemic and then we have all these pandemic scientists,

36
00:01:58.670 --> 00:02:02.540
but then we have tech and in there isn't like a tech safety community.

37
00:02:02.720 --> 00:02:04.670
That's super large. Like it's quite small.

38
00:02:05.120 --> 00:02:07.790
<v 1>Oh God, the extent that it exists, it's tiny.</v>

39
00:02:07.791 --> 00:02:11.690
Like it's Tristan people that he has on the podcast. And that that's a lot.

40
00:02:12.410 --> 00:02:15.830
<v 0>I think there's like Oxford, there's like some stuff at Oxford like there,</v>

41
00:02:15.890 --> 00:02:19.850
but it's still, it's like, it's just not, you know,

42
00:02:19.851 --> 00:02:23.570
it's interesting that when people consider career in tech, it's,

43
00:02:23.630 --> 00:02:26.600
it's all sort of like making the tech and none of it is the sort of like the

44
00:02:26.601 --> 00:02:29.690
stuff about making. Yeah. And it's like,

45
00:02:29.691 --> 00:02:32.420
especially like when we're getting close to AI and talking about AI, you know,

46
00:02:32.421 --> 00:02:34.910
it's like, it's going to be super important to have like,

47
00:02:35.330 --> 00:02:38.120
as many people working on AI safety as the,

48
00:02:38.270 --> 00:02:41.930
as much as we actually just building AI.

49
00:02:42.980 --> 00:02:46.880
<v 1>Well, this is one of the things I said on, on the trail Grimes is that I said,</v>

50
00:02:46.881 --> 00:02:47.570
look,

51
00:02:47.570 --> 00:02:51.800
some of the most powerful technologists in the world have said to me that they

52
00:02:51.801 --> 00:02:55.400
want guard rails on AI because all the incentives right now are to go as fast as

53
00:02:55.401 --> 00:02:58.730
possible because we're competing against each other and the Chinese and one of

54
00:02:58.731 --> 00:03:03.130
us is going do something very regrettable. Uh, and, and so I would say to folks,

55
00:03:03.250 --> 00:03:08.020
it's like, look, if Ilan is saying this, or, uh, Sam Altman is saying this,

56
00:03:08.021 --> 00:03:11.740
then we really need to listen because these are not people that frankly

57
00:03:12.070 --> 00:03:15.580
generally request guard rails like that. That's not good.

58
00:03:16.070 --> 00:03:20.130
<v 0>Yeah, yeah, yeah, no, exactly. Like, like, you know, they, yeah.</v>

59
00:03:21.150 --> 00:03:23.760
Um, uh, I mean,

60
00:03:23.761 --> 00:03:26.310
one of the things that really concerns me about the state of the government

61
00:03:26.311 --> 00:03:28.440
right now, um, you know,

62
00:03:28.441 --> 00:03:31.560
at the risk of sounding ageist is we just have a lot of people in the government

63
00:03:31.561 --> 00:03:34.740
who are like very old. They didn't grow up with the internet.

64
00:03:34.741 --> 00:03:37.800
They're not super tuned into technology. Like, you know,

65
00:03:37.801 --> 00:03:39.030
there's people who are like getting there.

66
00:03:39.280 --> 00:03:42.690
They're not even checking their email like in Congress and stuff, they.

67
00:03:42.690 --> 00:03:44.430
<v 1>Might never check their email and their title line.</v>

68
00:03:45.480 --> 00:03:48.390
<v 0>Emails get printed out and they read the paper and then like, say, you know,</v>

69
00:03:48.690 --> 00:03:50.430
it's just like, this is like really concerning.

70
00:03:50.431 --> 00:03:55.200
Like I don't think the government across the board has any understanding of

71
00:03:55.290 --> 00:03:55.921
technology.

72
00:03:55.921 --> 00:03:58.650
And that's part of the reason the Facebook situation is so out of control.

73
00:03:59.130 --> 00:04:02.140
<v 1>We can all see it's happening and you don't want to be a jerk to be like, Oh,</v>

74
00:04:02.300 --> 00:04:03.570
like, you know, the olds don't get this stuff.

75
00:04:03.600 --> 00:04:07.050
But the fact is the average age of a us Senator is 62.

76
00:04:07.170 --> 00:04:09.150
So like how many 62 year olds, you know,

77
00:04:09.151 --> 00:04:13.230
that like understand like the mechanics and the social media companies.

78
00:04:14.190 --> 00:04:16.080
Um, very few. And, uh,

79
00:04:16.090 --> 00:04:19.200
there has not been independent guidance on technology-related issues in the U S

80
00:04:19.201 --> 00:04:23.460
Congress since 1995, you know, 25 years. So,

81
00:04:23.500 --> 00:04:27.990
so they're completely out to lunch on this set of issues. Uh,

82
00:04:28.050 --> 00:04:31.680
it's corrosive for our democracy. It's corrosive for our mental health. Uh,

83
00:04:31.700 --> 00:04:35.880
and I've experienced these social media apps in a particular way

84
00:04:36.180 --> 00:04:37.410
where, uh,

85
00:04:37.440 --> 00:04:41.820
I ran for president and try to use these apps to the best of my ability to make

86
00:04:41.821 --> 00:04:46.410
my case and, uh, compete and generate a following. Uh,

87
00:04:46.470 --> 00:04:50.340
and so I've benefited from them tremendously. Um,

88
00:04:51.180 --> 00:04:51.800
but, uh.

89
00:04:51.800 --> 00:04:54.420
<v 0>Same, I rely on them completely to run my business.</v>

90
00:04:54.640 --> 00:04:56.850
<v 1>Yeah. Um, but I can very,</v>

91
00:04:56.851 --> 00:05:01.770
very clearly see the downsides for users,

92
00:05:02.610 --> 00:05:06.780
uh, where I think that I'm unusually good on them.

93
00:05:07.080 --> 00:05:08.730
And even for me sometimes, like, I,

94
00:05:08.731 --> 00:05:13.650
I sense myself lapsing into like a Twitter vortex or like a,

95
00:05:13.651 --> 00:05:17.310
you know, like an hour or two will pass. Um, and,

96
00:05:17.520 --> 00:05:19.200
and I'll look up and be like, Oh wait,

97
00:05:19.201 --> 00:05:20.640
like I was supposed to do this other thing.

98
00:05:21.900 --> 00:05:26.670
And it feels like you're being productive in a particular way and social in a

99
00:05:26.671 --> 00:05:27.510
particular way.

100
00:05:28.320 --> 00:05:30.870
<v 0>Wasn't you reward, you know, the way it's been set up,</v>

101
00:05:30.900 --> 00:05:33.360
like as Tristan Harris or whatever says, like, it's,

102
00:05:33.870 --> 00:05:38.160
it's just rewarded like, Ooh, like you hit the heart and go blue. Like,

103
00:05:38.250 --> 00:05:41.100
you know, like the score, like there's, it's so set up visually to just like,

104
00:05:41.130 --> 00:05:44.880
make your brain just be like, like tuned in and fed, you know,

105
00:05:45.100 --> 00:05:48.720
it does feel semi productive or, you know, you're like, Oh, I'm learning things,

106
00:05:48.721 --> 00:05:52.950
I'm learning things. But it's like a very shallow, um, comprehension, you know,

107
00:05:52.951 --> 00:05:55.440
if you're getting a one sentence on a thing it's like, Ooh, like a fact, like,

108
00:05:55.441 --> 00:05:59.540
are you even going to remember that? And there's no deep knowledge there. Um,

109
00:06:00.080 --> 00:06:02.540
and I'm not saying like, I don't want to say social media is all bad.

110
00:06:02.541 --> 00:06:04.640
Social media obviously is great. And you know,

111
00:06:04.670 --> 00:06:08.480
there's been a lot of like political organizing and stuff on it. So, you know,

112
00:06:08.481 --> 00:06:12.920
it's, it's definitely like, I'm not just saying like, you know, social media,

113
00:06:12.950 --> 00:06:13.783
but.

114
00:06:13.990 --> 00:06:14.000
<v 1>There,</v>

115
00:06:14.000 --> 00:06:17.920
there are excesses and the excesses are hitting teenage girls particularly hard.

116
00:06:17.980 --> 00:06:18.400
Yeah.

117
00:06:18.400 --> 00:06:19.900
<v 0>Yes, yeah, yeah, yeah, yeah, yeah. Yeah.</v>

118
00:06:20.500 --> 00:06:24.490
I mean the beauty stuff I think is like concerning.

119
00:06:25.660 --> 00:06:26.493
Um.

120
00:06:26.560 --> 00:06:30.820
<v 1>And I believe that that Tik TOK is the most powerful of these in its own way.</v>

121
00:06:30.850 --> 00:06:35.410
Like I just got on Tik TOK maybe, um, two weeks ago,

122
00:06:36.220 --> 00:06:40.180
three weeks ago. And Holy cow,

123
00:06:40.210 --> 00:06:44.650
is that stuff powerful. Like you, you get in there and, uh, it's,

124
00:06:45.010 --> 00:06:46.870
it's hypnotic, uh, you know,

125
00:06:46.871 --> 00:06:51.310
you just end up seeing these videos of human beings, like, like dads.

126
00:06:52.540 --> 00:06:55.570
<v 0>Humans are drained. You want to look at human faces, you know, it's like,</v>

127
00:06:55.630 --> 00:06:59.560
like considering the amount of portraits versus landscapes, you know,

128
00:06:59.561 --> 00:07:02.440
in the lineage of painting. Um,

129
00:07:02.830 --> 00:07:05.140
like we want to look at humans like we're, you know.

130
00:07:07.330 --> 00:07:08.770
<v 1>I think it behooves most,</v>

131
00:07:08.771 --> 00:07:13.540
any adult who wants to understand what's going on with the younger generation to

132
00:07:13.541 --> 00:07:15.820
try and spend a few hours on Tik TOK,

133
00:07:16.210 --> 00:07:19.210
just to see what the heck the kids are doing. Um,

134
00:07:19.240 --> 00:07:23.020
because it's like another language it's like another visual and digital

135
00:07:23.021 --> 00:07:27.160
language. It's got a different energy and vocabulary,

136
00:07:27.161 --> 00:07:31.270
different people. Uh, you know, it's like, it's fascinating because like,

137
00:07:31.271 --> 00:07:31.511
you know,

138
00:07:31.511 --> 00:07:34.930
I've been on Twitter now for a little bit and all the media and the journalists

139
00:07:34.931 --> 00:07:38.830
are on Twitter and then you go on and take talk, and it's like just actual,

140
00:07:38.831 --> 00:07:39.940
like kids having fun.

141
00:07:42.790 --> 00:07:43.930
<v 0>I think it's great for music.</v>

142
00:07:45.280 --> 00:07:49.510
<v 1>For you, for music, it would be, I'm sure like a really useful fire hose.</v>

143
00:07:49.540 --> 00:07:51.400
You could just like, turn it on and like, um,

144
00:07:51.430 --> 00:07:53.140
get stimulation in a particular way.

145
00:07:53.830 --> 00:07:57.160
<v 0>Well, in a sort of, I think a more natural, I mean, maybe there are, you know,</v>

146
00:07:57.430 --> 00:08:01.720
the dark forces of major labels pushing certain songs or whatever,

147
00:08:01.721 --> 00:08:03.610
but like, I, as far as I'm aware, like,

148
00:08:03.611 --> 00:08:05.230
I feel like it's just people like dig a song,

149
00:08:05.231 --> 00:08:07.390
a song has a cool vibe with a certain movement.

150
00:08:07.391 --> 00:08:11.620
And then like that song can kind of take off in a natural, organic way.

151
00:08:11.621 --> 00:08:15.430
And I think that's pretty cool. Um, cause music is super over gate kept.

152
00:08:15.431 --> 00:08:19.390
And I like, I think kick talk doesn't have that.

153
00:08:19.750 --> 00:08:20.770
I don't know I might be wrong.

154
00:08:21.130 --> 00:08:24.460
<v 1>But there's a music discovery element and they're like undiscovered artists who</v>

155
00:08:24.461 --> 00:08:28.480
are just making their stuff there. Uh, and yeah, and getting found.

156
00:08:28.660 --> 00:08:30.310
<v 0>Are you going to run again, do you think.</v>

157
00:08:31.660 --> 00:08:34.660
<v 1>I'm almost certainly going to run for something at some point I've been pretty</v>

158
00:08:34.661 --> 00:08:36.280
public about that. Um,

159
00:08:36.370 --> 00:08:39.430
and I dare say it would be more fun the second time than the first time,

160
00:08:39.431 --> 00:08:42.790
because we'd start at a higher base.

161
00:08:45.640 --> 00:08:48.340
Um, but the, the problems are just getting bigger and hairier and nastier,

162
00:08:48.341 --> 00:08:51.550
and I just want to try and solve them for your son, for my sons,

163
00:08:51.551 --> 00:08:53.320
for everyone's kids. Um,

164
00:08:53.350 --> 00:08:57.300
because you and I are in a where we're actually more able to make a difference

165
00:08:57.301 --> 00:09:01.050
in the world and frankly, you know, like the vast, vast majority of folks.

166
00:09:01.051 --> 00:09:04.290
And so, uh, like I take that responsibility to heart. So yeah,

167
00:09:04.291 --> 00:09:07.350
you're going to see Andrew Yang on some ballot, somewhere, some campaign. Um,

168
00:09:07.380 --> 00:09:12.270
I've got a couple of, uh, visions, uh, but first things first, you know,

169
00:09:12.440 --> 00:09:16.830
we, we have, um, right now this, this might be too, uh,

170
00:09:17.970 --> 00:09:18.990
uh, too, nitty-gritty for you,

171
00:09:18.991 --> 00:09:21.930
but we were heading to Georgia for the Senate races, uh,

172
00:09:21.960 --> 00:09:26.880
trying to win the Senate for, for the Dems. And then, uh,

173
00:09:27.180 --> 00:09:27.601
you know, and,

174
00:09:27.601 --> 00:09:30.810
and then I may end up in the administration trying to tackle some of the social

175
00:09:30.811 --> 00:09:32.580
media problems you're talking about. Um.

176
00:09:32.630 --> 00:09:34.520
<v 0>That would be cool. Yeah. If you, if you,</v>

177
00:09:34.521 --> 00:09:37.190
if that does end up happening and you end up, like, I don't know,

178
00:09:37.191 --> 00:09:40.730
like working with the center for humane tech people or anything like that,

179
00:09:40.731 --> 00:09:44.120
like if you need any help, definitely hit me up.

180
00:09:44.810 --> 00:09:49.340
<v 1>Hit you up. That would be so fun. Grimes. What we do is we'd have this, um,</v>

181
00:09:49.370 --> 00:09:51.680
you know, this message that we need to get out. Um,

182
00:09:51.710 --> 00:09:55.040
and then you could help and you could like make some freaking incredible,

183
00:09:55.190 --> 00:09:59.090
like creative for it. Um, we'll hit you up on that, for sure like that.

184
00:09:59.180 --> 00:10:00.590
And you could be like the champion of the,

185
00:10:00.591 --> 00:10:04.280
of the humane technology movement or the human tech movement. I would love it.

186
00:10:05.510 --> 00:10:08.120
<v 0>Cool. Yeah. Well, anyway, I can help. Just let me know.</v>

187
00:10:09.350 --> 00:10:13.280
<v 1>Thank you for listening in. I hope you enjoyed this conversation. If you did,</v>

188
00:10:13.670 --> 00:10:15.020
please do subscribe to yang,

189
00:10:15.021 --> 00:10:18.620
speaks and click on notifications so we can let you know every time we have a

190
00:10:18.621 --> 00:10:19.100
new episode.

